{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name load_data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7c79e4257641>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnbfinder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNotebookFinder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNotebookFinder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnotebooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnotebooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhelper_fxns\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_run_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnotebooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_n_plot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name load_data"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import argparse\n",
    "from nbfinder import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "from notebooks.data_loader import load_data\n",
    "from notebooks.helper_fxns import create_run_dir\n",
    "from notebooks.print_n_plot import *\n",
    "import warnings\n",
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "from lasagne.objectives import *\n",
    "from lasagne.regularization import regularize_network_params, l2\n",
    "from lasagne.updates import *\n",
    "from lasagne.init import HeNormal\n",
    "from lasagne.nonlinearities import rectify as relu\n",
    "from lasagne.nonlinearities import *\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "import sys\n",
    "import numpy as np\n",
    "import logging\n",
    "import time\n",
    "import pickle\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_cl_args():\n",
    "    # if inside a notebook, then get rid of weird notebook arguments, so that arg parsing still works\n",
    "    if any([\"jupyter\" in arg for arg in sys.argv]):\n",
    "        sys.argv=sys.argv[:1]\n",
    "\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('-l', '--learn_rate', default=0.01, type=float,\n",
    "        help='the learning rate for the network')\n",
    "\n",
    "    parser.add_argument('-n', '--num_ims', default=2000, type=int,\n",
    "        help='number of total images')\n",
    "\n",
    "    parser.add_argument('-f', '--num_filters', default=128, type=int,\n",
    "        help='number of filters in each conv layer')\n",
    "\n",
    "    parser.add_argument('-e', '--num_epochs', default=5000, type=int)\n",
    "\n",
    "    parser.add_argument('-b', '--batch_size', default=128, type=int)\n",
    "\n",
    "    parser.add_argument( '--fc', default=1024, type=int,\n",
    "        help='number of fully connected units')\n",
    "\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_network(args, network):\n",
    "    X = T.tensor4('input_var')\n",
    "    Y = T.ivector('target_var')\n",
    "    #network = build_layers(args)\n",
    "    '''write loss function equation'''\n",
    "    prediction = get_output(network, X)\n",
    "    loss = categorical_crossentropy(prediction, Y).mean()\n",
    "    weightsl2 = regularize_network_params(network, l2)\n",
    "    loss += args['weight_decay'] * weightsl2\n",
    "    \n",
    "    '''calculate test loss (cross entropy with no regularization) and accuracy'''\n",
    "    test_prediction = get_output(network, X, deterministic=True)\n",
    "    test_loss = categorical_crossentropy(test_prediction, Y).mean()\n",
    "    \n",
    "    '''classification percentage: we can change this based on false postive/false negative criteria'''\n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), Y))\n",
    "    params = get_all_params(network, trainable=True)\n",
    "    #new_weight += momentum*prev_step  - leaarning_rate * (dL(cur_weight + momentum*prev_step)/dcur_weight) \n",
    "    updates = nesterov_momentum(loss, params, learning_rate=args['learning_rate'], momentum=args['momentum'])\n",
    "    '''train_fn -> takes in input,label pairs -> outputs loss '''\n",
    "    train_fn = theano.function([X, Y], loss, updates=updates)\n",
    "    '''val_fn -> takes in input,label pairs -> outputs non regularized loss and accuracy '''\n",
    "    val_fn = theano.function([X, Y], [test_loss, test_acc])\n",
    "    out_fn = theano.function([X], [test_prediction])\n",
    "    return {\n",
    "            'tr_fn': train_fn, \n",
    "            'val_fn': val_fn, \n",
    "            'network': network,\n",
    "            'out_fn': out_fn\n",
    "            \n",
    "            }\n",
    "\n",
    "def build_layers(args):\n",
    "    \n",
    "    conv_kwargs = dict(num_filters=args['num_filters'], filter_size=3, pad=1, nonlinearity=relu, W=HeNormal())\n",
    "    network = InputLayer(shape=args['input_shape'])\n",
    "    for lay in range(args['num_layers']):\n",
    "        network = batch_norm(Conv2DLayer(network, **conv_kwargs))\n",
    "        network = MaxPool2DLayer(network, pool_size=(2,2),stride=2)\n",
    "    network = dropout(network, p=args['dropout_p'])\n",
    "    network = DenseLayer(network,num_units=args['num_fc_units'], nonlinearity=relu) \n",
    "    network = dropout(network, p=args['dropout_p'])\n",
    "    network = DenseLayer(network, num_units=2, nonlinearity=softmax)\n",
    "    \n",
    "    for layer in get_all_layers(network):\n",
    "        logger.info(str(layer) + str(layer.output_shape))\n",
    "    print count_params(layer)\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    if batchsize > inputs.shape[0]:\n",
    "        batchsize=inputs.shape[0]\n",
    "    for start_idx in range(0,len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx: start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shuffle(x,y):\n",
    "    inds = np.arange(x.shape[0])\n",
    "\n",
    "    #shuffle data\n",
    "    rng = np.random.RandomState(7)\n",
    "    rng.shuffle(inds)\n",
    "    return x[inds], y[inds]\n",
    "\n",
    "def split_train_val(x,y):\n",
    "    inds = np.arange(x.shape[0])\n",
    "    #split train, val, test\n",
    "    tr_inds = inds[:int(0.8*len(inds))] \n",
    "    val_inds = inds[int(0.8*len(inds)):]\n",
    "\n",
    "    x_tr, y_tr, x_val, y_val = x[tr_inds], y[tr_inds], x[val_inds], y[val_inds]\n",
    "    return x_tr, y_tr, x_val, y_val\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(x, max_abs=None):\n",
    "    '''a type of sparse preprocessing, which scales everything between -1 and 1 without losing sparsity'''\n",
    "    #only calculate the statistic using training set\n",
    "    if max_abs is None:\n",
    "        max_abs=np.abs(x).max(axis=(0,1,2,3))\n",
    "\n",
    "    #then scale all sets\n",
    "    x /= max_abs\n",
    "    print np.max(x)\n",
    "    print np.min(x)\n",
    "    return x, max_abs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_train_val(num_events=1000,bin_size=0.1):\n",
    "    x, y = load_data(num_events=num_events, bin_size=bin_size)\n",
    "    x_tr, y_tr, x_val, y_val  = split_train_val(*shuffle(x,y))\n",
    "    x_tr, max_abs = preprocess(x_tr)\n",
    "    x_val, _ = preprocess(x_val, max_abs)\n",
    "    return x_tr, y_tr, x_val, y_val\n",
    "\n",
    "def load_test(num_events=None):\n",
    "    x,y = load_data(bg_cfg_file='./config/TestSetBgFileListAug16.txt', \n",
    "                    sig_cfg_file='./config/TestSetSignalFileListAug16.txt',\n",
    "                    num_events=num_events, \n",
    "                    bin_size=0.1)\n",
    "    x_te, y_te = shuffle(x,y)\n",
    "    x_te = preprocess(x_te)\n",
    "    return x_te, y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_logger(run_dir):\n",
    "    logger = logging.getLogger('log_train')\n",
    "    if not getattr(logger, 'handler_set', None):\n",
    "        logger.setLevel(logging.INFO)\n",
    "        fh = logging.FileHandler('%s/training.log'%(run_dir))\n",
    "        fh.setLevel(logging.INFO)\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setLevel(logging.INFO)\n",
    "        logger.addHandler(ch)\n",
    "        logger.addHandler(fh)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_val(net_cfg, network_kwargs, data, logger, run_dir):\n",
    "\n",
    "\n",
    "    x_tr, y_tr, x_val, y_val = data\n",
    "    logger.info(\"training set size: %i, val set size %i \" %( x_tr.shape[0], x_val.shape[0]))\n",
    "\n",
    "    tr_losses = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    tr_accs = []\n",
    "    for epoch in range(network_kwargs['num_epochs']):\n",
    "\n",
    "        start = time.time() \n",
    "        tr_loss = 0\n",
    "        tr_acc = 0\n",
    "        for iteration, (x, y) in enumerate(iterate_minibatches(x_tr,y_tr, batchsize=network_kwargs['batch_size'])):\n",
    "            #x = np.squeeze(x)\n",
    "            loss = net_cfg['tr_fn'](x, y)\n",
    "            weights = sum([np.sum(a.eval()) for a in get_all_params(net_cfg['network']) if str(a) == 'W'])\n",
    "            #logger.info(\"weights : %6.3f\" %(weights))\n",
    "            #logger.info(\"x avg : %5.5f shape: %s : iter: %i  loss : %6.3f \" % (np.mean(x), str(x.shape), iteration, loss))\n",
    "            _, acc = net_cfg['val_fn'](x,y)\n",
    "            logger.info(\"iteration % i train loss is %f\"% (iteration, loss))\n",
    "            logger.info(\"iteration % i train acc is %f\"% (iteration, acc))\n",
    "            tr_acc += acc\n",
    "            tr_loss += loss\n",
    "\n",
    "        train_end = time.time()\n",
    "        tr_avgacc = tr_acc / (iteration + 1)\n",
    "        tr_avgloss = tr_loss / (iteration + 1)\n",
    "\n",
    "\n",
    "        logger.info(\"train time : %5.2f seconds\" % (train_end - start))\n",
    "        logger.info(\"  epoch %i of %i train loss is %f\" % (epoch, num_epochs, tr_avgloss))\n",
    "        logger.info(\"  epoch %i of %i train acc is %f percent\" % (epoch, num_epochs, tr_avgacc * 100))\n",
    "        tr_losses.append(tr_avgloss)\n",
    "        tr_accs.append(tr_avgacc)\n",
    "\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        for iteration, (xval, yval) in enumerate(iterate_minibatches(x_val,y_val, batchsize=network_kwargs['batch_size'])):\n",
    "            #xval = np.squeeze(xval)\n",
    "            loss, acc = net_cfg['val_fn'](xval, yval)\n",
    "            val_loss += loss\n",
    "            val_acc += acc\n",
    "\n",
    "        val_avgloss = val_loss / (iteration + 1)\n",
    "        val_avgacc = val_acc / (iteration + 1)\n",
    "\n",
    "        logger.info(\"val time : %5.2f seconds\" % (time.time() - train_end))\n",
    "        logger.info(\"  epoch %i of %i val loss is %f\" % (epoch, num_epochs, val_avgloss))\n",
    "        logger.info(\"  epoch %i of %i val acc is %f percent\" % (epoch, num_epochs, val_avgacc * 100))\n",
    "\n",
    "        val_losses.append(val_avgloss)\n",
    "        val_accs.append(val_avgacc)\n",
    "\n",
    "        plot_learn_curve(tr_losses, val_losses, save_dir=run_dir)\n",
    "        plot_learn_curve(tr_accs, val_accs, save_dir=run_dir, name=\"acc\")\n",
    "        pickle.dump(net_cfg['network'],open(run_dir + \"/model.pkl\", 'w'))\n",
    "\n",
    "    #     if epoch % 5 == 0:\n",
    "    #         plot_filters(net_cfg['network'], save_dir=run_dir)\n",
    "    #         for iteration, (xval, yval) in enumerate(iterate_minibatches(x_val,y_val, batchsize=batchsize)):\n",
    "    #             plot_feature_maps(iteration, xval,net_cfg['network'], save_dir=run_dir)\n",
    "    #             break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_example(x):\n",
    "    plt.imshow(np.log10(x).T,extent=[-3.15, 3.15, -5, 5], interpolation='none',aspect='auto', origin='low')\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_examples(x, dim, run_dir='.', name='ims'):\n",
    "    plt.clf()\n",
    "    assert x.shape[0] == dim**2, \"not the right number examples images\"\n",
    "    fig, axes = plt.subplots(nrows=dim, ncols=dim, figsize=(40,40))\n",
    "    for ex, ax in zip(x, axes.flat):\n",
    "        im = ax.imshow(np.log10(ex).T,extent=[-3.15, 3.15, -5, 5], interpolation='none',aspect='auto', origin='low', vmin=-10, vmax=0)\n",
    "\n",
    "    #cax = fig.add_axes([0.9, 0.1, 0.03, 0.8])\n",
    "    #fig.colorbar(im, cax=cax)\n",
    "    #plt.show()\n",
    "    plt.savefig(run_dir + '/' + name + \".png\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_filters(network,num_channels_to_plot = 16, save_dir='.'):\n",
    "    plt.figure(figsize=(30,30))\n",
    "    plt.clf()\n",
    "    lay_ind = 0\n",
    "    convlayers = [layer for layer in get_all_layers(network) if isinstance(layer, Conv2DLayer)]\n",
    "    num_layers = len(convlayers)\n",
    "    spind = 1 \n",
    "    for i,layer in enumerate(convlayers):\n",
    "        filters = layer.get_params()[0].eval()\n",
    "        if i==0:\n",
    "            filt = filters\n",
    "        else:\n",
    "            filt= filters[0]\n",
    "        for ch_ind in range(num_channels_to_plot):\n",
    "            p1 = plt.subplot(num_layers,num_channels_to_plot, spind )\n",
    "            p1.imshow(filt[ch_ind], cmap=\"gray\")\n",
    "            spind = spind + 1\n",
    "    \n",
    "    #plt.show()\n",
    "    plt.savefig(save_dir +'/filters.png')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_feature_maps(im, network, save_dir='.', name=\"\"):\n",
    "    plt.figure(figsize=(30,30))\n",
    "    plt.clf()\n",
    "    convlayers = [layer for layer in get_all_layers(network) if isinstance(layer,Conv2DLayer) or isinstance(layer,Pool2DLayer)]\n",
    "    num_layers = len(convlayers)\n",
    "    spind = 1 \n",
    "    num_fmaps_to_plot = 16\n",
    "    print im.shape\n",
    "    for ch in range(num_fmaps_to_plot):\n",
    "        p1 = plt.subplot(num_layers + 1,num_fmaps_to_plot, spind )\n",
    "        p1.imshow(np.log10(im.T))\n",
    "        spind = spind + 1\n",
    "    \n",
    "    # take im from 2d to 4d so the network likes it\n",
    "    x=np.expand_dims(np.expand_dims(im, axis=0), axis=0)\n",
    "    for layer in convlayers:\n",
    "        # shape is batch_size, num_filters, x,y \n",
    "        fmaps = get_output(layer,x ).eval()\n",
    "        print fmaps.shape\n",
    "        for fmap_ind in range(num_fmaps_to_plot):\n",
    "            p1 = plt.subplot(num_layers + 1,num_fmaps_to_plot, spind )\n",
    "            p1.imshow(fmaps[0][fmap_ind].T)\n",
    "            spind = spind + 1\n",
    "    \n",
    "    #plt.show()\n",
    "    plt.savefig(save_dir +'/fmaps_%s.png' %(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-293aab9805dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.expand_dims(np.expand_dims(np.zeros((20,20)), axis=0), axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.subplots?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_network(network_path):\n",
    "    x_te, y_te = load_test()\n",
    "\n",
    "    net = pickle.load(open(network_path))\n",
    "\n",
    "    cfg = build_network(network_kwargs,net)\n",
    "    return cfg['val_fn'](x_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = get_cl_args()\n",
    "\n",
    "\n",
    "    run_dir = create_run_dir()\n",
    "    logger = get_logger(run_dir)\n",
    "    data = load_train_val(args.num_ims)\n",
    "    network_kwargs = {'input_shape':tuple([None] + [data[0].shape[i] for i in [1,2,3]]), \n",
    "                      'learning_rate': args.learn_rate, \n",
    "                      'dropout_p': 0, \n",
    "                      'weight_decay': 0.0001, \n",
    "                      'num_filters': args.num_filters, \n",
    "                      'num_fc_units': args.fc,\n",
    "                      'num_layers': 4,\n",
    "                      'momentum': 0.9,\n",
    "                      'num_epochs': args.num_epochs,\n",
    "                      'batch_size': args.batch_size}\n",
    "    logger.info(str(network_kwargs))\n",
    "    net_cfg = build_network(network_kwargs, build_layers(network_kwargs))\n",
    "    train_val(net_cfg, network_kwargs, data, logger, run_dir)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test\n",
    "x, y, xv,yv = load_train_val(num_events=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "network_path = './results/run84/model.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = pickle.load(open(network_path))\n",
    "\n",
    "cfg = build_network(network_kwargs,net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = cfg['out_fn'](xv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_sig = xv[np.argmax(y_pred[:,1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_bg = xv[np.argmin(y_pred[:,1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_example(np.squeeze(best_sig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_example(np.squeeze(best_bg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inds = np.argsort(y_pred[:,1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_bgs = np.squeeze(xv[inds[:25]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_sigs = np.squeeze(xv[inds[-26:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_examples(best_bgs,5, run_dir,\"best_bg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_examples(best_sigs,5, run_dir, \"best_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_filters(net,save_dir=run_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_feature_maps(best_bgs[0], net, run_dir, name=\"best_bg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "def compile_saliency_function(net):\n",
    "    \"\"\"\n",
    "    Compiles a function to compute the saliency maps and predicted classes\n",
    "    for a given minibatch of input images.\n",
    "    \"\"\"\n",
    "    inp = T.tensor4('inp')\n",
    "    outp = lasagne.layers.get_output(net,inp, deterministic=True)\n",
    "    max_outp = T.max(outp, axis=1)\n",
    "    saliency = theano.grad(max_outp.sum(), wrt=inp)\n",
    "    max_class = T.argmax(outp, axis=1)\n",
    "    return theano.function([inp], [saliency, max_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = ['bg', 'sig']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_images(img_original, saliency, max_class, title, save_dir='.'):\n",
    "    # get out the first map and class from the mini-batch\n",
    "    saliency = saliency[0]\n",
    "    max_class = max_class[0]\n",
    "    # convert saliency from BGR to RGB, and from c01 to 01c\n",
    "    # plot the original image and the three saliency map variants\n",
    "    im_args = dict(extent=[-3.15, 3.15, -5, 5], interpolation='none',aspect='auto', origin='low')\n",
    "    plt.figure(figsize=(10, 10), facecolor='w')\n",
    "    plt.suptitle(\"Class: \" + classes[max_class] + \". Saliency: \" + title)\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.title('input')\n",
    "    plt.imshow(np.log10(img_original).T, **im_args)\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.title('abs. saliency')\n",
    "    plt.imshow(np.squeeze(np.abs(saliency)).T, cmap='gray', **im_args)\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.title('pos. saliency')\n",
    "    plt.imshow(np.squeeze((np.maximum(0, saliency) / saliency.max())).T, **im_args)\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.title('neg. saliency')\n",
    "    plt.imshow(np.squeeze((np.maximum(0, -saliency) / -saliency.min())).T, **im_args)\n",
    "    plt.savefig(run_dir + '/saliency_' + classes[max_class] + \".png\" )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_bg = np.expand_dims(np.expand_dims(best_bgs[0], axis=0),axis=0)\n",
    "best_sig = np.expand_dims(np.expand_dims(best_sigs[-1], axis=0),axis=0)\n",
    "saliency_fn = compile_saliency_function(net)\n",
    "saliency, max_class = saliency_fn(best_sig)\n",
    "#np.squeeze(np.abs(saliency)).shape\n",
    "show_images(best_sigs[-1], saliency, max_class, \"default gradient\", save_dir=run_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "deeplearning_plus_root",
   "language": "python",
   "name": "deeplearning_plus_root"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
