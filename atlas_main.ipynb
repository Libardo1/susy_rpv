{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import argparse\n",
    "from nbfinder import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "from notebooks.data_loader import load_data\n",
    "from notebooks.helper_fxns import create_run_dir\n",
    "from notebooks.print_n_plot import *\n",
    "import warnings\n",
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "from lasagne.objectives import *\n",
    "from lasagne.regularization import regularize_network_params, l2\n",
    "from lasagne.updates import *\n",
    "from lasagne.init import HeNormal\n",
    "from lasagne.nonlinearities import rectify as relu\n",
    "from lasagne.nonlinearities import *\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "import sys\n",
    "import numpy as np\n",
    "import logging\n",
    "import time\n",
    "import pickle\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if inside a notebook, then get rid of weird notebook arguments, so that arg parsing still works\n",
    "if any([\"jupyter\" in arg for arg in sys.argv]):\n",
    "    sys.argv=sys.argv[:1]\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('-l', '--learn_rate', default=0.01, type=float,\n",
    "    help='the learning rate for the network')\n",
    "\n",
    "parser.add_argument('-n', '--num_ims', default=2000, type=int,\n",
    "    help='number of total images')\n",
    "\n",
    "parser.add_argument('-f', '--num_filters', default=128, type=int,\n",
    "    help='number of filters in each conv layer')\n",
    "\n",
    "parser.add_argument( '--fc', default=1024, type=int,\n",
    "    help='number of fully connected units')\n",
    "\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_network(args):\n",
    "    X = T.tensor4('input_var')\n",
    "    Y = T.ivector('target_var')\n",
    "    network = build_layers(args)\n",
    "    '''write loss function equation'''\n",
    "    prediction = get_output(network, X)\n",
    "    loss = categorical_crossentropy(prediction, Y).mean()\n",
    "    weightsl2 = regularize_network_params(network, l2)\n",
    "    loss += args['weight_decay'] * weightsl2\n",
    "    \n",
    "    '''calculate test loss (cross entropy with no regularization) and accuracy'''\n",
    "    test_prediction = get_output(network, X, deterministic=True)\n",
    "    test_loss = categorical_crossentropy(test_prediction, Y).mean()\n",
    "    \n",
    "    '''classification percentage: we can change this based on false postive/false negative criteria'''\n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), Y))\n",
    "    params = get_all_params(network, trainable=True)\n",
    "    #new_weight += momentum*prev_step  - leaarning_rate * (dL(cur_weight + momentum*prev_step)/dcur_weight) \n",
    "    updates = nesterov_momentum(loss, params, learning_rate=args['learning_rate'], momentum=args['momentum'])\n",
    "    '''train_fn -> takes in input,label pairs -> outputs loss '''\n",
    "    train_fn = theano.function([X, Y], loss, updates=updates)\n",
    "    '''val_fn -> takes in input,label pairs -> outputs non regularized loss and accuracy '''\n",
    "    val_fn = theano.function([X, Y], [test_loss, test_acc])\n",
    "\n",
    "    return {\n",
    "            'tr_fn': train_fn, \n",
    "            'val_fn': val_fn, \n",
    "            'network': network\n",
    "            \n",
    "            }\n",
    "\n",
    "def build_layers(args):\n",
    "    \n",
    "    conv_kwargs = dict(num_filters=args['num_filters'], filter_size=3, pad=1, nonlinearity=relu, W=HeNormal())\n",
    "    network = InputLayer(shape=args['input_shape'])\n",
    "    for lay in range(args['num_layers']):\n",
    "        network = batch_norm(Conv2DLayer(network, **conv_kwargs))\n",
    "        network = MaxPool2DLayer(network, pool_size=(2,2),stride=2)\n",
    "    network = dropout(network, p=args['dropout_p'])\n",
    "    network = DenseLayer(network,num_units=args['num_fc_units'], nonlinearity=relu) \n",
    "    network = dropout(network, p=args['dropout_p'])\n",
    "    network = DenseLayer(network, num_units=2, nonlinearity=softmax)\n",
    "    \n",
    "    for layer in get_all_layers(network):\n",
    "        logger.info(str(layer) + str(layer.output_shape))\n",
    "    print count_params(layer)\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    if batchsize > inputs.shape[0]:\n",
    "        batchsize=inputs.shape[0]\n",
    "    for start_idx in range(0,len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx: start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_dir = create_run_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y = load_data(num_events=args.num_ims, bin_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "inds = np.arange(x.shape[0])\n",
    "\n",
    "#shuffle data\n",
    "rng = np.random.RandomState(7)\n",
    "rng.shuffle(inds)\n",
    "\n",
    "#split train, val, test\n",
    "tr_inds = inds[:int(0.8*len(inds))] \n",
    "val_inds = inds[int(0.8*len(inds)):]\n",
    "\n",
    "x_tr, y_tr, x_val, y_val = x[tr_inds], y[tr_inds], x[val_inds], y[val_inds]\n",
    "\n",
    "\n",
    "'''a type of sparse preprocessing, which scales everything between -1 and 1 without losing sparsity'''\n",
    "#only calculate the statistic using training set\n",
    "max_abs=np.abs(x_tr).max(axis=(0,1,2,3))\n",
    "\n",
    "#then scale all sets\n",
    "x_tr /= max_abs\n",
    "x_val /= max_abs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training set size: 1600, val set size 400 \n",
      "{'dropout_p': 0, 'momentum': 0.9, 'num_filters': 128, 'learning_rate': 0.01, 'weight_decay': 0.0001, 'num_layers': 4, 'input_shape': (None, 1, 62, 100), 'num_fc_units': 1024}\n",
      "<lasagne.layers.input.InputLayer object at 0x2b246be387d0>(None, 1, 62, 100)\n",
      "<lasagne.layers.conv.Conv2DLayer object at 0x2b246be38bd0>(None, 128, 62, 100)\n",
      "<lasagne.layers.pool.MaxPool2DLayer object at 0x2b246be38b50>(None, 128, 31, 50)\n",
      "<lasagne.layers.conv.Conv2DLayer object at 0x2b246be45250>(None, 128, 31, 50)\n",
      "<lasagne.layers.pool.MaxPool2DLayer object at 0x2b246be45190>(None, 128, 15, 25)\n",
      "<lasagne.layers.conv.Conv2DLayer object at 0x2b246be45650>(None, 128, 15, 25)\n",
      "<lasagne.layers.pool.MaxPool2DLayer object at 0x2b246be45d50>(None, 128, 7, 12)\n",
      "<lasagne.layers.conv.Conv2DLayer object at 0x2b246c76e0d0>(None, 128, 7, 12)\n",
      "<lasagne.layers.pool.MaxPool2DLayer object at 0x2b246c76e410>(None, 128, 3, 6)\n",
      "<lasagne.layers.noise.DropoutLayer object at 0x2b246be38890>(None, 128, 3, 6)\n",
      "<lasagne.layers.dense.DenseLayer object at 0x2b246c76e510>(None, 1024)\n",
      "<lasagne.layers.noise.DropoutLayer object at 0x2b246c76e710>(None, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ExtendedLogger log_train at 0x2b245ba87810>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<lasagne.layers.dense.DenseLayer object at 0x2b246c76e810>(None, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2806402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:log_train:training set size: 1600, val set size 400 \r\n",
      "INFO:log_train:{'dropout_p': 0, 'momentum': 0.9, 'num_filters': 128, 'learning_rate': 0.01, 'weight_decay': 0.0001, 'num_layers': 4, 'input_shape': (None, 1, 62, 100), 'num_fc_units': 1024}\r\n",
      "INFO:log_train:<lasagne.layers.input.InputLayer object at 0x2b246be387d0>(None, 1, 62, 100)\r\n",
      "INFO:log_train:<lasagne.layers.conv.Conv2DLayer object at 0x2b246be38bd0>(None, 128, 62, 100)\r\n",
      "INFO:log_train:<lasagne.layers.pool.MaxPool2DLayer object at 0x2b246be38b50>(None, 128, 31, 50)\r\n",
      "INFO:log_train:<lasagne.layers.conv.Conv2DLayer object at 0x2b246be45250>(None, 128, 31, 50)\r\n",
      "INFO:log_train:<lasagne.layers.pool.MaxPool2DLayer object at 0x2b246be45190>(None, 128, 15, 25)\r\n",
      "INFO:log_train:<lasagne.layers.conv.Conv2DLayer object at 0x2b246be45650>(None, 128, 15, 25)\r\n",
      "INFO:log_train:<lasagne.layers.pool.MaxPool2DLayer object at 0x2b246be45d50>(None, 128, 7, 12)\r\n",
      "INFO:log_train:<lasagne.layers.conv.Conv2DLayer object at 0x2b246c76e0d0>(None, 128, 7, 12)\r\n",
      "INFO:log_train:<lasagne.layers.pool.MaxPool2DLayer object at 0x2b246c76e410>(None, 128, 3, 6)\r\n",
      "INFO:log_train:<lasagne.layers.noise.DropoutLayer object at 0x2b246be38890>(None, 128, 3, 6)\r\n",
      "INFO:log_train:<lasagne.layers.dense.DenseLayer object at 0x2b246c76e510>(None, 1024)\r\n",
      "INFO:log_train:<lasagne.layers.noise.DropoutLayer object at 0x2b246c76e710>(None, 1024)\r\n",
      "INFO:log_train:<lasagne.layers.dense.DenseLayer object at 0x2b246c76e810>(None, 2)\r\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5000\n",
    "batchsize = 128\n",
    "try:\n",
    "    print logger\n",
    "except:\n",
    "    logger = logging.getLogger('log_train')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    fh = logging.FileHandler('%s/training.log'%(run_dir))\n",
    "    fh.setLevel(logging.INFO)\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.INFO)\n",
    "    logger.addHandler(ch)\n",
    "    logger.addHandler(fh)\n",
    "    \n",
    "    \n",
    "logger.info(\"training set size: %i, val set size %i \" %( x_tr.shape[0], x_val.shape[0]))\n",
    "    \n",
    "'''set params'''\n",
    "                    # (None,1,62,100) -> none b/c batch size could vary\n",
    "network_kwargs = {'input_shape':tuple([None] + [x.shape[i] for i in [1,2,3]]), \n",
    "                  'learning_rate': args.learn_rate, \n",
    "                  'dropout_p': 0, \n",
    "                  'weight_decay': 0.0001, \n",
    "                  'num_filters': args.num_filters, \n",
    "                  'num_fc_units': args.fc,\n",
    "                  'num_layers': 4,\n",
    "                  'momentum': 0.9}\n",
    "\n",
    "logger.info(str(network_kwargs))\n",
    "\n",
    "'''get network and train_fns'''\n",
    "net_cfg = build_network(network_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "weights :  0.719\n",
      "x avg : 0.00021 shape: (128, 1, 62, 100) : iter: 0  loss :  0.896 \n",
      "weights :  0.311\n",
      "x avg : 0.00021 shape: (128, 1, 62, 100) : iter: 1  loss :  0.891 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-4b3175bbee94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterate_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#x = np.squeeze(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_cfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tr_fn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_all_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_cfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'network'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'W'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weights : %6.3f\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/cori/software/python/2.7-anaconda/envs/deeplearning/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:log_train:weights :  0.719\r\n",
      "INFO:log_train:x avg : 0.00021 shape: (128, 1, 62, 100) : iter: 0  loss :  0.896 \r\n",
      "INFO:log_train:weights :  0.311\r\n",
      "INFO:log_train:x avg : 0.00021 shape: (128, 1, 62, 100) : iter: 1  loss :  0.891 \r\n"
     ]
    }
   ],
   "source": [
    "tr_losses = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "tr_accs = []\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    start = time.time() \n",
    "    tr_loss = 0\n",
    "    tr_acc = 0\n",
    "    for iteration, (x, y) in enumerate(iterate_minibatches(x_tr,y_tr, batchsize=batchsize)):\n",
    "        #x = np.squeeze(x)\n",
    "        loss = net_cfg['tr_fn'](x, y)\n",
    "        weights = sum([np.sum(a.eval()) for a in get_all_params(net_cfg['network']) if str(a) == 'W'])\n",
    "        logger.info(\"weights : %6.3f\" %(weights))\n",
    "        logger.info(\"x avg : %5.5f shape: %s : iter: %i  loss : %6.3f \" % (np.mean(x), str(x.shape), iteration, loss))\n",
    "        _, acc = net_cfg['val_fn'](x,y)\n",
    "        tr_acc += acc\n",
    "        tr_loss += loss\n",
    "    \n",
    "    train_end = time.time()\n",
    "    tr_avgacc = tr_acc / (iteration + 1)\n",
    "    tr_avgloss = tr_loss / (iteration + 1)\n",
    "    \n",
    "    \n",
    "    logger.info(\"train time : %5.2f seconds\" % (train_end - start))\n",
    "    logger.info(\"  epoch %i of %i train loss is %f\" % (epoch, num_epochs, tr_avgloss))\n",
    "    logger.info(\"  epoch %i of %i train acc is %f percent\" % (epoch, num_epochs, tr_avgacc * 100))\n",
    "    tr_losses.append(tr_avgloss)\n",
    "    tr_accs.append(tr_avgacc)\n",
    "    \n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    for iteration, (xval, yval) in enumerate(iterate_minibatches(x_val,y_val, batchsize=batchsize)):\n",
    "        #xval = np.squeeze(xval)\n",
    "        loss, acc = net_cfg['val_fn'](xval, yval)\n",
    "        val_loss += loss\n",
    "        val_acc += acc\n",
    "    \n",
    "    val_avgloss = val_loss / (iteration + 1)\n",
    "    val_avgacc = val_acc / (iteration + 1)\n",
    "    \n",
    "    logger.info(\"val time : %5.2f seconds\" % (time.time() - train_end))\n",
    "    logger.info(\"  epoch %i of %i val loss is %f\" % (epoch, num_epochs, val_avgloss))\n",
    "    logger.info(\"  epoch %i of %i val acc is %f percent\" % (epoch, num_epochs, val_avgacc * 100))\n",
    "    \n",
    "    val_losses.append(val_avgloss)\n",
    "    val_accs.append(val_avgacc)\n",
    "    \n",
    "    plot_learn_curve(tr_losses, val_losses, save_dir=run_dir)\n",
    "    plot_learn_curve(tr_accs, val_accs, save_dir=run_dir, name=\"acc\")\n",
    "    pickle.dump(net_cfg['network'],open(run_dir + \"/model.pkl\", 'w'))\n",
    "    \n",
    "#     if epoch % 5 == 0:\n",
    "#         plot_filters(net_cfg['network'], save_dir=run_dir)\n",
    "#         for iteration, (xval, yval) in enumerate(iterate_minibatches(x_val,y_val, batchsize=batchsize)):\n",
    "#             plot_feature_maps(iteration, xval,net_cfg['network'], save_dir=run_dir)\n",
    "#             break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "deeplearning_plus_root",
   "language": "python",
   "name": "deeplearning_plus_root"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
