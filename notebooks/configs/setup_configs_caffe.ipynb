{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from nbfinder import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "import argparse\n",
    "import os\n",
    "from os.path import join\n",
    "import notebooks.load_data.data_loader_caffe as dl\n",
    "from notebooks.load_data.data_loader_caffe import DataIterator\n",
    "from notebooks.networks import binary_classifier_caffe as bc\n",
    "from notebooks.util import create_run_dir, get_logger, dump_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_args = {'input_shape': tuple([None] + [3, 224, 224]),\n",
    "                      'learning_rate': 0.00001, \n",
    "                      'dropout_p': 0.5,\n",
    "                      'leakiness': 0.1,\n",
    "                      'weight_decay': 0.0,\n",
    "                      'num_filters': 128, \n",
    "                      'num_fc_units': 1024,\n",
    "                      'num_layers': 4,\n",
    "                      'momentum': 0.9,\n",
    "                      'num_epochs': 20000,\n",
    "                      'batch_size': 128,\n",
    "                      \"save_path\": \"None\",\n",
    "                      \"num_tr\": -1,\n",
    "                      \"test\":False, \n",
    "                      \"seed\": 7,\n",
    "                      \"mode\":\"classif\",\n",
    "                      \"exp_name\": \"run\",\n",
    "                      \"load_path\": \"None\",\n",
    "                      \"num_test\": -1,\n",
    "                      \"batch_norm\": False,\n",
    "                    \"datadir\": \"/global/cscratch1/sd/tkurth/atlas_dl/data_delphes\",\n",
    "                    \"datakey\": \"data\"\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_configs():\n",
    "    \n",
    "    # if inside a notebook, then get rid of weird notebook arguments, so that arg parsing still works\n",
    "    if any([\"jupyter\" in arg for arg in sys.argv]):\n",
    "        sys.argv=sys.argv[:1]\n",
    "\n",
    "\n",
    "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    #make a command line argument for every flag in default args\n",
    "    for k,v in default_args.iteritems():\n",
    "        if type(v) is bool:\n",
    "            parser.add_argument('--' + k, action='store_true', help=k)\n",
    "        else:\n",
    "            parser.add_argument('--' + k, type=type(v), default=v, help=k)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "\n",
    "    kwargs = default_args\n",
    "    kwargs.update(args.__dict__)\n",
    "    \n",
    "    \n",
    "    kwargs = setup_res_dir(kwargs)\n",
    "    \n",
    "    kwargs = setup_iterators(kwargs)\n",
    "\n",
    "    kwargs[\"logger\"] = get_logger(kwargs['save_path'])\n",
    "    \n",
    "    #only classifier so far\n",
    "    net = bc\n",
    "        \n",
    "    kwargs[\"net\"] = net\n",
    "\n",
    "\n",
    "    #kwargs[\"num_train\"], kwargs[\"num_val\"] = trdi.hgroup[\"hist\"].shape[0], valdi.hgroup[\"hist\"].shape[0]\n",
    "    kwargs[\"logger\"].info(str(kwargs))\n",
    "    \n",
    "    dump_hyperparams(dic=kwargs,path=kwargs[\"save_path\"])\n",
    "\n",
    "\n",
    "    return kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_iterators(kwargs):\n",
    "    mainpath=kwargs['datadir']\n",
    "    trainfiles=[mainpath+'/'+x for x in os.listdir(mainpath) if x.startswith('hep_training_')]\n",
    "    validationfiles=[mainpath+'/'+x for x in os.listdir(mainpath) if x.startswith('hep_validation_')]\n",
    "    testfiles=[mainpath+'/'+x for x in os.listdir(mainpath) if x.startswith('hep_test_')]\n",
    "    \n",
    "    \n",
    "    #DEBUG\n",
    "    trainfiles=trainfiles[1:2]\n",
    "    validationfiles=validationfiles[1:2]\n",
    "    testfiles=testfiles[1:2]\n",
    "    #DEBUG\n",
    "    \n",
    "    \n",
    "    loader_kwargs = dict(batch_size=kwargs[\"batch_size\"],\n",
    "                         keys=[\"data\", \"label\", \"normweight\", \"weight\", \"psr\"])\n",
    "    kwargs[\"loader_kwargs\"] = loader_kwargs\n",
    "    kwargs[\"trainfiles\"]=trainfiles\n",
    "    kwargs[\"validationfiles\"]=validationfiles\n",
    "    kwargs[\"testfiles\"]=testfiles\n",
    "    \n",
    "    if not kwargs[\"test\"]:\n",
    "        #training\n",
    "        trdi = DataIterator(kwargs[\"trainfiles\"], batch_size=kwargs[\"batch_size\"], \n",
    "                            allow_fractional_batches=False, keys=loader_kwargs['keys'])\n",
    "        kwargs[\"train_iterator\"] = trdi\n",
    "        kwargs[\"num_train\"] = trdi.num_events\n",
    "        #validation\n",
    "        valdi = DataIterator(kwargs[\"validationfiles\"], batch_size=kwargs[\"batch_size\"], \n",
    "                             allow_fractional_batches=True, keys=loader_kwargs['keys'])\n",
    "        kwargs[\"validation_iterator\"] = valdi\n",
    "        kwargs[\"num_validation\"] = valdi.num_events\n",
    "        \n",
    "        #shape\n",
    "        kwargs[\"input_shape\"] = tuple([None] + list(trdi.hgroup[kwargs[\"datakey\"]].shape[1:]))\n",
    "    \n",
    "    else:\n",
    "        #test\n",
    "        tsdi = DataIterator(kwargs[\"testfiles\"], batch_size=kwargs[\"batch_size\"], \n",
    "                            allow_fractional_batches=True, keys=loader_kwargs['keys'])\n",
    "        kwargs[\"test_iterator\"] = tsdi\n",
    "        kwargs[\"num_test\"] = tsdi.num_events\n",
    "        \n",
    "        #shape\n",
    "        kwargs[\"input_shape\"] = tuple([None] + list(tsdi.hgroup[kwargs[\"datakey\"]].shape[1:]))\n",
    "\n",
    "    return kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_res_dir(kwargs):\n",
    "    if kwargs[\"save_path\"]== \"None\":\n",
    "        kwargs[\"save_path\"] = None\n",
    "\n",
    "    run_dir = create_run_dir(kwargs[\"save_path\"], name=kwargs[\"exp_name\"])\n",
    "    kwargs['save_path'] = run_dir\n",
    "    return kwargs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
