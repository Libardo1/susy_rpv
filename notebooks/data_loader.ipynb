{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__author__ = 'racah'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from nbfinder import NotebookFinder\n",
    "import sys\n",
    "import os\n",
    "from os.path import join, exists\n",
    "from os import makedirs, mkdir\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "%matplotlib inline\n",
    "import time\n",
    "import h5py\n",
    "#from helper_fxns import suppress_stdout_stderr\n",
    "import copy\n",
    "import pickle\n",
    "#sys.path.append('/global/homes/w/wbhimji/cori-envs/nersc-rootpy/lib/python2.7/site-packages/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shuffle(kwargs):\n",
    "    inds = np.arange(kwargs[kwargs.keys()[0]].shape[0])\n",
    "\n",
    "    #shuffle data\n",
    "    rng = np.random.RandomState(7)\n",
    "    rng.shuffle(inds)\n",
    "    return {k:v[inds] for k,v in kwargs.iteritems()}\n",
    "\n",
    "def split_train_val(prop, kwargs):\n",
    "    tr_prop = prop\n",
    "    inds = np.arange(kwargs[kwargs.keys()[0]].shape[0])\n",
    "    #split train, val, test\n",
    "    num_tr_ex = int((tr_prop*len(inds)))\n",
    "    dind = {}\n",
    "    dind[\"tr\"] = inds[:num_tr_ex]\n",
    "    #dind[\"test\"] = inds[num_tr_ex:num_tr_ex + num_val_ex]\n",
    "    \n",
    "    dind[\"val\"] = inds[num_tr_ex:]\n",
    "    \n",
    "    data = {}\n",
    "    for typ, inds in dind.iteritems():\n",
    "        data[typ] = {k:v[inds] for k,v in kwargs.iteritems()}\n",
    "        \n",
    "        \n",
    "    #dict of dicts, where key is tr, val or test and value is dict of x,y,w,psr, etc.\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(x, max_abs=None):\n",
    "    '''a type of sparse preprocessing, which scales everything between -1 and 1 without losing sparsity'''\n",
    "    #only calculate the statistic using training set\n",
    "    if max_abs is None:\n",
    "        max_abs=np.max(np.abs(x))\n",
    "\n",
    "    #then scale all sets\n",
    "    x /= max_abs\n",
    "\n",
    "    return x, max_abs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    def __init__(self, bg_cfg_file = './config/BgFileListAug16.txt',\n",
    "                sig_cfg_file='./config/SignalFileListAug16.txt',\n",
    "                type_ = \"hdf5\",\n",
    "                num_events=50000,\n",
    "                preprocess=True,\n",
    "                bin_size=0.025,\n",
    "                eta_range = [-5,5],\n",
    "                phi_range = [-3.14, 3.14], \n",
    "                tr_prop=0.8,\n",
    "                use_premade=True,\n",
    "                seed=3, \n",
    "                test = False, \n",
    "                desired_dims={\"phi\":None, \"eta\":None}):\n",
    "        \n",
    "        \n",
    "        self.desired_dims = desired_dims\n",
    "        self.bg_files = bg_cfg_file if isinstance(bg_cfg_file, list) else [bg_cfg_file]\n",
    "        self.sig_files = sig_cfg_file if isinstance(sig_cfg_file, list) else [sig_cfg_file]\n",
    "        self.all_files = self.bg_files + self.sig_files\n",
    "        self.test = test\n",
    "\n",
    "        self.tr_prop = tr_prop\n",
    "        self.eta_range = eta_range\n",
    "        self.phi_range = phi_range\n",
    "        self.bin_size = bin_size\n",
    "        self.make_bins()\n",
    "\n",
    "\n",
    "        self.seed = seed\n",
    "        assert num_events != 0, \"whoa no events?!\"\n",
    "        self.num_events = num_events\n",
    "        \n",
    "\n",
    "       \n",
    "#         self.fil_dict = self.get_file_metadata()\n",
    "\n",
    "        \n",
    "        self.use_premade = use_premade\n",
    "        self.type_ = type_\n",
    "\n",
    "#         self.file_type = \"hdf5\"\n",
    "#         self.set_h5_cfgs()\n",
    "        \n",
    "\n",
    "    def make_bins(self):\n",
    "        \n",
    "        phi_dim, eta_dim = self.desired_dims[\"phi\"], self.desired_dims[\"eta\"]\n",
    "        if phi_dim is None:\n",
    "            phi_diff = self.phi_range[1] - self.phi_range[0]\n",
    "            self.phi_bins = int(np.floor((phi_diff) / self.bin_size))\n",
    "        else:\n",
    "            self.phi_bins = phi_dim\n",
    "        if eta_dim is None:\n",
    "            eta_diff = self.eta_range[1] - self.eta_range[0]\n",
    "            self.eta_bins = int(np.floor((eta_diff) / self.bin_size))\n",
    "        else:\n",
    "            self.eta_bins = eta_dim\n",
    "        \n",
    "    \n",
    "#     def get_file_metadata(self):\n",
    "#         dirname = os.path.dirname(self.bg_files[0])\n",
    "#         fil_dict = pickle.load(open(join(dirname,\"file_max_inds.pkl\")))\n",
    "#         return fil_dict\n",
    "    \n",
    "    \n",
    "#     def set_h5_cfgs(self):\n",
    "#         self.group_prefix = \"event_\"\n",
    "#         self.h5keys = ['clusE',\n",
    "#                        'clusEta',\n",
    "#                        'clusPhi']\n",
    "         \n",
    "    \n",
    "    def grab_events(self, file_list, num_events, start=0):\n",
    "        files_dict = {k:[] for k in [\"w\", \"x\", \"psr\"]}\n",
    "        \n",
    "        if len(file_list) > 0:\n",
    "\n",
    "            num_events = num_events / len(file_list) if num_events != -1 else -1\n",
    "            for file_ in file_list:\n",
    "                files_dict = self.grab_file(file_,\n",
    "                                             num_events,\n",
    "                                             files_dict)\n",
    "\n",
    "            files_dict = self.vstack_all(files_dict)\n",
    "\n",
    "            \n",
    "            \n",
    "        return files_dict\n",
    "    \n",
    "    def grab_file(self, file_, num_events,files_dict):\n",
    "        file_dict = self._grab_hdf5_events(file_, num_events , start=0)\n",
    "        \n",
    "        for k, v in file_dict.iteritems():\n",
    "            files_dict[k].append(v)\n",
    "        \n",
    "        \n",
    "        return files_dict  #,rest_of_events_dict\n",
    "        \n",
    "\n",
    "    def _grab_hdf5_events(self,file_, num_events, start):\n",
    "        h5f = h5py.File(file_)\n",
    "        print file_\n",
    "        \n",
    "        all_events = h5f[\"all_events\"]\n",
    "        num_events_in_file = all_events[\"hist\"].shape[0]\n",
    "        if num_events == -1 or num_events > num_events_in_file:\n",
    "            num_events = num_events_in_file\n",
    "            \n",
    "        arr_slice = slice(0,num_events)\n",
    "        x,w,psr = [np.expand_dims(all_events[k][arr_slice],axis=1) for k in [\"hist\", \"weight\", \"passSR\"]]\n",
    "        \n",
    "        return dict(x=x, w=w, psr=psr)        \n",
    "    \n",
    "    def vstack_all(self, data):\n",
    "        for k,v in data.iteritems():\n",
    "            data[k] = np.vstack(tuple(v))\n",
    "        return data\n",
    "    \n",
    "    \n",
    " \n",
    "        \n",
    "            \n",
    "    def make_hist(self, d):\n",
    "        \n",
    "        return np.histogram2d(d['clusphi'],d['cluseta'], bins=(self.phi_bins, self.eta_bins),\n",
    "                              weights=d[\"cluse\"], range=[self.phi_range,self.eta_range])[0] \n",
    "\n",
    "   \n",
    "\n",
    "    def get_data_block(self):\n",
    "                        \n",
    "        #because we use weights -> we just pull the same number of events from each file\n",
    "        num_bg = int((float(self.num_events) / len(self.all_files)) * len(self.bg_files)) if self.num_events !=-1 else -1\n",
    "        num_sig = int((float(self.num_events) / len(self.all_files)) * len(self.sig_files)) if self.num_events !=-1 else -1\n",
    "        \n",
    "        bg = self.grab_events(self.bg_files, num_bg)\n",
    "        sig = self.grab_events(self.sig_files, num_sig)\n",
    "        if len(sig[sig.keys()[0]]) > 0:\n",
    "            if len(bg[bg.keys()[0]]) > 0:\n",
    "                data = {k:np.vstack((bg[k], sig[k])) for k in bg.keys()}\n",
    "            else:\n",
    "                data = sig\n",
    "        elif len(bg[bg.keys()[0]]) > 0:\n",
    "            data = bg\n",
    "        else:\n",
    "            assert False, \"you got no data\"\n",
    "        \n",
    "\n",
    "        \n",
    "        num_data_bg = len(bg[\"w\"])\n",
    "        num_data_sig = len(sig[\"w\"])\n",
    "\n",
    "        \n",
    "        # 1 means signal, 0 means background\n",
    "        data[\"y\"] = np.zeros((num_data_bg + num_data_sig)).astype('int32')\n",
    "\n",
    "        \n",
    "        #make the last half signal label\n",
    "        data[\"y\"][num_data_bg:] = 1\n",
    "   \n",
    "        return data\n",
    "        \n",
    "\n",
    "    def load_data(self):\n",
    "        t = time.time()\n",
    "        data = self.get_data_block()\n",
    "        print time.time() - t\n",
    "        data = shuffle(data)\n",
    "        if not self.test:\n",
    "            data = split_train_val(self.tr_prop, data)\n",
    "        data = self.preprocess(data, keys=[\"x\", \"w\"])\n",
    "\n",
    "            \n",
    "        \n",
    "\n",
    "        \n",
    "        return data\n",
    "\n",
    "    \n",
    "    def preprocess(self, data, keys=[\"x\"]):\n",
    "        \n",
    "        if not self.test:\n",
    "            for k in keys:\n",
    "                data[\"tr\"][k],tm = preprocess(data[\"tr\"][k])\n",
    "                data[\"val\"][k], _ = preprocess(data[\"val\"][k],tm)\n",
    "        else:\n",
    "            for k in keys:\n",
    "                data[k], _  = preprocess(data[k]) \n",
    "            data = {\"test\":data}\n",
    "        return data\n",
    "        \n",
    "    def iterate_data(self, batch_size=128):\n",
    "#         if self.num_each < batch_size / 2:\n",
    "#             batch_size = 2 * self.num_each\n",
    "#         #only support for hdf5\n",
    "#         for i in range(0, self.num_each, batch_size / 2):\n",
    "#             x_bg = self.grab_events(self.bg_files, batch_size / 2, i)\n",
    "#             x_sig = self.grab_events(self.sig_files, batch_size / 2, i)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#does same thing as DataLoader, but makes sure train set is all bg\n",
    "class AnomalyLoader(DataLoader):\n",
    "    def __init__(self, **kwargs):\n",
    "        DataLoader.__init__(self, **kwargs)\n",
    "    \n",
    "    \n",
    "    def load_data(self):\n",
    "        data = self.get_data_block()\n",
    "        data = shuffle(data)\n",
    "        if not self.test:\n",
    "            data = self.split_train_val_anom(self.tr_prop, data)\n",
    "        data = self.preprocess(data)\n",
    "        return data\n",
    "\n",
    "            \n",
    "        \n",
    "\n",
    "        \n",
    "    def split_train_val_anom(self,prop, data):\n",
    "        tr_prop = prop\n",
    "        \n",
    "        all_inds = np.arange(data[data.keys()[0]].shape[0])\n",
    "        bg_inds = all_inds[data[\"y\"] == 0]\n",
    "        sig_inds = all_inds[data[\"y\"] == 1]\n",
    "        \n",
    "        #split train, val\n",
    "        #only use background for train\n",
    "        num_tr_ex = int((tr_prop*len(bg_inds)))\n",
    "        \n",
    "        dind = {}\n",
    "        dind[\"tr\"] = bg_inds[:num_tr_ex]\n",
    "\n",
    "\n",
    "        dind[\"val\"] = np.concatenate((bg_inds[num_tr_ex:], sig_inds))\n",
    "\n",
    "        final_data = {}\n",
    "        for typ, inds in dind.iteritems():\n",
    "            final_data[typ] = {k:v[inds] for k,v in data.iteritems()}\n",
    "\n",
    "\n",
    "        #dict of dicts, where key is tr, val or test and value is dict of x,y,w,psr, etc.\n",
    "        return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_train_test_files(file_path_list, test_prop=0.2):\n",
    "    \n",
    "    def add_to_file(file_name, data_dict):\n",
    "        f = h5py.File(file_name, \"w\")\n",
    "        group = f.create_group(\"all_events\")\n",
    "        for k in data_dict:\n",
    "            group[k] = data_dict[k]\n",
    "        f.close()\n",
    "        \n",
    "    for file_path in file_path_list:\n",
    "        print file_path\n",
    "        h5f = h5py.File(file_path)\n",
    "        all_events = h5f[\"all_events\"]\n",
    "        num_events = all_events[\"hist\"].shape[0]\n",
    "        \n",
    "        num_test = int(test_prop * num_events)\n",
    "        \n",
    "        test_file_name = join(os.path.dirname(file_path),\"test_\" + os.path.basename(file_path))\n",
    "        train_file_name = join(os.path.dirname(file_path),\"train_\" + os.path.basename(file_path))\n",
    "        \n",
    "        inds = np.arange(num_events)\n",
    "        np.random.RandomState(11).shuffle(inds)\n",
    "        raw_data = {k:all_events[k][:] for k in all_events.keys()}\n",
    "        te_data = {k:raw_data[k][inds[:num_test]] for k in all_events.keys()}\n",
    "        tr_data = {k:raw_data[k][inds[num_test:]] for k in all_events.keys()}\n",
    "        add_to_file(test_file_name, te_data)\n",
    "        add_to_file(train_file_name, tr_data)\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_split():\n",
    "    h5_prefix = \"/global/cscratch1/sd/racah/atlas_h5\"\n",
    "    bg_cfg_file=[join(h5_prefix, \"jetjet_JZ%i.h5\"% (i)) for i in range(3,12)]\n",
    "    sig_cfg_file=[join(h5_prefix, \"GG_RPV10_1400_850.h5\")]\n",
    "    file_list = bg_cfg_file + sig_cfg_file\n",
    "    split_train_test_files(file_path_list=file_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/cscratch1/sd/racah/atlas_h5/jetjet_JZ3.h5\n",
      "/global/cscratch1/sd/racah/atlas_h5/jetjet_JZ4.h5\n",
      "/global/cscratch1/sd/racah/atlas_h5/jetjet_JZ5.h5\n",
      "/global/cscratch1/sd/racah/atlas_h5/jetjet_JZ6.h5\n",
      "/global/cscratch1/sd/racah/atlas_h5/jetjet_JZ7.h5\n",
      "/global/cscratch1/sd/racah/atlas_h5/jetjet_JZ8.h5\n",
      "/global/cscratch1/sd/racah/atlas_h5/jetjet_JZ9.h5\n",
      "/global/cscratch1/sd/racah/atlas_h5/jetjet_JZ10.h5\n",
      "/global/cscratch1/sd/racah/atlas_h5/jetjet_JZ11.h5\n",
      "/global/cscratch1/sd/racah/atlas_h5/GG_RPV10_1400_850.h5\n",
      "0.43409204483\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    run_split()\n",
    "#     h5_prefix = \"/global/cscratch1/sd/racah/atlas_h5\"\n",
    "    \n",
    "\n",
    "#     dl = DataLoader(bg_cfg_file=[join(h5_prefix, \"jetjet_JZ%i.h5\"% (i)) for i in range(3,12)],\n",
    "#                     sig_cfg_file=join(h5_prefix, \"GG_RPV10_1400_850.h5\"),\n",
    "#                num_events=10000, \n",
    "#                type_=\"hdf5\",\n",
    "#               use_premade=True, test=False, bin_size=0.1)\n",
    "\n",
    "#     data= dl.load_data()\n",
    "#     h5_prefix = \"/global/cscratch1/sd/racah/atlas_h5/\"\n",
    "#     bg_cfg_file=[join(h5_prefix, \"jetjet_JZ%i.h5\"% (i)) for i in range(6,12)]\n",
    "#     sig_cfg_file=[join(h5_prefix, \"GG_RPV10_1400_850.h5\")]\n",
    "\n",
    "#     split_train_test_files(bg_cfg_file + sig_cfg_file)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
