{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = 'tkurth'\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from nbfinder import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "%matplotlib inline\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theano modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import theano.sandbox.rng_mrg\n",
    "Trng = theano.sandbox.rng_mrg.MRG_RandomStreams(9)\n",
    "import lasagne as ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROOT stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "require(['codemirror/mode/clike/clike'], function(Clike) { console.log('ROOTaaS - C++ CodeMirror module loaded'); });"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "IPython.CodeCell.config_defaults.highlight_modes['magic_text/x-c++src'] = {'reg':[/^%%cpp/]};"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to ROOTaaS 6.06/06\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('/global/homes/w/wbhimji/cori-envs/nersc-rootpy/lib/python2.7/site-packages/')\n",
    "sys.path.append('/global/common/cori/software/root/6.06.06/lib/root')\n",
    "import ROOT\n",
    "import rootpy\n",
    "import root_numpy as rnp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a context manager to suppress stdout and stderr.\n",
    "class suppress_stdout_stderr(object):\n",
    "    '''\n",
    "    A context manager for doing a \"deep suppression\" of stdout and stderr in \n",
    "    Python, i.e. will suppress all print, even if the print originates in a \n",
    "    compiled C/Fortran sub-function.\n",
    "       This will not suppress raised exceptions, since exceptions are printed\n",
    "    to stderr just before a script exits, and after the context manager has\n",
    "    exited (at least, I think that is why it lets exceptions through).      \n",
    "\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # Open a pair of null files\n",
    "        self.null_fds =  [os.open(os.devnull,os.O_RDWR) for x in range(2)]\n",
    "        # Save the actual stdout (1) and stderr (2) file descriptors.\n",
    "        self.save_fds = (os.dup(1), os.dup(2))\n",
    "\n",
    "    def __enter__(self):\n",
    "        # Assign the null pointers to stdout and stderr.\n",
    "        os.dup2(self.null_fds[0],1)\n",
    "        os.dup2(self.null_fds[1],2)\n",
    "\n",
    "    def __exit__(self, *_):\n",
    "        # Re-assign the real stdout/stderr back to (1) and (2)\n",
    "        os.dup2(self.save_fds[0],1)\n",
    "        os.dup2(self.save_fds[1],2)\n",
    "        # Close the null files\n",
    "        os.close(self.null_fds[0])\n",
    "        os.close(self.null_fds[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader and preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(bg_cfg_file = '../config/BgFileListAug16.txt',\n",
    "                sig_cfg_file='../config/SignalFileListAug16.txt',\n",
    "                num_files=10,  \n",
    "                group_name='CollectionTree',\n",
    "                branches=['CaloCalTopoClustersAuxDyn.calPhi', \\\n",
    "                          'CaloCalTopoClustersAuxDyn.calEta', \\\n",
    "                          'CaloCalTopoClustersAuxDyn.calE'],\n",
    "                dataset_name='histo',\n",
    "                type_='root'):\n",
    "\n",
    "    #get list of files\n",
    "    bg_files = [line.rstrip() for line in open(bg_cfg_file)]\n",
    "    sig_files = [line.rstrip() for line in open(sig_cfg_file)]\n",
    "    \n",
    "    #so we don't have annoying stderr messages\n",
    "    with suppress_stdout_stderr():\n",
    "            \n",
    "        #bgarray has n_events groups of 3 parallel numpy arrays \n",
    "        #(each numpy within a group is of equal length and each array corresponds to phi, eta and the corresponding energy)\n",
    "        bgarray = rnp.root2array(bg_files[:num_files], \\\n",
    "                                treename=group_name, \\\n",
    "                                branches=branches, \\\n",
    "                                start=0, \\\n",
    "                                warn_missing_tree=True)\n",
    "\n",
    "        sigarray = rnp.root2array(sig_files[:num_files],\\\n",
    "                                treename=group_name,\\\n",
    "                                branches=branches,\\\n",
    "                                start=0, \\\n",
    "                                warn_missing_tree=True)\n",
    "        \n",
    "    #create dataframe with all entries\n",
    "    #store in dataframe\n",
    "    bgdf = pd.DataFrame.from_records(bgarray)\n",
    "    bgdf['label']=0\n",
    "    sigdf = pd.DataFrame.from_records(sigarray)\n",
    "    sigdf['label']=1\n",
    "    \n",
    "    #concat\n",
    "    return pd.concat([bgdf,sigdf])\n",
    "\n",
    "\n",
    "#preprocessor\n",
    "def preprocess_autoenc_data(df,num_resamplings,eta_range,phi_range,eta_bins,phi_bins):\n",
    "    #empty array\n",
    "    xvals = np.zeros((df.shape[0]*num_resamplings, 1, phi_bins, eta_bins ),dtype='float32')\n",
    "    yvals = np.zeros((df.shape[0]*num_resamplings,),dtype='int32')\n",
    "    \n",
    "    for i in range(df.shape[0]):        \n",
    "        phi, eta, E =  df.iloc[i]['CaloCalTopoClustersAuxDyn.calPhi'],\\\n",
    "                       df.iloc[i]['CaloCalTopoClustersAuxDyn.calEta'],\\\n",
    "                       df.iloc[i]['CaloCalTopoClustersAuxDyn.calE']\n",
    "        \n",
    "        start = i * num_resamplings\n",
    "        stop = (i+1) * num_resamplings\n",
    "        \n",
    "        #x is histogrammed\n",
    "        xtmp = np.histogram2d(phi,eta,\n",
    "                            bins=(phi_bins, eta_bins), \\\n",
    "                            weights=E,\n",
    "                            range=[phi_range,eta_range])[0]\n",
    "        xvals[start:stop] = xtmp\n",
    "        \n",
    "        #y is simple:\n",
    "        ytmp = np.zeros((num_resamplings),dtype='int32')\n",
    "        ytmp.fill(df.iloc[i]['label'])\n",
    "        yvals[start:stop] = ytmp[:]\n",
    "        \n",
    "    return xvals, yvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class hep_autoenc_data_iterator:\n",
    "    \n",
    "    #class constructor\n",
    "    def __init__(self,\n",
    "                 datadf,\n",
    "                 num_resamplings,\n",
    "                 max_frequency=None,\n",
    "                 shuffle=True,\n",
    "                 bin_size=0.025,\n",
    "                 eta_range = [-5,5],\n",
    "                 phi_range = [-3.14, 3.14]\n",
    "                ):\n",
    "\n",
    "        #set parameters\n",
    "        self.num_resamplings = num_resamplings\n",
    "        self.shuffle = shuffle\n",
    "        self.bin_size = bin_size\n",
    "        self.eta_range = eta_range\n",
    "        self.phi_range = phi_range\n",
    "        \n",
    "        #compute bins\n",
    "        self.eta_bins = int(np.floor((self.eta_range[1] - self.eta_range[0]) / self.bin_size))\n",
    "        self.phi_bins = int(np.floor((self.phi_range[1] - self.phi_range[0]) / self.bin_size))\n",
    "        \n",
    "        #dataframe\n",
    "        self.df = datadf\n",
    "        self.df.sort_values(by='label',inplace=True)\n",
    "        \n",
    "        #make class frequencies even:\n",
    "        tmpdf=self.df.groupby('label').count().reset_index()\n",
    "        self.num_classes=tmpdf.shape[0]\n",
    "        \n",
    "        #determine minimum frequency\n",
    "        min_frequency=tmpdf['CaloCalTopoClustersAuxDyn.calE'].min()\n",
    "        if max_frequency:\n",
    "            min_frequency=np.min([min_frequency,max_frequency])\n",
    "        tmpdf=self.df.groupby(['label']).apply(lambda x: x[['CaloCalTopoClustersAuxDyn.calPhi', \\\n",
    "                                                            'CaloCalTopoClustersAuxDyn.calEta', \\\n",
    "                                                            'CaloCalTopoClustersAuxDyn.calE']].iloc[:min_frequency,:]).copy()\n",
    "        tmpdf.reset_index(inplace=True)\n",
    "        del tmpdf['level_1']\n",
    "        self.df=tmpdf.copy()\n",
    "        \n",
    "        #compute max:\n",
    "        self.compute_data_max()\n",
    "        \n",
    "        #shuffle if wanted (highly recommended)\n",
    "        if self.shuffle:\n",
    "            self.df=self.df.reindex(np.random.permutation(self.df.index))\n",
    "        \n",
    "        #number of examples\n",
    "        self.num_examples=self.df.shape[0]\n",
    "        \n",
    "        #shapes:\n",
    "        self.xshape=(1, self.phi_bins, self.eta_bins)\n",
    "        \n",
    "    \n",
    "    #compute max over all data\n",
    "    def compute_data_max(self):\n",
    "        '''compute the maximum over all event entries for rescaling data between -1 and 1'''\n",
    "        self.max_abs=(self.df['CaloCalTopoClustersAuxDyn.calE'].abs()).apply(lambda x: np.max(x)).max()\n",
    "    \n",
    "    \n",
    "    #this is the batch iterator:\n",
    "    def next_batch(self,batchsize,num_units_latent):\n",
    "        '''batch iterator'''\n",
    "        \n",
    "        #shuffle:\n",
    "        if self.shuffle:\n",
    "            self.df=self.df.reindex(np.random.permutation(self.df.index))\n",
    "        \n",
    "        #iterate\n",
    "        for idx in range(0,self.num_examples-batchsize,batchsize):\n",
    "            #yield next batch\n",
    "            x,y=preprocess_autoenc_data(self.df.iloc[idx:idx+batchsize,:], self.num_resamplings, \\\n",
    "                             self.eta_range,\n",
    "                             self.phi_range,\n",
    "                             self.eta_bins,self.phi_bins)\n",
    "            #rescale x:\n",
    "            x/=self.max_abs\n",
    "            \n",
    "            #compute epsilon\n",
    "            eps=np.random.normal(size=(batchsize*self.num_resamplings,num_units_latent))\n",
    "            \n",
    "            #return result\n",
    "            yield x,y,eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct data iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/w/wbhimji/cori-envs/nersc-rootpy/lib/python2.7/site-packages/root_numpy/_tree.py:209: RuntimeWarning: tree 'CollectionTree' not found in /global/projecta/projectdirs/atlas/atlaslocalgroupdisk/rucio/mc15_13TeV/14/bb/DAOD_EXOT3.08597479._000007.pool.root.1\n",
      "  warn_missing_tree)\n",
      "/global/common/cori/software/python/2.7-anaconda/envs/deeplearning/lib/python2.7/site-packages/ipykernel/__main__.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#parameters\n",
    "train_fraction=0.8\n",
    "binsize=0.1\n",
    "numfiles=2\n",
    "num_resamplings=4\n",
    "\n",
    "#load data\n",
    "datadf=load_data(num_files=numfiles)\n",
    "\n",
    "#create views for different labels\n",
    "sigdf=datadf[ datadf.label==1 ]\n",
    "bgdf=datadf[ datadf.label==0 ]\n",
    "\n",
    "#split the sets\n",
    "num_bg_train=int(np.floor(bgdf.shape[0]*train_fraction))\n",
    "traindf_bg=bgdf.iloc[:num_bg_train]\n",
    "validdf_bg=bgdf.iloc[num_bg_train:]\n",
    "validdf_sig=sigdf\n",
    "\n",
    "#create iterators\n",
    "hditer_train_bg=hep_autoenc_data_iterator(traindf_bg,num_resamplings,max_frequency=4000,bin_size=binsize)\n",
    "hditer_validation_bg=hep_autoenc_data_iterator(validdf_bg,num_resamplings,max_frequency=1000,bin_size=binsize)\n",
    "hditer_validation_sig=hep_autoenc_data_iterator(validdf_sig,num_resamplings,max_frequency=1000,bin_size=binsize)\n",
    "\n",
    "#the preprocessing for the validation iterator has to be taken from the training iterator\n",
    "hditer_validation_bg.max_abs=hditer_train_bg.max_abs\n",
    "hditer_validation_sig.max_abs=hditer_train_bg.max_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print hditer_train_bg.num_examples\n",
    "print hditer_validation_bg.num_examples\n",
    "print hditer_validation_sig.num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some useful definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#KL divergence\n",
    "def KLdiv(z_mu, z_log_sigma):\n",
    "    return -0.5 * (1. + 2. * z_log_sigma - z_mu**2 - T.exp(2. * z_log_sigma)).sum(1)\n",
    "\n",
    "\n",
    "class ReparamLayer(ls.layers.MergeLayer):\n",
    "    \"\"\"Layer for reparametrization trick: order of parameters: eps, mu, log_std.\n",
    "    Computes mu + sigma * eps, so the result ~N(mu,sigma) if eps~N(0,1)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, incomings, **kwargs):\n",
    "        super(ReparamLayer, self).__init__(incomings, **kwargs)\n",
    "\n",
    "    def get_output_for(self, inputs, **kwargs):\n",
    "        return inputs[1] + inputs[0] * T.exp(inputs[2])\n",
    "\n",
    "    def get_output_shape_for(self, input_shapes):\n",
    "        return (input_shapes[0])\n",
    "\n",
    "    \n",
    "#class TileLayer(ls.layers.Layer):\n",
    "#    \"\"\"Layer for tiling a given layer accoding to reps:\n",
    "#    \"\"\"\n",
    "#    def __init__(self, incoming, reps, **kwargs):\n",
    "#        super(TileLayer, self).__init__(incoming, **kwargs)\n",
    "#        self.reps = reps\n",
    "#        \n",
    "#    def get_output_for(self, input, **kwargs):\n",
    "#        return T.tile(input,self.reps)\n",
    "#\n",
    "#    def get_output_shape_for(self, input_shape):\n",
    "#        assert len(self.reps) == len(input_shape)\n",
    "#        return (self.reps*input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct autoencoder network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#some parameters\n",
    "keep_prob=0.5\n",
    "num_filters=128\n",
    "num_units_latent=10\n",
    "initial_learning_rate=0.005\n",
    "\n",
    "#input layer\n",
    "l_inp_data = ls.layers.InputLayer((None,hditer_train_bg.xshape[0],\\\n",
    "                                   hditer_train_bg.xshape[1],\\\n",
    "                                   hditer_train_bg.xshape[2]))\n",
    "l_inp_eps = ls.layers.InputLayer((None,num_units_latent))\n",
    "\n",
    "#conv layers\n",
    "#encoder 1:\n",
    "l_conv1 = ls.layers.Conv2DLayer(incoming=l_inp_data,\n",
    "                                num_filters=num_filters,\n",
    "                                filter_size=3,\n",
    "                                stride=(1,1),\n",
    "                                pad=0,\n",
    "                                W=ls.init.HeUniform(),\n",
    "                                b=ls.init.Constant(0.),\n",
    "                                nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                               )\n",
    "l_drop1 = ls.layers.DropoutLayer(incoming=l_conv1,\n",
    "                       p=keep_prob,\n",
    "                       rescale=True\n",
    "                      )\n",
    "l_pool1 = ls.layers.MaxPool2DLayer(incoming=l_drop1,\n",
    "                                   pool_size=(2,2),\n",
    "                                   stride=2,\n",
    "                                   pad=0                                   \n",
    "                                  )\n",
    "\n",
    "#encoder 2:\n",
    "l_conv2 = ls.layers.Conv2DLayer(incoming=l_pool1,\n",
    "                                num_filters=num_filters,\n",
    "                                filter_size=3,\n",
    "                                stride=(1,1),\n",
    "                                pad=0,\n",
    "                                W=ls.init.HeUniform(),\n",
    "                                b=ls.init.Constant(0.),\n",
    "                                nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                               )\n",
    "l_drop2 = ls.layers.DropoutLayer(incoming=l_conv2,\n",
    "                       p=keep_prob,\n",
    "                       rescale=True\n",
    "                      )\n",
    "l_pool2 = ls.layers.MaxPool2DLayer(incoming=l_drop2,\n",
    "                                   pool_size=(2,2),\n",
    "                                   stride=2,\n",
    "                                   pad=0                                   \n",
    "                                  )\n",
    "\n",
    "#encoder 3:\n",
    "l_conv3 = ls.layers.Conv2DLayer(incoming=l_pool2,\n",
    "                                num_filters=num_filters,\n",
    "                                filter_size=3,\n",
    "                                stride=(1,1),\n",
    "                                pad=0,\n",
    "                                W=ls.init.HeUniform(),\n",
    "                                b=ls.init.Constant(0.),\n",
    "                                nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                               )\n",
    "l_drop3 = ls.layers.DropoutLayer(incoming=l_conv3,\n",
    "                       p=keep_prob,\n",
    "                       rescale=True\n",
    "                      )\n",
    "l_pool3 = ls.layers.MaxPool2DLayer(incoming=l_drop3,\n",
    "                                   pool_size=(2,2),\n",
    "                                   stride=2,\n",
    "                                   pad=0                                   \n",
    "                                  )\n",
    "\n",
    "#encoder 4:\n",
    "l_conv4 = ls.layers.Conv2DLayer(incoming=l_pool3,\n",
    "                                num_filters=num_filters,\n",
    "                                filter_size=3,\n",
    "                                stride=(1,1),\n",
    "                                pad=0,\n",
    "                                W=ls.init.HeUniform(),\n",
    "                                b=ls.init.Constant(0.),\n",
    "                                nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                               )\n",
    "l_drop4 = ls.layers.DropoutLayer(incoming=l_conv4,\n",
    "                       p=keep_prob,\n",
    "                       rescale=True\n",
    "                      )\n",
    "l_pool4 = ls.layers.MaxPool2DLayer(incoming=l_drop4,\n",
    "                                   pool_size=(2,2),\n",
    "                                   stride=2,\n",
    "                                   pad=0                         \n",
    "                                  )\n",
    "\n",
    "#flatten\n",
    "l_flatten = ls.layers.FlattenLayer(incoming=l_pool4,\n",
    "                                outdim=2)\n",
    "\n",
    "l_project = ls.layers.DenseLayer(incoming=l_flatten, \n",
    "                             num_units=num_units_latent, \n",
    "                             W=ls.init.GlorotUniform(np.sqrt(2./(1+0.01**2))), \n",
    "                             b=ls.init.Constant(0.0),\n",
    "                             nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                            )\n",
    "\n",
    "#sampling layer\n",
    "l_z_mean = ls.layers.DenseLayer(incoming=l_project, \n",
    "                             num_units=num_units_latent, \n",
    "                             W=ls.init.GlorotUniform(np.sqrt(2./(1+0.01**2))), \n",
    "                             b=ls.init.Constant(0.0),\n",
    "                             nonlinearity=ls.nonlinearities.identity\n",
    "                            )\n",
    "l_z_log_sigma = ls.layers.DenseLayer(incoming=l_project, \n",
    "                             num_units=num_units_latent, \n",
    "                             W=ls.init.GlorotUniform(np.sqrt(2./(1+0.01**2))), \n",
    "                             b=ls.init.Constant(0.0),\n",
    "                             nonlinearity=ls.nonlinearities.identity\n",
    "                            )\n",
    "l_z = ReparamLayer(incomings=[l_inp_eps, l_z_mean, l_z_log_sigma])\n",
    "\n",
    "#unproject\n",
    "l_unproject = ls.layers.InverseLayer(incoming=l_z, layer=l_project)\n",
    "\n",
    "#deflatten\n",
    "#l_unflatten = ls.layers.ReshapeLayer(incoming=l_unproject,\n",
    "#                                    shape=([0],1,l_drop4.output_shape[2],l_drop4.output_shape[3]))\n",
    "l_unflatten = ls.layers.InverseLayer(incoming=l_unproject, layer=l_flatten)\n",
    "\n",
    "#decoder 4\n",
    "l_unpool4 = ls.layers.InverseLayer(incoming=l_unflatten,\n",
    "                                   layer=l_pool4)\n",
    "l_deconv4 = ls.layers.TransposedConv2DLayer(incoming=l_unpool4,\n",
    "                                            num_filters=num_filters,\n",
    "                                            filter_size=3,\n",
    "                                            stride=(1,1),\n",
    "                                            W=ls.init.HeUniform(),\n",
    "                                            b=ls.init.Constant(0.),\n",
    "                                            nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                                           )\n",
    "\n",
    "#decoder 3\n",
    "l_unpool3 = ls.layers.InverseLayer(incoming=l_deconv4,\n",
    "                                   layer=l_pool3)\n",
    "l_deconv3 = ls.layers.TransposedConv2DLayer(incoming=l_unpool3,\n",
    "                                            num_filters=num_filters,\n",
    "                                            filter_size=3,\n",
    "                                            stride=(1,1),\n",
    "                                            W=ls.init.HeUniform(),\n",
    "                                            b=ls.init.Constant(0.),\n",
    "                                            nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                                           )\n",
    "\n",
    "#decoder 2\n",
    "l_unpool2 = ls.layers.InverseLayer(incoming=l_deconv3,\n",
    "                                   layer=l_pool2)\n",
    "l_deconv2 = ls.layers.TransposedConv2DLayer(incoming=l_unpool2,\n",
    "                                            num_filters=num_filters,\n",
    "                                            filter_size=3,\n",
    "                                            stride=(1,1),\n",
    "                                            W=ls.init.HeUniform(),\n",
    "                                            b=ls.init.Constant(0.),\n",
    "                                            nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                                           )\n",
    "\n",
    "#decoder 1\n",
    "l_unpool1 = ls.layers.InverseLayer(incoming=l_deconv2,\n",
    "                                   layer=l_pool1)\n",
    "l_out = ls.layers.TransposedConv2DLayer(incoming=l_unpool1,\n",
    "                                            num_filters=1,\n",
    "                                            filter_size=3,\n",
    "                                            stride=(1,1),\n",
    "                                            W=ls.init.HeUniform(),\n",
    "                                            b=ls.init.Constant(0.),\n",
    "                                            nonlinearity=ls.nonlinearities.identity\n",
    "                                           )\n",
    "\n",
    "#network\n",
    "network = [l_inp_data, l_inp_eps,\n",
    "           l_conv1, l_pool1, l_drop1,\n",
    "           l_conv2, l_pool2, l_drop2,\n",
    "           l_conv3, l_pool3, l_drop3,\n",
    "           l_conv4, l_pool4, l_drop4,\n",
    "           l_project, l_flatten,\n",
    "           l_z_mean, l_z_log_sigma, l_z,\n",
    "           l_unproject, l_unflatten,\n",
    "           l_unpool4, l_deconv4,\n",
    "           l_unpool3, l_deconv3,\n",
    "           l_unpool2, l_deconv2,\n",
    "           l_unpool1, l_out\n",
    "          ]\n",
    "\n",
    "#variables\n",
    "inp = l_inp_data.input_var\n",
    "eps = l_inp_eps.input_var\n",
    "\n",
    "#output from the whole network\n",
    "l_pred_data = ls.layers.get_output(l_out, {l_inp_data: inp, l_inp_eps: eps})\n",
    "l_pred_data_det = ls.layers.get_output(l_out, {l_inp_data: inp, l_inp_eps: eps}, deterministic=True)\n",
    "\n",
    "#output for the sampling layer\n",
    "l_out_z_mean = ls.layers.get_output(l_z_mean,{l_inp_data: inp})\n",
    "l_out_z_log_sigma = ls.layers.get_output(l_z_log_sigma,{l_inp_data: inp})\n",
    "\n",
    "#loss functions:\n",
    "klloss = KLdiv(l_out_z_mean, l_out_z_log_sigma).mean()\n",
    "celoss = ls.objectives.squared_error(l_pred_data,inp).mean()\n",
    "celoss_det = ls.objectives.squared_error(l_pred_data_det,inp).mean()\n",
    "loss = celoss + klloss\n",
    "loss_det = celoss_det\n",
    "\n",
    "#parameters\n",
    "params = ls.layers.get_all_params(network, trainable=True)\n",
    "\n",
    "#updates\n",
    "updates = ls.updates.adam(loss, params, learning_rate=initial_learning_rate)\n",
    "\n",
    "#compile network function\n",
    "fnn = theano.function([inp,eps], l_pred_data_det)\n",
    "#training function to minimize\n",
    "fnn_train = theano.function([inp,eps], loss, updates=updates)\n",
    "#validation function with accuracy\n",
    "fnn_validate = theano.function([inp,eps], loss_det)\n",
    "#generator\n",
    "#fnn_generate = theano.function([eps], ls.layers.get_output(l_out, {l_inp_eps: eps}, deterministic=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  0.706114989405\n",
      "train:  3.41144622068\n",
      "train:  2.28303934698\n",
      "train:  1.71959253176\n",
      "train:  1.38256145038\n",
      "train:  1.15943105567\n",
      "train:  1.00109859576\n",
      "train:  0.882038901114\n",
      "train:  0.788557565435\n",
      "train:  0.713028744354\n",
      "train:  0.650499666101\n",
      "train:  0.597866193803\n",
      "train:  0.552999182831\n",
      "train:  0.514345314645\n",
      "train:  0.480716178059\n",
      "train:  0.451211055269\n",
      "train:  0.425125566704\n",
      "train:  0.401910719997\n",
      "train:  0.381119743765\n",
      "train:  0.362388851377\n",
      "train:  0.345424934695\n",
      "train:  0.329986507042\n",
      "train:  0.315875964388\n",
      "train:  0.302929047367\n",
      "train:  0.291005752203\n",
      "train:  0.279989122955\n",
      "train:  0.269778414996\n",
      "train:  0.26028671223\n",
      "train:  0.251440110733\n",
      "train:  0.243174884659\n",
      "train:  0.235434294913\n",
      "Epoch 1 of 10 took 3604.536s\n",
      "  training loss:\t\t0.235434\n",
      "  validation loss (bg):\t\t0.000353\n",
      "  validation loss (sig):\t\t0.000439\n",
      "train:  0.00297035252424\n",
      "train:  0.0028505504001\n",
      "train:  0.00273489432921\n",
      "train:  0.00262543829762\n",
      "train:  0.00251990251709\n",
      "train:  0.00241954811565\n",
      "train:  0.00232322321798\n",
      "train:  0.00223202531536\n",
      "train:  0.00214489028332\n",
      "train:  0.00206269895774\n",
      "train:  0.00198502444365\n",
      "train:  0.00191215295628\n",
      "train:  0.00184344259688\n",
      "train:  0.00177793623644\n",
      "train:  0.00171604631241\n",
      "train:  0.00165725766934\n",
      "train:  0.00160245388235\n",
      "train:  0.00155083597498\n",
      "train:  0.00150222245909\n",
      "train:  0.00145612244968\n",
      "train:  0.00141260500685\n",
      "train:  0.00137167450067\n",
      "train:  0.00133307738791\n",
      "train:  0.00129687691831\n",
      "train:  0.00126255590524\n",
      "train:  0.00122996732543\n",
      "train:  0.00119906499448\n",
      "train:  0.00116976023596\n",
      "train:  0.00114171529961\n",
      "train:  0.00111543678218\n",
      "train:  0.00109056471967\n",
      "Epoch 2 of 10 took 3600.297s\n",
      "  training loss:\t\t0.001091\n",
      "  validation loss (bg):\t\t0.000175\n",
      "  validation loss (sig):\t\t0.000015\n",
      "train:  0.00032383056126\n",
      "train:  0.000319341165933\n",
      "train:  0.000313208216864\n",
      "train:  0.000307240580494\n",
      "train:  0.000302254393329\n",
      "train:  0.000296845190277\n",
      "train:  0.00029115942295\n",
      "train:  0.000286760663919\n",
      "train:  0.000282386119451\n",
      "train:  0.000278258276533\n",
      "train:  0.000274579795433\n",
      "train:  0.000270498836693\n",
      "train:  0.000267152609412\n",
      "train:  0.000263450264852\n",
      "train:  0.000260027712516\n",
      "train:  0.000256880545025\n",
      "train:  0.000254008456505\n",
      "train:  0.000251229222838\n",
      "train:  0.000248267514944\n",
      "train:  0.000245291506427\n",
      "train:  0.000242353851918\n",
      "train:  0.00024005193801\n",
      "train:  0.000237899992735\n",
      "train:  0.000235738618198\n",
      "train:  0.000234183423621\n",
      "train:  0.000232134149742\n",
      "train:  0.000230365149032\n",
      "train:  0.000228818277882\n",
      "train:  0.000227213422384\n",
      "train:  0.000225599421765\n",
      "train:  0.000224021689954\n",
      "Epoch 3 of 10 took 3615.425s\n",
      "  training loss:\t\t0.000224\n",
      "  validation loss (bg):\t\t0.000169\n",
      "  validation loss (sig):\t\t0.000010\n",
      "train:  0.00017230176938\n",
      "train:  0.000173543526046\n",
      "train:  0.000175200236778\n",
      "train:  0.000176878604172\n",
      "train:  0.000175692133583\n",
      "train:  0.000176503641568\n",
      "train:  0.000176385590218\n",
      "train:  0.000176419442554\n",
      "train:  0.000176379837722\n",
      "train:  0.000176048869152\n",
      "train:  0.000176038714584\n",
      "train:  0.000176094325689\n",
      "train:  0.000175613703354\n",
      "train:  0.000175712884485\n",
      "train:  0.000175630679218\n",
      "train:  0.000175623756206\n",
      "train:  0.000175574635735\n",
      "train:  0.000175150819742\n",
      "train:  0.000174943748593\n",
      "train:  0.000174819441799\n",
      "train:  0.00017500188915\n",
      "train:  0.00017484881038\n",
      "train:  0.000174765389045\n",
      "train:  0.000174493687078\n",
      "train:  0.000174312524005\n",
      "train:  0.000174267913948\n",
      "train:  0.000174087678027\n",
      "train:  0.000173892496882\n",
      "train:  0.000173882008469\n",
      "train:  0.000173768389357\n",
      "train:  0.000173954516896\n",
      "Epoch 4 of 10 took 3652.100s\n",
      "  training loss:\t\t0.000174\n",
      "  validation loss (bg):\t\t0.000170\n",
      "  validation loss (sig):\t\t0.000010\n",
      "train:  0.000176015776777\n",
      "train:  0.000173960739369\n",
      "train:  0.000175433635548\n",
      "train:  0.000174845505801\n",
      "train:  0.000173702200543\n",
      "train:  0.000173498645167\n",
      "train:  0.000173826032946\n",
      "train:  0.000174484419602\n",
      "train:  0.000174354879203\n",
      "train:  0.000174509111336\n",
      "train:  0.000174153827777\n",
      "train:  0.000174202492911\n",
      "train:  0.000173818825897\n",
      "train:  0.0001736970788\n",
      "train:  0.0001735944065\n",
      "train:  0.000173513821969\n",
      "train:  0.000173103287987\n",
      "train:  0.000173026546944\n",
      "train:  0.000173058141863\n",
      "train:  0.000172804629901\n",
      "train:  0.000172862324769\n",
      "train:  0.000172685126815\n",
      "train:  0.000172591650661\n",
      "train:  0.000172234552063\n",
      "train:  0.00017217848475\n",
      "train:  0.000172024644673\n",
      "train:  0.0001722301898\n",
      "train:  0.000172327568657\n",
      "train:  0.000172155310434\n",
      "train:  0.000171930636731\n",
      "train:  0.000171887615999\n",
      "Epoch 5 of 10 took 3666.956s\n",
      "  training loss:\t\t0.000172\n",
      "  validation loss (bg):\t\t0.000170\n",
      "  validation loss (sig):\t\t0.000010\n",
      "train:  0.000174248212716\n",
      "train:  0.00017177779871\n",
      "train:  0.000171939176543\n",
      "train:  0.000172870401398\n",
      "train:  0.000171798297815\n",
      "train:  0.000171826734177\n",
      "train:  0.000171386813146\n",
      "train:  0.000171755082081\n",
      "train:  0.000171943761841\n",
      "train:  0.000172365506948\n",
      "train:  0.000172323953431\n",
      "train:  0.000172828458366\n",
      "train:  0.000172345758457\n",
      "train:  0.000172537625282\n",
      "train:  0.000172516659714\n",
      "train:  0.000172186626877\n",
      "train:  0.000172230017142\n",
      "train:  0.000171859319827\n",
      "train:  0.000171714770586\n",
      "train:  0.000171867359144\n",
      "train:  0.000171631737096\n",
      "train:  0.000171864833784\n",
      "train:  0.000171735547645\n",
      "train:  0.000171640055614\n",
      "train:  0.000171925450662\n",
      "train:  0.000171843231154\n",
      "train:  0.000171796916053\n",
      "train:  0.00017185107487\n",
      "train:  0.000171870727344\n",
      "train:  0.0001718305452\n",
      "train:  0.00017180363338\n",
      "Epoch 6 of 10 took 3587.629s\n",
      "  training loss:\t\t0.000172\n",
      "  validation loss (bg):\t\t0.000169\n",
      "  validation loss (sig):\t\t0.000010\n",
      "train:  0.000164257810494\n",
      "train:  0.000166311960663\n",
      "train:  0.00016782887571\n",
      "train:  0.000167371272273\n",
      "train:  0.000168765568669\n",
      "train:  0.00016876048562\n",
      "train:  0.000169055205013\n",
      "train:  0.000169784548189\n",
      "train:  0.000170183645689\n",
      "train:  0.000170645741955\n",
      "train:  0.000170288559592\n",
      "train:  0.000170518016965\n",
      "train:  0.000170937507156\n",
      "train:  0.000171051706088\n",
      "train:  0.000171183618247\n",
      "train:  0.000171231000386\n",
      "train:  0.000171092160382\n",
      "train:  0.000171163268169\n",
      "train:  0.000171319587451\n",
      "train:  0.000171354075342\n",
      "train:  0.000171141359803\n",
      "train:  0.000171324130689\n",
      "train:  0.000171658813596\n",
      "train:  0.000171812502733\n",
      "train:  0.000171927206494\n",
      "train:  0.000171930532987\n",
      "train:  0.000172058928584\n",
      "train:  0.00017197667437\n",
      "train:  0.000171881433072\n",
      "train:  0.000171781503555\n",
      "train:  0.000171679913349\n",
      "Epoch 7 of 10 took 3604.801s\n",
      "  training loss:\t\t0.000172\n",
      "  validation loss (bg):\t\t0.000170\n",
      "  validation loss (sig):\t\t0.000010\n",
      "train:  0.000171766223383\n",
      "train:  0.000170502659134\n",
      "train:  0.000170693457972\n",
      "train:  0.000171730420705\n",
      "train:  0.000171311363492\n",
      "train:  0.000171447102961\n",
      "train:  0.000171446819455\n",
      "train:  0.00017163499034\n",
      "train:  0.000171634883023\n",
      "train:  0.000171761004769\n",
      "train:  0.000172117307515\n",
      "train:  0.00017177062015\n",
      "train:  0.000171833267578\n",
      "train:  0.000171641372275\n",
      "train:  0.00017192023433\n",
      "train:  0.0001718694528\n",
      "train:  0.000171709997789\n",
      "train:  0.000171579670176\n",
      "train:  0.000171489492527\n",
      "train:  0.00017124900344\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1917e655cece>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhditer_train_bg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_units_latent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mtrain_err\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfnn_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mtrain_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/cori/software/python/2.7-anaconda/envs/deeplearning/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs=10\n",
    "batchsize=128\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0.\n",
    "    train_batches = 0.\n",
    "    start_time = time.time()\n",
    "    for batch in hditer_train_bg.next_batch(batchsize,num_units_latent):\n",
    "        inputs, targets, epsilon = batch        \n",
    "        train_err += fnn_train(inputs,epsilon)\n",
    "        train_batches += 1.\n",
    "        \n",
    "        #debugging output\n",
    "        print 'train: ', train_err/train_batches\n",
    "        \n",
    "    # And a full pass over the validation data for background and signal:\n",
    "    val_bg_err = 0.\n",
    "    val_bg_batches = 0.\n",
    "    for batch in hditer_validation_bg.next_batch(batchsize,num_units_latent):\n",
    "        inputs, targets, epsilon = batch\n",
    "        val_bg_err += fnn_validate(inputs,epsilon)\n",
    "        val_bg_batches += 1.\n",
    "    \n",
    "    val_sig_err = 0.\n",
    "    val_sig_batches = 0.\n",
    "    for batch in hditer_validation_sig.next_batch(batchsize,num_units_latent):\n",
    "        inputs, targets, epsilon = batch\n",
    "        val_sig_err += fnn_validate(inputs,epsilon)\n",
    "        val_sig_batches += 1.\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  validation loss (bg):\\t\\t{:.6f}\".format(val_bg_err / val_bg_batches))\n",
    "    print(\"  validation loss (sig):\\t\\t{:.6f}\".format(val_sig_err / val_sig_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNG Debugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = np.zeros((4,10))\n",
    "mu[:]=1.\n",
    "std = np.zeros((4,10))\n",
    "std[:]=3.\n",
    "\n",
    "z0 = np.zeros((4,10)) \n",
    "z1 = np.zeros((4,10))\n",
    "z1[:,:] = 1.\n",
    "eps=T.matrix('eps')\n",
    "zmean0 = theano.shared(z0, name='zmean0')\n",
    "zstd0 = theano.shared(z1, name='zstd0')\n",
    "zmean = theano.shared(mu, name='zmean')\n",
    "zstd = theano.shared(std, name='zstd')\n",
    "zrng = zmean + zstd * eps #Trng.normal(size=zmean0.shape, avg=zmean0, std=zstd0)\n",
    "zrng2 = zmean + zstd * Trng.normal(size=zmean0.shape)\n",
    "f_theano = theano.function([eps], zrng)\n",
    "f_theano2 = theano.function([], zrng2)\n",
    "\n",
    "zmt=T.matrix('zmt')\n",
    "zst=T.matrix('zst')\n",
    "f_KL = theano.function([zmt,zst],KLdiv(zmt,zst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr=[]\n",
    "arr2=[]\n",
    "for n in range(10000):\n",
    "    eps=np.random.normal(loc=0., scale=1., size=(4,10))\n",
    "    arr.append(f_theano(eps)[0,0])\n",
    "    arr2.append(f_theano2()[0,0])\n",
    "n, bins, patches = plt.hist(arr, 100, normed=1, facecolor='green', alpha=0.75)\n",
    "y = mlab.normpdf( bins, 1, 3)\n",
    "l = plt.plot(bins, y, 'r--', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [deeplearning]",
   "language": "python",
   "name": "Python [deeplearning]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
