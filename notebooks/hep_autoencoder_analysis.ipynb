{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__author__ = 'tkurth'\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.stats import cumfreq\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from nbfinder import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "%matplotlib inline\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_cdf(data, weights=None, normalize=True):\n",
    "    \n",
    "    #sort the values first\n",
    "    if weights:\n",
    "        data=np.vstack([data,weights])\n",
    "    else:\n",
    "        data=np.vstack([data,np.ones(len(data))])\n",
    "    data=np.sort(data,axis=1)\n",
    "    \n",
    "    #normalize if requested\n",
    "    if normalize:\n",
    "        data[1][:]=data[1][:]/np.sum(data[1][:])\n",
    "    \n",
    "    #integrate\n",
    "    X=data[0][:]\n",
    "    Y=[data[1][0]]    \n",
    "    for i in range(1,data.shape[1]):\n",
    "        Y.append(Y[i-1]+data[1][i])\n",
    "    Y=np.asarray(Y)\n",
    "    \n",
    "    #return result\n",
    "    return X,Y\n",
    "\n",
    "\n",
    "#compute quantiles:\n",
    "def get_quantile(cdf, ptarget):\n",
    "\n",
    "    #invert CDF\n",
    "    idx=0\n",
    "    pcurrent=cdf[1][idx]\n",
    "    while (pcurrent<ptarget) and (idx<len(cdf[1][:])):\n",
    "        idx+=1\n",
    "        pcurrent=cdf[1][idx]\n",
    "\n",
    "    #report x when done\n",
    "    return cdf[0][idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Error Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Fitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#retrieve files\n",
    "directory=\"/global/cscratch1/sd/tkurth/atlas_dl/atlas_caffe/autoencoder/runs_preselect_autoencoder\"\n",
    "filelist=[x for x in os.listdir(directory) if x.startswith('output_fit_chunk')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate Fitting Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fitdatalist=[]\n",
    "for fname in tqdm(filelist):\n",
    "    \n",
    "    #determine chunkid:\n",
    "    chunkid=int(re.match(r'^.*?chunk(.*?).h5',fname).groups()[0])\n",
    "    \n",
    "    #print \"Open file \"+fname\n",
    "    f = h5py.File(directory+'/'+fname,'r')\n",
    "    \n",
    "    tmpdata={}\n",
    "    for item in f.items():\n",
    "        \n",
    "        #determine name and ID of item\n",
    "        itemname=item[0].split('_')[0]\n",
    "        itemid=int(item[0].split('_')[1])\n",
    "        \n",
    "        #read data\n",
    "        data=list(f[item[0]].value)\n",
    "        \n",
    "        #add to dictionary\n",
    "        if (chunkid,itemid) not in tmpdata.keys():\n",
    "            tmpdata[(chunkid,itemid)]={itemname: data}\n",
    "        else:\n",
    "            tmpdata[(chunkid,itemid)][itemname]=data\n",
    "            \n",
    "    #close the file\n",
    "    f.close()\n",
    "    \n",
    "    #put in list:\n",
    "    for item in tmpdata:\n",
    "        dct=tmpdata[item].copy()\n",
    "        dct['chunk_id']=item[0]\n",
    "        dct['item_id']=item[1]\n",
    "        fitdatalist.append(dct) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Convert to stacked array:\n",
    "p_data=[]\n",
    "w_data=[]\n",
    "for item in fitdatalist:\n",
    "    p_data+=item['loss']\n",
    "    w_data+=item['weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot CDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X,Y=calc_cdf(p_data,w_data)\n",
    "plt.plot(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compute quantiles:\n",
    "pvalue_target=0.999\n",
    "\n",
    "#report x when done\n",
    "x_target=get_quantile(np.vstack([X,Y]),pvalue_target)\n",
    "\n",
    "#print the value\n",
    "print \"Exclusion threshold: \",x_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#retrieve files\n",
    "directory=\"/global/cscratch1/sd/tkurth/atlas_dl/atlas_caffe/autoencoder/runs_preselect_autoencoder\"\n",
    "filelist=[x for x in os.listdir(directory) if x.startswith('output_chunk')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datalist=[]\n",
    "\n",
    "for fname in tqdm(filelist):\n",
    "    \n",
    "    #determine chunkid:\n",
    "    chunkid=int(re.match(r'output_chunk(.*?).h5',fname).groups()[0])\n",
    "    \n",
    "    #print \"Open file \"+fname\n",
    "    f = h5py.File(directory+'/'+fname,'r')\n",
    "    \n",
    "    tmpdata={}\n",
    "    for item in f.items():\n",
    "        \n",
    "        #determine name and ID of item\n",
    "        itemname=item[0].split('_')[0]\n",
    "        itemid=int(item[0].split('_')[1])\n",
    "        \n",
    "        #read data\n",
    "        data=list(f[item[0]].value)\n",
    "        \n",
    "        #add to dictionary\n",
    "        if (chunkid,itemid) not in tmpdata.keys():\n",
    "            tmpdata[(chunkid,itemid)]={itemname: data}\n",
    "        else:\n",
    "            tmpdata[(chunkid,itemid)][itemname]=data\n",
    "    \n",
    "    #close the file\n",
    "    f.close()\n",
    "    \n",
    "    #put in list:\n",
    "    for item in tmpdata:\n",
    "        dct=tmpdata[item].copy()\n",
    "        dct['chunk_id']=item[0]\n",
    "        dct['item_id']=item[1]\n",
    "        datalist.append(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#unfold datalist\n",
    "cleandatalist=[]\n",
    "for idx,item in enumerate(datalist):\n",
    "    p_data=item['loss']\n",
    "    w_data=item['weight']\n",
    "    mg_data=item['mg']\n",
    "    mn_data=item['mn']\n",
    "    psr_data=item['psr']\n",
    "    \n",
    "    for idy in range(len(p_data)):\n",
    "        if psr_data[idy]>0.:\n",
    "            psr_val=True\n",
    "        else:\n",
    "            psr_val=False\n",
    "        \n",
    "        cleandatalist.append(\n",
    "                            {'loss': p_data[idy],\n",
    "                             'weight': w_data[idy],\n",
    "                             'mg': mg_data[idy],\n",
    "                             'mn': mn_data[idy],\n",
    "                             'psr': psr_val\n",
    "                            }\n",
    "                            )\n",
    "\n",
    "#convert to stacked arrays:\n",
    "datadf=pd.DataFrame(cleandatalist)\n",
    "datadf.sort_values(by=['mg','mn'],inplace=True)\n",
    "datadf.reset_index(drop=True,inplace=True)\n",
    "\n",
    "#split in background and signal sets\n",
    "bgdf=datadf.ix[ (datadf.mg==0.) & (datadf.mn==0.) ].copy()\n",
    "sigdf=datadf.ix[ (datadf.mg>0.) | (datadf.mn>0.) ].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#threshold\n",
    "threshold=0.01\n",
    "\n",
    "#compute background rejection\n",
    "bval_per=bgdf['weight'].ix[ bgdf.loss<threshold ].sum()\n",
    "bval_psr=bgdf['weight'].ix[ bgdf.psr==True ].sum()\n",
    "\n",
    "#compute signal efficiencies\n",
    "#PER\n",
    "resultdf=pd.DataFrame(sigdf.groupby(['mg','mn']).apply(lambda x: x['weight'].ix[ x.loss<threshold ].sum()))\n",
    "resultdf.reset_index(inplace=True)\n",
    "resultdf.rename(columns={0:'sval_per'},inplace=True)\n",
    "#PSR\n",
    "tmpdf=pd.DataFrame(sigdf.groupby(['mg','mn']).apply(lambda x: x['weight'].ix[ x.psr==True ].sum()))\n",
    "tmpdf.reset_index(inplace=True)\n",
    "tmpdf.rename(columns={0:'sval_psr'},inplace=True)\n",
    "\n",
    "#merge the two\n",
    "resultdf=resultdf.merge(tmpdf,on=['mg','mn'], how='left')\n",
    "\n",
    "#compute AMS\n",
    "brval=10.\n",
    "\n",
    "#print AMS results\n",
    "resultdf[\"ams_per\"]=np.sqrt(2.*((resultdf.sval_per+bval_per+brval)*np.log(1.+resultdf.sval_per/(bval_per+brval))-resultdf.sval_per))\n",
    "resultdf[\"ams_psr\"]=np.sqrt(2.*((resultdf.sval_psr+bval_psr+brval)*np.log(1.+resultdf.sval_psr/(bval_psr+brval))-resultdf.sval_psr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compute ROC from data\n",
    "fpr, tpr, thresholds = metrics.roc_curve(l_data, p_data, pos_label=1, sample_weight=w_data)\n",
    "fpr_cut, tpr_cut, thresholds = metrics.roc_curve(l_data, c_data, pos_label=1, sample_weight=w_data)\n",
    "fpr_cut=fpr_cut[1]\n",
    "tpr_cut=tpr_cut[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot the data\n",
    "plt.figure()\n",
    "lw = 2\n",
    "#full curve\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % metrics.auc(fpr,tpr,reorder=True))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.scatter([fpr_cut],[tpr_cut], color='dodgerblue', label='standard cuts')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('plots/ROC_1400_850.png',dpi=300)\n",
    "\n",
    "#zoomed-in\n",
    "#plt.plot(fpr, tpr, color='darkorange',\n",
    "#         lw=lw, label='ROC curve (area = %0.2f)' % metrics.auc(fpr,tpr))\n",
    "#plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 0.0002])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.savefig('plots/ROC_1400_850_zoom.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set threshold\n",
    "threshold=0.5\n",
    "\n",
    "#compute signal efficiency:\n",
    "total_signal=np.sum(l_data)\n",
    "found_signal=np.sum([1. if x[0]*x[1]>threshold else 0. for x in zip(l_data,p_data)])\n",
    "print 'sig-efficiency(CNN): ',found_signal/total_signal\n",
    "print 'sig-survivors(CNN): ',found_signal\n",
    "\n",
    "#compute bg rejection:\n",
    "total_background=np.sum([1.-x for x in l_data])\n",
    "false_positive_bg=np.sum([1. if (1.-x[0])*x[1]>threshold else 0. for x in zip(l_data,p_data)])\n",
    "print 'bg-rejection(CNN): ',1.-false_positive_bg/total_background\n",
    "print 'bg-survivors(CNN): ',(1.-(1.-false_positive_bg/total_background))*total_background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#AMS\n",
    "threshold=0.8\n",
    "\n",
    "#compute s:\n",
    "sval=np.sum([x[2] if x[0]*x[1]>threshold else 0. for x in zip(l_data,p_data,w_data)])\n",
    "bval=np.sum([x[2] if (1.-x[0])*x[1]>threshold else 0. for x in zip(l_data,p_data,w_data)])\n",
    "brval=10.\n",
    "\n",
    "#print AMS results\n",
    "print \"AMS(CNN) = \",np.sqrt(2.*((sval+bval+brval)*np.log(1.+sval/(bval+brval))-sval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cut-based results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set threshold\n",
    "threshold=0.9\n",
    "\n",
    "#compute signal efficiency:\n",
    "total_signal=np.sum(l_data)\n",
    "found_signal=np.sum([1. if x[0]*x[1]>0. else 0. for x in zip(l_data,c_data)])\n",
    "print 'sig-efficiency(CUT): ',found_signal/total_signal\n",
    "print 'sig-survivors(CUT): ',found_signal\n",
    "\n",
    "#compute bg rejection:\n",
    "total_background=np.sum([1.-x for x in l_data])\n",
    "false_positive_bg=np.sum([1. if (1.-x[0])*x[1]>0. else 0. for x in zip(l_data,c_data)])\n",
    "print 'bg-rejection(CUT): ',1.-false_positive_bg/total_background\n",
    "print 'bg-survivors(CUT): ',(1.-(1.-false_positive_bg/total_background))*total_background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#AMS\n",
    "threshold=0.8\n",
    "\n",
    "#compute s:\n",
    "sval=np.sum([x[2] if x[0]*x[1]>threshold else 0. for x in zip(l_data,c_data,w_data)])\n",
    "bval=np.sum([x[2] if (1.-x[0])*x[1]>threshold else 0. for x in zip(l_data,c_data,w_data)])\n",
    "brval=10.\n",
    "\n",
    "#print AMS results\n",
    "print \"AMS(CUT) = \",np.sqrt(2.*((sval+bval+brval)*np.log(1.+sval/(bval+brval))-sval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [thorstendl]",
   "language": "python",
   "name": "Python [thorstendl]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
