{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__author__ = 'tkurth'\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from nbfinder import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "%matplotlib inline\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theano modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import theano.sandbox.rng_mrg\n",
    "Trng = theano.sandbox.rng_mrg.MRG_RandomStreams(9)\n",
    "import lasagne as ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROOT stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "require(['codemirror/mode/clike/clike'], function(Clike) { console.log('ROOTaaS - C++ CodeMirror module loaded'); });"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "IPython.CodeCell.config_defaults.highlight_modes['magic_text/x-c++src'] = {'reg':[/^%%cpp/]};"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to ROOTaaS 6.06/06\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('/global/homes/w/wbhimji/cori-envs/nersc-rootpy/lib/python2.7/site-packages/')\n",
    "sys.path.append('/global/common/cori/software/root/6.06.06/lib/root')\n",
    "import ROOT\n",
    "import rootpy\n",
    "import root_numpy as rnp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a context manager to suppress stdout and stderr.\n",
    "class suppress_stdout_stderr(object):\n",
    "    '''\n",
    "    A context manager for doing a \"deep suppression\" of stdout and stderr in \n",
    "    Python, i.e. will suppress all print, even if the print originates in a \n",
    "    compiled C/Fortran sub-function.\n",
    "       This will not suppress raised exceptions, since exceptions are printed\n",
    "    to stderr just before a script exits, and after the context manager has\n",
    "    exited (at least, I think that is why it lets exceptions through).      \n",
    "\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # Open a pair of null files\n",
    "        self.null_fds =  [os.open(os.devnull,os.O_RDWR) for x in range(2)]\n",
    "        # Save the actual stdout (1) and stderr (2) file descriptors.\n",
    "        self.save_fds = (os.dup(1), os.dup(2))\n",
    "\n",
    "    def __enter__(self):\n",
    "        # Assign the null pointers to stdout and stderr.\n",
    "        os.dup2(self.null_fds[0],1)\n",
    "        os.dup2(self.null_fds[1],2)\n",
    "\n",
    "    def __exit__(self, *_):\n",
    "        # Re-assign the real stdout/stderr back to (1) and (2)\n",
    "        os.dup2(self.save_fds[0],1)\n",
    "        os.dup2(self.save_fds[1],2)\n",
    "        # Close the null files\n",
    "        os.close(self.null_fds[0])\n",
    "        os.close(self.null_fds[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader and preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_dicts(dict1,dict2):\n",
    "    tmp = dict1.copy()\n",
    "    tmp.update(dict2)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#file string parser\n",
    "def parse_filename(fname,directory='.'):\n",
    "    directory=re.sub(r'^(.*?)(/+)$',r'\\1',directory)\n",
    "    \n",
    "    #signal file?\n",
    "    smatch=re.compile(r'GG_RPV(.*?)_(.*?)_(.*?)\\.merge')\n",
    "    tmpres=smatch.findall(fname)\n",
    "    if tmpres:\n",
    "        tmpres=tmpres[0]\n",
    "        return {'rpv':int(tmpres[0]), 'mass1':int(tmpres[1]), 'mass2':int(tmpres[2]), 'name':directory+'/'+fname}\n",
    "\n",
    "    #background file?\n",
    "    smatch=re.compile(r'JZ(.*?)\\.merge')\n",
    "    tmpres=smatch.findall(fname)\n",
    "    if tmpres:\n",
    "        return {'jz':int(tmpres[0]), 'name':directory+'/'+fname}\n",
    "\n",
    "    #nothing at all\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(filelists,\n",
    "                group_name='CollectionTree',\n",
    "                branches=['CaloCalTopoClustersAuxDyn.calPhi', \\\n",
    "                          'CaloCalTopoClustersAuxDyn.calEta', \\\n",
    "                          'CaloCalTopoClustersAuxDyn.calE'],\n",
    "                dataset_name='histo',\n",
    "                type_='root'):\n",
    "    \n",
    "    #iterate over elements in the filelists\n",
    "    records=[]\n",
    "    for fname in filelists:\n",
    "        #read specifics of that list\n",
    "        masterrec=parse_filename(fname.split('/')[-1])\n",
    "        #determine if it is label or background\n",
    "        if 'jz' in masterrec.keys():\n",
    "            masterrec['label']=0\n",
    "        else:\n",
    "            masterrec['label']=1\n",
    "        \n",
    "        #read the files in the filelist\n",
    "        files = [line.rstrip() for line in open(fname)]\n",
    "        \n",
    "        #we don't want annoying stderr messages\n",
    "        with suppress_stdout_stderr():\n",
    "            \n",
    "            #bgarray has n_events groups of 3 parallel numpy arrays \n",
    "            #(each numpy within a group is of equal length and each array corresponds to phi, eta and the corresponding energy)\n",
    "            try:\n",
    "                datarec = rnp.root2array(files, \\\n",
    "                                        treename=group_name, \\\n",
    "                                        branches=branches, \\\n",
    "                                        start=0, \\\n",
    "                                        warn_missing_tree=True)\n",
    "                tmpdf=pd.DataFrame.from_records(datarec)\n",
    "                reclist=tmpdf[['CaloCalTopoClustersAuxDyn.calPhi', \\\n",
    "                                'CaloCalTopoClustersAuxDyn.calEta', \\\n",
    "                                'CaloCalTopoClustersAuxDyn.calE']].to_dict('records')\n",
    "                reclist=[merge_dicts(masterrec,rec) for rec in reclist]\n",
    "                    \n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "        #append to records\n",
    "        records+=reclist\n",
    "            \n",
    "    #return dataframe\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "#preprocessor\n",
    "def preprocess_data(df,eta_range,phi_range,eta_bins,phi_bins):\n",
    "    #empty array\n",
    "    xvals = np.zeros((df.shape[0], 1, phi_bins, eta_bins ),dtype='float32')\n",
    "    yvals = np.zeros((df.shape[0],),dtype='int32')\n",
    "    \n",
    "    for i in range(df.shape[0]):        \n",
    "        phi, eta, E =  df.iloc[i]['CaloCalTopoClustersAuxDyn.calPhi'],\\\n",
    "                       df.iloc[i]['CaloCalTopoClustersAuxDyn.calEta'],\\\n",
    "                       df.iloc[i]['CaloCalTopoClustersAuxDyn.calE']\n",
    "        \n",
    "        xvals[i]=np.histogram2d(phi,eta,\n",
    "                                bins=(phi_bins, eta_bins), \\\n",
    "                                weights=E,\n",
    "                                range=[phi_range,eta_range])[0]\n",
    "        yvals[i]=df.iloc[i]['label']\n",
    "        \n",
    "    return xvals, yvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class hep_data_iterator:\n",
    "    \n",
    "    #class constructor\n",
    "    def __init__(self,\n",
    "                 datadf,\n",
    "                 max_frequency=None,\n",
    "                 even_frequencies=True,\n",
    "                 shuffle=True,\n",
    "                 nbins=(100,100),\n",
    "                 eta_range = [-5,5],\n",
    "                 phi_range = [-3.1416, 3.1416]\n",
    "                ):\n",
    "\n",
    "        #set parameters\n",
    "        self.shuffle = shuffle\n",
    "        self.nbins = nbins\n",
    "        self.eta_range = eta_range\n",
    "        self.phi_range = phi_range\n",
    "        self.even_frequencies = even_frequencies\n",
    "        \n",
    "        #compute bins\n",
    "        self.eta_bins = self.nbins[0]\n",
    "        self.phi_bins = self.nbins[1]\n",
    "        \n",
    "        #dataframe\n",
    "        self.df = datadf\n",
    "        self.df.sort_values(by='label',inplace=True)\n",
    "        \n",
    "        #make class frequencies even:\n",
    "        tmpdf=self.df.groupby('label').count().reset_index()\n",
    "        self.num_classes=tmpdf.shape[0]\n",
    "        \n",
    "        #determine minimum frequency\n",
    "        min_frequency=tmpdf['CaloCalTopoClustersAuxDyn.calE'].min()\n",
    "        if max_frequency:\n",
    "            min_frequency=np.min([min_frequency,max_frequency])\n",
    "        elif not even_frequencies:\n",
    "            min_frequency=-1\n",
    "        tmpdf=self.df.groupby(['label']).apply(lambda x: x[['CaloCalTopoClustersAuxDyn.calPhi', \\\n",
    "                                                            'CaloCalTopoClustersAuxDyn.calEta', \\\n",
    "                                                            'CaloCalTopoClustersAuxDyn.calE']].iloc[:min_frequency,:]).copy()\n",
    "        tmpdf.reset_index(inplace=True)\n",
    "        del tmpdf['level_1']\n",
    "        \n",
    "        #copy tmpdf into self.df\n",
    "        self.df=tmpdf.copy()\n",
    "        \n",
    "        #compute max:\n",
    "        self.compute_data_max()\n",
    "        \n",
    "        #shuffle if wanted (highly recommended)\n",
    "        if self.shuffle:\n",
    "            self.df=self.df.reindex(np.random.permutation(self.df.index))\n",
    "        \n",
    "        #number of examples\n",
    "        self.num_examples=self.df.shape[0]\n",
    "        \n",
    "        #shapes:\n",
    "        self.xshape=(1, self.phi_bins, self.eta_bins)\n",
    "        \n",
    "    \n",
    "    #compute max over all data\n",
    "    def compute_data_max(self):\n",
    "        '''compute the maximum over all event entries for rescaling data between -1 and 1'''\n",
    "        self.max_abs=(self.df['CaloCalTopoClustersAuxDyn.calE'].abs()).apply(lambda x: np.max(x)).max()\n",
    "    \n",
    "    \n",
    "    #this is the batch iterator:\n",
    "    def next_batch(self,batchsize):\n",
    "        '''batch iterator'''\n",
    "        \n",
    "        #shuffle:\n",
    "        if self.shuffle:\n",
    "            self.df=self.df.reindex(np.random.permutation(self.df.index))\n",
    "        \n",
    "        #iterate\n",
    "        for idx in range(0,self.num_examples-batchsize,batchsize):\n",
    "            #yield next batch\n",
    "            x,y=preprocess_data(self.df.iloc[idx:idx+batchsize,:],\\\n",
    "                             self.eta_range,\n",
    "                             self.phi_range,\n",
    "                             self.eta_bins,self.phi_bins)\n",
    "            #rescale x:\n",
    "            x/=self.max_abs\n",
    "        \n",
    "            #return result\n",
    "            yield x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curate File list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directory='/project/projectdirs/das/wbhimji/RPVSusyJetLearn/atlas_dl/config/'\n",
    "filelists=[parse_filename(x,directory) for x in os.listdir(directory) if x.startswith('mc')]\n",
    "filenamedf=pd.DataFrame(filelists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#select signal configuration\n",
    "mass1=1400\n",
    "mass2=850\n",
    "sig_cfg_files=list(filenamedf[ (filenamedf['mass1']==1400) & (filenamedf['mass2']==850) ]['name'])\n",
    "\n",
    "#select background configuration\n",
    "jzmin=4\n",
    "jzmax=5\n",
    "bg_cfg_files=list(filenamedf[ (filenamedf['jz']>=jzmin) & (filenamedf['jz']<=jzmax) ]['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct data iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load background files\n",
    "bgdf=load_data(bg_cfg_files)\n",
    "bgdf=bgdf.reindex(np.random.permutation(bgdf.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load signal data\n",
    "sigdf=load_data(sig_cfg_files)\n",
    "sigdf=sigdf.reindex(np.random.permutation(sigdf.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#parameters\n",
    "train_fraction=0.8\n",
    "validation_fraction=0.1\n",
    "nbins=(100,100)\n",
    "\n",
    "#create sizes:\n",
    "num_sig_train=int(np.floor(sigdf.shape[0]*train_fraction))\n",
    "#num_bg_train=int(np.floor(bgdf.shape[0]*train_fraction))\n",
    "num_bg_train=num_sig_train\n",
    "num_sig_validation=int(np.floor(sigdf.shape[0]*validation_fraction))\n",
    "#num_bg_validation=int(np.floor(bgdf.shape[0]*validation_fraction))\n",
    "num_bg_validation=num_sig_validation\n",
    "\n",
    "#split the sets\n",
    "traindf=pd.concat([bgdf.iloc[:num_bg_train],sigdf.iloc[:num_sig_train]])\n",
    "validdf=pd.concat([bgdf.iloc[num_bg_train:num_bg_train+num_bg_validation], \\\n",
    "                   sigdf.iloc[num_sig_train:num_sig_train+num_sig_validation]])\n",
    "testdf=pd.concat([bgdf.iloc[num_bg_train+num_bg_validation:], \\\n",
    "                   sigdf.iloc[num_sig_train+num_sig_validation:]])\n",
    "\n",
    "#create iterators\n",
    "hditer_train=hep_data_iterator(traindf,nbins=nbins)\n",
    "hditer_validation=hep_data_iterator(validdf,nbins=nbins)\n",
    "hditer_test=hep_data_iterator(testdf,nbins=nbins,even_frequencies=False)\n",
    "\n",
    "#the preprocessing for the validation iterator has to be taken from the training iterator\n",
    "hditer_validation.max_abs=hditer_train.max_abs\n",
    "hditer_test.max_abs=hditer_train.max_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n",
      "4000\n",
      "3165384\n"
     ]
    }
   ],
   "source": [
    "print hditer_train.num_examples\n",
    "print hditer_validation.num_examples\n",
    "print hditer_test.num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Matthews correlation coefficient objective, only for binary classifications\n",
    "def matthews_correlation_coefficient(predictions, targets):\n",
    "    #preprocess\n",
    "    if targets.ndim == predictions.ndim:\n",
    "        targets = T.argmax(targets, axis=-1)\n",
    "    #make predictions flat as well:\n",
    "    predictions = T.argmax(predictions, axis=-1)\n",
    "    \n",
    "    #true predictions\n",
    "    true_pred=T.eq(predictions, targets)\n",
    "    #false predictions\n",
    "    false_pred=T.neq(predictions, targets)\n",
    "    \n",
    "    #true positives:\n",
    "    tp=(true_pred*predictions).sum()\n",
    "    #false positives:\n",
    "    fp=(false_pred*predictions).sum()\n",
    "    #true negatives\n",
    "    tn=(true_pred*(1-predictions)).sum()\n",
    "    #false negatives\n",
    "    fn=(false_pred*(1-predictions)).sum()\n",
    "    \n",
    "    #now, assemble ratio\n",
    "    mcc=(tp*tn-fp*fn)/T.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "    \n",
    "    return mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#debug\n",
    "ypred=T.matrix('ypred')\n",
    "ytrue=T.imatrix('ytrue')\n",
    "mccdummy=matthews_correlation_coefficient(ypred,ytrue)\n",
    "mcc=theano.function([ypred,ytrue],mccdummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1.0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcc(np.asarray([[0.3,0.7],[0.8,0.2],[0.56,0.44],[0.1,0.9]],dtype=np.float32),np.asarray([[0,1],[1,0],[1,0],[0,1]],dtype=np.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct classification network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#some parameters\n",
    "keep_prob=0.5\n",
    "num_filters=128\n",
    "num_units_dense=1024\n",
    "initial_learning_rate=0.001\n",
    "\n",
    "#input layer\n",
    "l_inp_data = ls.layers.InputLayer((None,hditer_train.xshape[0],hditer_train.xshape[1],hditer_train.xshape[2]))\n",
    "l_inp_label = ls.layers.InputLayer((None,1))\n",
    "\n",
    "#conv layers\n",
    "#first layer\n",
    "l_conv1 = ls.layers.Conv2DLayer(incoming=l_inp_data,\n",
    "                                num_filters=num_filters,\n",
    "                                filter_size=3,\n",
    "                                stride=(1,1),\n",
    "                                pad=0,\n",
    "                                W=ls.init.HeUniform(),\n",
    "                                b=ls.init.Constant(0.),\n",
    "                                nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                               )\n",
    "l_drop1 = ls.layers.DropoutLayer(incoming=l_conv1,\n",
    "                       p=keep_prob,\n",
    "                       rescale=True\n",
    "                      )\n",
    "l_pool1 = ls.layers.MaxPool2DLayer(incoming=l_drop1,\n",
    "                                   pool_size=(2,2),\n",
    "                                   stride=2,\n",
    "                                   pad=0                                   \n",
    "                                  )\n",
    "\n",
    "#second layer:\n",
    "l_conv2 = ls.layers.Conv2DLayer(incoming=l_pool1,\n",
    "                                num_filters=num_filters,\n",
    "                                filter_size=3,\n",
    "                                stride=(1,1),\n",
    "                                pad=0,\n",
    "                                W=ls.init.HeUniform(),\n",
    "                                b=ls.init.Constant(0.),\n",
    "                                nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                               )\n",
    "l_drop2 = ls.layers.DropoutLayer(incoming=l_conv2,\n",
    "                       p=keep_prob,\n",
    "                       rescale=True\n",
    "                      )\n",
    "l_pool2 = ls.layers.MaxPool2DLayer(incoming=l_drop2,\n",
    "                                   pool_size=(2,2),\n",
    "                                   stride=2,\n",
    "                                   pad=0                                   \n",
    "                                  )\n",
    "\n",
    "#third layer:\n",
    "l_conv3 = ls.layers.Conv2DLayer(incoming=l_pool2,\n",
    "                                num_filters=num_filters,\n",
    "                                filter_size=3,\n",
    "                                stride=(1,1),\n",
    "                                pad=0,\n",
    "                                W=ls.init.HeUniform(),\n",
    "                                b=ls.init.Constant(0.),\n",
    "                                nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                               )\n",
    "\n",
    "l_drop3 = ls.layers.DropoutLayer(incoming=l_conv3,\n",
    "                       p=keep_prob,\n",
    "                       rescale=True\n",
    "                      )\n",
    "l_pool3 = ls.layers.MaxPool2DLayer(incoming=l_drop3,\n",
    "                                   pool_size=(2,2),\n",
    "                                   stride=2,\n",
    "                                   pad=0                                   \n",
    "                                  )\n",
    "\n",
    "#fourth layer:\n",
    "l_conv4 = ls.layers.Conv2DLayer(incoming=l_pool3,\n",
    "                                num_filters=num_filters,\n",
    "                                filter_size=3,\n",
    "                                stride=(1,1),\n",
    "                                pad=0,\n",
    "                                W=ls.init.HeUniform(),\n",
    "                                b=ls.init.Constant(0.),\n",
    "                                nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                               )\n",
    "l_drop4 = ls.layers.DropoutLayer(incoming=l_conv4,\n",
    "                       p=keep_prob,\n",
    "                       rescale=True\n",
    "                      )\n",
    "l_pool4 = ls.layers.MaxPool2DLayer(incoming=l_drop4,\n",
    "                                   pool_size=(2,2),\n",
    "                                   stride=2,\n",
    "                                   pad=0                                   \n",
    "                                  )\n",
    "\n",
    "#flatten\n",
    "l_flat = ls.layers.FlattenLayer(incoming=l_pool4, \n",
    "                                outdim=2)\n",
    "\n",
    "#crossfire\n",
    "l_fc1 = ls.layers.DenseLayer(incoming=l_flat, \n",
    "                             num_units=num_units_dense, \n",
    "                             W=ls.init.GlorotUniform(np.sqrt(2./(1+0.01**2))), \n",
    "                             b=ls.init.Constant(0.0),\n",
    "                             nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                            )\n",
    "\n",
    "l_drop5 = ls.layers.DropoutLayer(incoming=l_fc1,\n",
    "                       p=keep_prob,\n",
    "                       rescale=True\n",
    "                      )\n",
    "\n",
    "l_fc2 = ls.layers.DenseLayer(incoming=l_drop5, \n",
    "                             num_units=num_units_dense, \n",
    "                             W=ls.init.GlorotUniform(np.sqrt(2./(1+0.01**2))), \n",
    "                             b=ls.init.Constant(0.0),\n",
    "                             nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                            )\n",
    "\n",
    "l_drop6 = ls.layers.DropoutLayer(incoming=l_fc2,\n",
    "                       p=keep_prob,\n",
    "                       rescale=True\n",
    "                      )\n",
    "\n",
    "#output layer\n",
    "l_out = ls.layers.DenseLayer(incoming=l_drop6, \n",
    "                             num_units=hditer_train.num_classes, \n",
    "                             W=ls.init.GlorotUniform(np.sqrt(2./(1+0.01**2))), \n",
    "                             b=ls.init.Constant(0.0),\n",
    "                             nonlinearity=ls.nonlinearities.softmax\n",
    "                            )\n",
    "\n",
    "#network\n",
    "network = [l_inp_data, l_inp_label,\n",
    "           l_conv1, l_pool1, l_drop1,\n",
    "           l_conv2, l_pool2, l_drop2,\n",
    "           l_conv3, l_pool3, l_drop3,\n",
    "           l_conv4, l_pool4, l_drop4,\n",
    "           l_flat, \n",
    "           l_fc1, l_drop5,\n",
    "           l_fc2, l_drop6,\n",
    "           l_out\n",
    "          ]\n",
    "\n",
    "#variables\n",
    "inp = l_inp_data.input_var\n",
    "lab = T.ivector('lab')\n",
    "\n",
    "#output\n",
    "lab_pred = ls.layers.get_output(l_out, {l_inp_data: inp})\n",
    "lab_pred_det = ls.layers.get_output(l_out, {l_inp_data: inp}, deterministic=True)\n",
    "\n",
    "#loss functions:\n",
    "loss = ls.objectives.categorical_crossentropy(lab_pred,lab).mean()\n",
    "loss_det = ls.objectives.categorical_crossentropy(lab_pred_det,lab).mean()\n",
    "\n",
    "#accuracy\n",
    "acc_det = ls.objectives.categorical_accuracy(lab_pred_det, lab, top_k=1).mean()\n",
    "\n",
    "#MCC\n",
    "mcc_det = matthews_correlation_coefficient(lab_pred_det,lab)\n",
    "\n",
    "#parameters\n",
    "params = ls.layers.get_all_params(network, trainable=True)\n",
    "\n",
    "#updates\n",
    "updates = ls.updates.adam(loss, params, learning_rate=initial_learning_rate)\n",
    "\n",
    "#compile network function\n",
    "fnn = theano.function([inp], lab_pred)\n",
    "fnn_det = theano.function([inp], lab_pred_det)\n",
    "#training function to minimize\n",
    "fnn_train = theano.function([inp,lab], loss, updates=updates)\n",
    "#validation function with accuracy\n",
    "fnn_validate = theano.function([inp,lab], [loss_det,acc_det,mcc_det])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train ( 1 ):    loss =  0.709079333228 \n",
      "\t\tacc =  53.90625 \n",
      "\t\tmcc =  nan\n",
      "train ( 2 ):    loss =  0.761499853563 \n",
      "\t\tacc =  55.859375 \n",
      "\t\tmcc =  nan\n",
      "train ( 3 ):    loss =  0.738404231102 \n",
      "\t\tacc =  52.8645833333 \n",
      "\t\tmcc =  nan\n",
      "train ( 4 ):    loss =  0.723772431965 \n",
      "\t\tacc =  52.9296875 \n",
      "\t\tmcc =  nan\n",
      "train ( 5 ):    loss =  0.711038115879 \n",
      "\t\tacc =  53.59375 \n",
      "\t\tmcc =  nan\n",
      "train ( 6 ):    loss =  0.701387060355 \n",
      "\t\tacc =  53.90625 \n",
      "\t\tmcc =  nan\n",
      "train ( 7 ):    loss =  0.695889346653 \n",
      "\t\tacc =  55.9151785714 \n",
      "\t\tmcc =  nan\n",
      "train ( 8 ):    loss =  0.690075785307 \n",
      "\t\tacc =  57.71484375 \n",
      "\t\tmcc =  nan\n",
      "train ( 9 ):    loss =  0.686932439384 \n",
      "\t\tacc =  57.3784722222 \n",
      "\t\tmcc =  nan\n",
      "train ( 10 ):    loss =  0.679435376486 \n",
      "\t\tacc =  58.203125 \n",
      "\t\tmcc =  nan\n",
      "train ( 11 ):    loss =  0.673571047158 \n",
      "\t\tacc =  58.3096590909 \n",
      "\t\tmcc =  nan\n",
      "train ( 12 ):    loss =  0.669048726582 \n",
      "\t\tacc =  58.8541666667 \n",
      "\t\tmcc =  nan\n",
      "train ( 13 ):    loss =  0.66123573553 \n",
      "\t\tacc =  59.5552884615 \n",
      "\t\tmcc =  nan\n",
      "train ( 14 ):    loss =  0.656481554337 \n",
      "\t\tacc =  60.0446428571 \n",
      "\t\tmcc =  nan\n",
      "train ( 15 ):    loss =  0.651597349241 \n",
      "\t\tacc =  60.46875 \n",
      "\t\tmcc =  nan\n",
      "train ( 16 ):    loss =  0.644644117762 \n",
      "\t\tacc =  61.181640625 \n",
      "\t\tmcc =  nan\n",
      "train ( 17 ):    loss =  0.636196800766 \n",
      "\t\tacc =  61.7647058824 \n",
      "\t\tmcc =  nan\n",
      "train ( 18 ):    loss =  0.62761478341 \n",
      "\t\tacc =  63.1076388889 \n",
      "\t\tmcc =  nan\n",
      "train ( 19 ):    loss =  0.620107230493 \n",
      "\t\tacc =  64.3914473684 \n",
      "\t\tmcc =  nan\n",
      "train ( 20 ):    loss =  0.613824318759 \n",
      "\t\tacc =  65.078125 \n",
      "\t\tmcc =  nan\n",
      "train ( 21 ):    loss =  0.603516494968 \n",
      "\t\tacc =  65.9226190476 \n",
      "\t\tmcc =  nan\n",
      "train ( 22 ):    loss =  0.591384300138 \n",
      "\t\tacc =  66.9744318182 \n",
      "\t\tmcc =  nan\n",
      "train ( 23 ):    loss =  0.577796339748 \n",
      "\t\tacc =  68.2065217391 \n",
      "\t\tmcc =  nan\n",
      "train ( 24 ):    loss =  0.564694837698 \n",
      "\t\tacc =  69.3033854167 \n",
      "\t\tmcc =  nan\n",
      "train ( 25 ):    loss =  0.552382827918 \n",
      "\t\tacc =  70.375 \n",
      "\t\tmcc =  nan\n",
      "train ( 26 ):    loss =  0.55027926661 \n",
      "\t\tacc =  69.921875 \n",
      "\t\tmcc =  nan\n",
      "train ( 27 ):    loss =  0.546410174978 \n",
      "\t\tacc =  70.3703703704 \n",
      "\t\tmcc =  nan\n",
      "train ( 28 ):    loss =  0.545772189734 \n",
      "\t\tacc =  71.2332589286 \n",
      "\t\tmcc =  nan\n",
      "train ( 29 ):    loss =  0.541797160234 \n",
      "\t\tacc =  71.9558189655 \n",
      "\t\tmcc =  nan\n",
      "train ( 30 ):    loss =  0.533097785456 \n",
      "\t\tacc =  72.578125 \n",
      "\t\tmcc =  nan\n",
      "train ( 31 ):    loss =  0.523972190095 \n",
      "\t\tacc =  73.2358870968 \n",
      "\t\tmcc =  nan\n",
      "train ( 32 ):    loss =  0.513280378833 \n",
      "\t\tacc =  73.779296875 \n",
      "\t\tmcc =  nan\n",
      "train ( 33 ):    loss =  0.506223454963 \n",
      "\t\tacc =  74.0293560606 \n",
      "\t\tmcc =  nan\n",
      "train ( 34 ):    loss =  0.499774915576 \n",
      "\t\tacc =  74.6783088235 \n",
      "\t\tmcc =  nan\n",
      "train ( 35 ):    loss =  0.492589208324 \n",
      "\t\tacc =  75.2678571429 \n",
      "\t\tmcc =  nan\n",
      "train ( 36 ):    loss =  0.486999721985 \n",
      "\t\tacc =  75.78125 \n",
      "\t\tmcc =  nan\n",
      "train ( 37 ):    loss =  0.480398242969 \n",
      "\t\tacc =  76.245777027 \n",
      "\t\tmcc =  nan\n",
      "train ( 38 ):    loss =  0.474002615981 \n",
      "\t\tacc =  76.6447368421 \n",
      "\t\tmcc =  nan\n",
      "train ( 39 ):    loss =  0.465649382671 \n",
      "\t\tacc =  77.0032051282 \n",
      "\t\tmcc =  nan\n",
      "train ( 40 ):    loss =  0.459840314248 \n",
      "\t\tacc =  77.40234375 \n",
      "\t\tmcc =  nan\n",
      "train ( 41 ):    loss =  0.451887550272 \n",
      "\t\tacc =  77.724847561 \n",
      "\t\tmcc =  nan\n",
      "train ( 42 ):    loss =  0.445125698983 \n",
      "\t\tacc =  78.0319940476 \n",
      "\t\tmcc =  nan\n",
      "train ( 43 ):    loss =  0.440650614065 \n",
      "\t\tacc =  78.0341569767 \n",
      "\t\tmcc =  nan\n",
      "train ( 44 ):    loss =  0.434708023891 \n",
      "\t\tacc =  78.0717329545 \n",
      "\t\tmcc =  nan\n",
      "train ( 45 ):    loss =  0.430735368076 \n",
      "\t\tacc =  78.3680555556 \n",
      "\t\tmcc =  nan\n",
      "train ( 46 ):    loss =  0.424954881755 \n",
      "\t\tacc =  78.6514945652 \n",
      "\t\tmcc =  nan\n",
      "train ( 47 ):    loss =  0.418578738295 \n",
      "\t\tacc =  78.9893617021 \n",
      "\t\tmcc =  nan\n",
      "train ( 48 ):    loss =  0.413426685222 \n",
      "\t\tacc =  79.2317708333 \n",
      "\t\tmcc =  nan\n",
      "train ( 49 ):    loss =  0.408792546756 \n",
      "\t\tacc =  79.4164540816 \n",
      "\t\tmcc =  nan\n",
      "train ( 50 ):    loss =  0.404639182416 \n",
      "\t\tacc =  79.53125 \n",
      "\t\tmcc =  nan\n",
      "train ( 51 ):    loss =  0.401416117157 \n",
      "\t\tacc =  79.6262254902 \n",
      "\t\tmcc =  nan\n",
      "train ( 52 ):    loss =  0.396845940326 \n",
      "\t\tacc =  79.8978365385 \n",
      "\t\tmcc =  nan\n",
      "train ( 53 ):    loss =  0.391358988726 \n",
      "\t\tacc =  80.1297169811 \n",
      "\t\tmcc =  nan\n",
      "train ( 54 ):    loss =  0.387282349952 \n",
      "\t\tacc =  80.2951388889 \n",
      "\t\tmcc =  nan\n",
      "train ( 55 ):    loss =  0.382289358619 \n",
      "\t\tacc =  80.4261363636 \n",
      "\t\tmcc =  nan\n",
      "train ( 56 ):    loss =  0.377631928681 \n",
      "\t\tacc =  80.6222098214 \n",
      "\t\tmcc =  nan\n",
      "train ( 57 ):    loss =  0.373668433441 \n",
      "\t\tacc =  80.8662280702 \n",
      "\t\tmcc =  nan\n",
      "train ( 58 ):    loss =  0.368307026733 \n",
      "\t\tacc =  81.1422413793 \n",
      "\t\tmcc =  nan\n",
      "train ( 59 ):    loss =  0.365810418788 \n",
      "\t\tacc =  81.3029661017 \n",
      "\t\tmcc =  nan\n",
      "train ( 60 ):    loss =  0.363136280918 \n",
      "\t\tacc =  81.4713541667 \n",
      "\t\tmcc =  nan\n",
      "train ( 61 ):    loss =  0.358220610974 \n",
      "\t\tacc =  81.6342213115 \n",
      "\t\tmcc =  nan\n",
      "train ( 62 ):    loss =  0.35338951334 \n",
      "\t\tacc =  81.7414314516 \n",
      "\t\tmcc =  nan\n",
      "train ( 63 ):    loss =  0.350626174018 \n",
      "\t\tacc =  81.8328373016 \n",
      "\t\tmcc =  nan\n",
      "train ( 64 ):    loss =  0.346598528044 \n",
      "\t\tacc =  82.0556640625 \n",
      "\t\tmcc =  nan\n",
      "train ( 65 ):    loss =  0.342183406356 \n",
      "\t\tacc =  82.2956730769 \n",
      "\t\tmcc =  nan\n",
      "train ( 66 ):    loss =  0.338975842636 \n",
      "\t\tacc =  82.4810606061 \n",
      "\t\tmcc =  nan\n",
      "train ( 67 ):    loss =  0.335238193183 \n",
      "\t\tacc =  82.6026119403 \n",
      "\t\tmcc =  nan\n",
      "train ( 68 ):    loss =  0.332733881939 \n",
      "\t\tacc =  82.6401654412 \n",
      "\t\tmcc =  nan\n",
      "train ( 69 ):    loss =  0.329391538827 \n",
      "\t\tacc =  82.7785326087 \n",
      "\t\tmcc =  nan\n",
      "train ( 70 ):    loss =  0.326662044415 \n",
      "\t\tacc =  82.96875 \n",
      "\t\tmcc =  nan\n",
      "train ( 71 ):    loss =  0.323872679451 \n",
      "\t\tacc =  83.1426056338 \n",
      "\t\tmcc =  nan\n",
      "train ( 72 ):    loss =  0.320208726289 \n",
      "\t\tacc =  83.30078125 \n",
      "\t\tmcc =  nan\n",
      "train ( 73 ):    loss =  0.316797935113 \n",
      "\t\tacc =  83.4225171233 \n",
      "\t\tmcc =  nan\n",
      "train ( 74 ):    loss =  0.313731389751 \n",
      "\t\tacc =  83.59375 \n",
      "\t\tmcc =  nan\n",
      "train ( 75 ):    loss =  0.311215981019 \n",
      "\t\tacc =  83.7083333333 \n",
      "\t\tmcc =  nan\n",
      "train ( 76 ):    loss =  0.308229899667 \n",
      "\t\tacc =  83.8712993421 \n",
      "\t\tmcc =  nan\n",
      "train ( 77 ):    loss =  0.306240907349 \n",
      "\t\tacc =  83.9995941558 \n",
      "\t\tmcc =  nan\n",
      "train ( 78 ):    loss =  0.303238004903 \n",
      "\t\tacc =  84.1446314103 \n",
      "\t\tmcc =  nan\n",
      "train ( 79 ):    loss =  0.301986215035 \n",
      "\t\tacc =  84.2859968354 \n",
      "\t\tmcc =  nan\n",
      "train ( 80 ):    loss =  0.298956320861 \n",
      "\t\tacc =  84.345703125 \n",
      "\t\tmcc =  nan\n",
      "train ( 81 ):    loss =  0.296631476526 \n",
      "\t\tacc =  84.3846450617 \n",
      "\t\tmcc =  nan\n",
      "train ( 82 ):    loss =  0.295780016046 \n",
      "\t\tacc =  84.4702743902 \n",
      "\t\tmcc =  nan\n",
      "train ( 83 ):    loss =  0.29421724732 \n",
      "\t\tacc =  84.6009036145 \n",
      "\t\tmcc =  nan\n",
      "train ( 84 ):    loss =  0.291477205606 \n",
      "\t\tacc =  84.7005208333 \n",
      "\t\tmcc =  nan\n",
      "train ( 85 ):    loss =  0.289026839875 \n",
      "\t\tacc =  84.7886029412 \n",
      "\t\tmcc =  nan\n",
      "train ( 86 ):    loss =  0.286273634129 \n",
      "\t\tacc =  84.7747093023 \n",
      "\t\tmcc =  nan\n",
      "train ( 87 ):    loss =  0.285160440207 \n",
      "\t\tacc =  84.7611350575 \n",
      "\t\tmcc =  nan\n",
      "train ( 88 ):    loss =  0.282914132373 \n",
      "\t\tacc =  84.86328125 \n",
      "\t\tmcc =  nan\n",
      "train ( 89 ):    loss =  0.280949443302 \n",
      "\t\tacc =  84.9894662921 \n",
      "\t\tmcc =  nan\n",
      "train ( 90 ):    loss =  0.279160862692 \n",
      "\t\tacc =  85.1215277778 \n",
      "\t\tmcc =  nan\n",
      "train ( 91 ):    loss =  0.276736760186 \n",
      "\t\tacc =  85.1476648352 \n",
      "\t\tmcc =  nan\n",
      "train ( 92 ):    loss =  0.274678652202 \n",
      "\t\tacc =  85.0628396739 \n",
      "\t\tmcc =  nan\n",
      "train ( 93 ):    loss =  0.274274487531 \n",
      "\t\tacc =  85.0470430108 \n",
      "\t\tmcc =  nan\n",
      "train ( 94 ):    loss =  0.272410766213 \n",
      "\t\tacc =  85.1479388298 \n",
      "\t\tmcc =  nan\n",
      "train ( 95 ):    loss =  0.270468612947 \n",
      "\t\tacc =  85.2713815789 \n",
      "\t\tmcc =  nan\n",
      "train ( 96 ):    loss =  0.268425391742 \n",
      "\t\tacc =  85.3922526042 \n",
      "\t\tmcc =  nan\n",
      "train ( 97 ):    loss =  0.266754158169 \n",
      "\t\tacc =  85.4703608247 \n",
      "\t\tmcc =  nan\n",
      "train ( 98 ):    loss =  0.26482501841 \n",
      "\t\tacc =  85.4830994898 \n",
      "\t\tmcc =  nan\n",
      "train ( 99 ):    loss =  0.26293059678 \n",
      "\t\tacc =  85.5271464646 \n",
      "\t\tmcc =  nan\n",
      "train ( 100 ):    loss =  0.262672622667 \n",
      "\t\tacc =  85.5703125 \n",
      "\t\tmcc =  nan\n",
      "train ( 101 ):    loss =  0.261189464919 \n",
      "\t\tacc =  85.7054455446 \n",
      "\t\tmcc =  nan\n",
      "train ( 102 ):    loss =  0.260011008213 \n",
      "\t\tacc =  85.8072916667 \n",
      "\t\tmcc =  nan\n",
      "train ( 103 ):    loss =  0.259175471718 \n",
      "\t\tacc =  85.8995752427 \n",
      "\t\tmcc =  nan\n",
      "train ( 104 ):    loss =  0.25754382739 \n",
      "\t\tacc =  85.9750600962 \n",
      "\t\tmcc =  nan\n",
      "train ( 105 ):    loss =  0.255983365806 \n",
      "\t\tacc =  85.9970238095 \n",
      "\t\tmcc =  nan\n",
      "train ( 106 ):    loss =  0.254253428351 \n",
      "\t\tacc =  86.0480542453 \n",
      "\t\tmcc =  nan\n",
      "train ( 107 ):    loss =  0.252698996283 \n",
      "\t\tacc =  86.0835280374 \n",
      "\t\tmcc =  nan\n",
      "train ( 108 ):    loss =  0.251191959893 \n",
      "\t\tacc =  86.1545138889 \n",
      "\t\tmcc =  nan\n",
      "train ( 109 ):    loss =  0.249858237949 \n",
      "\t\tacc =  86.1883600917 \n",
      "\t\tmcc =  nan\n",
      "train ( 110 ):    loss =  0.248105978232 \n",
      "\t\tacc =  86.2997159091 \n",
      "\t\tmcc =  nan\n",
      "train ( 111 ):    loss =  0.246773217211 \n",
      "\t\tacc =  86.3809121622 \n",
      "\t\tmcc =  nan\n",
      "train ( 112 ):    loss =  0.245545662584 \n",
      "\t\tacc =  86.4606584821 \n",
      "\t\tmcc =  nan\n",
      "train ( 113 ):    loss =  0.244038902165 \n",
      "\t\tacc =  86.5666482301 \n",
      "\t\tmcc =  nan\n",
      "train ( 114 ):    loss =  0.242394131604 \n",
      "\t\tacc =  86.6091008772 \n",
      "\t\tmcc =  nan\n",
      "train ( 115 ):    loss =  0.240780653682 \n",
      "\t\tacc =  86.6576086957 \n",
      "\t\tmcc =  nan\n",
      "train ( 116 ):    loss =  0.239441360354 \n",
      "\t\tacc =  86.7052801724 \n",
      "\t\tmcc =  nan\n",
      "train ( 117 ):    loss =  0.23850836864 \n",
      "\t\tacc =  86.7922008547 \n",
      "\t\tmcc =  nan\n",
      "train ( 118 ):    loss =  0.237251816815 \n",
      "\t\tacc =  86.8511652542 \n",
      "\t\tmcc =  nan\n",
      "train ( 119 ):    loss =  0.235898147088 \n",
      "\t\tacc =  86.9222689076 \n",
      "\t\tmcc =  nan\n",
      "train ( 120 ):    loss =  0.234765922805 \n",
      "\t\tacc =  87.0052083333 \n",
      "\t\tmcc =  nan\n",
      "train ( 121 ):    loss =  0.233579381046 \n",
      "\t\tacc =  87.0609504132 \n",
      "\t\tmcc =  nan\n",
      "train ( 122 ):    loss =  0.232106141893 \n",
      "\t\tacc =  87.1413934426 \n",
      "\t\tmcc =  nan\n",
      "train ( 123 ):    loss =  0.230681118722 \n",
      "\t\tacc =  87.2014735772 \n",
      "\t\tmcc =  nan\n",
      "train ( 124 ):    loss =  0.229153107893 \n",
      "\t\tacc =  87.2731854839 \n",
      "\t\tmcc =  nan\n",
      "train ( 125 ):    loss =  0.227606549774 \n",
      "\t\tacc =  87.34375 \n",
      "\t\tmcc =  nan\n",
      "train ( 126 ):    loss =  0.226258222601 \n",
      "\t\tacc =  87.4255952381 \n",
      "\t\tmcc =  nan\n",
      "train ( 127 ):    loss =  0.225064454689 \n",
      "\t\tacc =  87.5061515748 \n",
      "\t\tmcc =  nan\n",
      "train ( 128 ):    loss =  0.223607617952 \n",
      "\t\tacc =  87.5732421875 \n",
      "\t\tmcc =  nan\n",
      "train ( 129 ):    loss =  0.22236964678 \n",
      "\t\tacc =  87.621124031 \n",
      "\t\tmcc =  nan\n",
      "train ( 130 ):    loss =  0.221207499747 \n",
      "\t\tacc =  87.65625 \n",
      "\t\tmcc =  nan\n",
      "train ( 131 ):    loss =  0.220393835033 \n",
      "\t\tacc =  87.6968034351 \n",
      "\t\tmcc =  nan\n",
      "train ( 132 ):    loss =  0.219237870222 \n",
      "\t\tacc =  87.7604166667 \n",
      "\t\tmcc =  nan\n",
      "train ( 133 ):    loss =  0.218493038181 \n",
      "\t\tacc =  87.7995770677 \n",
      "\t\tmcc =  nan\n",
      "train ( 134 ):    loss =  0.217058061476 \n",
      "\t\tacc =  87.8789645522 \n",
      "\t\tmcc =  nan\n",
      "train ( 135 ):    loss =  0.215832462494 \n",
      "\t\tacc =  87.9513888889 \n",
      "\t\tmcc =  nan\n",
      "train ( 136 ):    loss =  0.214887147856 \n",
      "\t\tacc =  87.98828125 \n",
      "\t\tmcc =  nan\n",
      "train ( 137 ):    loss =  0.213879304297 \n",
      "\t\tacc =  88.0303375912 \n",
      "\t\tmcc =  nan\n",
      "train ( 138 ):    loss =  0.212465413948 \n",
      "\t\tacc =  88.0831068841 \n",
      "\t\tmcc =  nan\n",
      "train ( 139 ):    loss =  0.211450802 \n",
      "\t\tacc =  88.1463579137 \n",
      "\t\tmcc =  nan\n",
      "train ( 140 ):    loss =  0.210186359541 \n",
      "\t\tacc =  88.2142857143 \n",
      "\t\tmcc =  nan\n",
      "train ( 141 ):    loss =  0.208940610376 \n",
      "\t\tacc =  88.2757092199 \n",
      "\t\tmcc =  nan\n",
      "train ( 142 ):    loss =  0.207725323119 \n",
      "\t\tacc =  88.3307658451 \n",
      "\t\tmcc =  nan\n",
      "train ( 143 ):    loss =  0.206582285886 \n",
      "\t\tacc =  88.3795891608 \n",
      "\t\tmcc =  nan\n",
      "train ( 144 ):    loss =  0.205715277035 \n",
      "\t\tacc =  88.4114583333 \n",
      "\t\tmcc =  nan\n",
      "train ( 145 ):    loss =  0.204893218498 \n",
      "\t\tacc =  88.432112069 \n",
      "\t\tmcc =  nan\n",
      "train ( 146 ):    loss =  0.203842873292 \n",
      "\t\tacc =  88.4631849315 \n",
      "\t\tmcc =  nan\n",
      "train ( 147 ):    loss =  0.202844962898 \n",
      "\t\tacc =  88.493835034 \n",
      "\t\tmcc =  nan\n",
      "train ( 148 ):    loss =  0.201829263759 \n",
      "\t\tacc =  88.5399070946 \n",
      "\t\tmcc =  nan\n",
      "train ( 149 ):    loss =  0.200773300737 \n",
      "\t\tacc =  88.5906040268 \n",
      "\t\tmcc =  nan\n",
      "train ( 150 ):    loss =  0.199940978649 \n",
      "\t\tacc =  88.6197916667 \n",
      "\t\tmcc =  nan\n",
      "train ( 151 ):    loss =  0.198912444612 \n",
      "\t\tacc =  88.6382450331 \n",
      "\t\tmcc =  nan\n",
      "train ( 152 ):    loss =  0.197806413397 \n",
      "\t\tacc =  88.6821546053 \n",
      "\t\tmcc =  nan\n",
      "train ( 153 ):    loss =  0.196888555795 \n",
      "\t\tacc =  88.7305964052 \n",
      "\t\tmcc =  nan\n",
      "train ( 154 ):    loss =  0.195819758686 \n",
      "\t\tacc =  88.7987012987 \n",
      "\t\tmcc =  nan\n",
      "train ( 155 ):    loss =  0.194987425258 \n",
      "\t\tacc =  88.8508064516 \n",
      "\t\tmcc =  nan\n",
      "train ( 156 ):    loss =  0.193967579968 \n",
      "\t\tacc =  88.8922275641 \n",
      "\t\tmcc =  nan\n",
      "train ( 157 ):    loss =  0.193238722129 \n",
      "\t\tacc =  88.9281449045 \n",
      "\t\tmcc =  nan\n",
      "train ( 158 ):    loss =  0.192149917965 \n",
      "\t\tacc =  88.9586629747 \n",
      "\t\tmcc =  nan\n",
      "train ( 159 ):    loss =  0.191359120873 \n",
      "\t\tacc =  88.9838836478 \n",
      "\t\tmcc =  nan\n",
      "train ( 160 ):    loss =  0.190428140898 \n",
      "\t\tacc =  88.984375 \n",
      "\t\tmcc =  nan\n",
      "train ( 161 ):    loss =  0.190538410396 \n",
      "\t\tacc =  88.970302795 \n",
      "\t\tmcc =  nan\n",
      "train ( 162 ):    loss =  0.189901466032 \n",
      "\t\tacc =  89.0094521605 \n",
      "\t\tmcc =  nan\n",
      "train ( 163 ):    loss =  0.189026239374 \n",
      "\t\tacc =  89.0672929448 \n",
      "\t\tmcc =  nan\n",
      "train ( 164 ):    loss =  0.188034846583 \n",
      "\t\tacc =  89.1291920732 \n",
      "\t\tmcc =  nan\n",
      "train ( 165 ):    loss =  0.18752320189 \n",
      "\t\tacc =  89.1429924242 \n",
      "\t\tmcc =  nan\n",
      "train ( 166 ):    loss =  0.186724212627 \n",
      "\t\tacc =  89.1378012048 \n",
      "\t\tmcc =  nan\n",
      "train ( 167 ):    loss =  0.185858246229 \n",
      "\t\tacc =  89.1092814371 \n",
      "\t\tmcc =  nan\n",
      "train ( 168 ):    loss =  0.186202001063 \n",
      "\t\tacc =  89.122953869 \n",
      "\t\tmcc =  nan\n",
      "train ( 169 ):    loss =  0.185414512045 \n",
      "\t\tacc =  89.1780695266 \n",
      "\t\tmcc =  nan\n",
      "train ( 170 ):    loss =  0.185191290133 \n",
      "\t\tacc =  89.2049632353 \n",
      "\t\tmcc =  nan\n",
      "train ( 171 ):    loss =  0.185233502074 \n",
      "\t\tacc =  89.2635233918 \n",
      "\t\tmcc =  nan\n",
      "train ( 172 ):    loss =  0.184473355677 \n",
      "\t\tacc =  89.3032340116 \n",
      "\t\tmcc =  nan\n",
      "train ( 173 ):    loss =  0.183505807494 \n",
      "\t\tacc =  89.2973265896 \n",
      "\t\tmcc =  nan\n",
      "train ( 174 ):    loss =  0.182777283293 \n",
      "\t\tacc =  89.2420977011 \n",
      "\t\tmcc =  nan\n",
      "train ( 175 ):    loss =  0.18269812297 \n",
      "\t\tacc =  89.2366071429 \n",
      "\t\tmcc =  nan\n",
      "train ( 176 ):    loss =  0.182720348798 \n",
      "\t\tacc =  89.2711292614 \n",
      "\t\tmcc =  nan\n",
      "train ( 177 ):    loss =  0.182167082461 \n",
      "\t\tacc =  89.3185028249 \n",
      "\t\tmcc =  nan\n",
      "train ( 178 ):    loss =  0.18172573825 \n",
      "\t\tacc =  89.3697331461 \n",
      "\t\tmcc =  nan\n",
      "train ( 179 ):    loss =  0.181570234261 \n",
      "\t\tacc =  89.4203910615 \n",
      "\t\tmcc =  nan\n",
      "train ( 180 ):    loss =  0.181109775751 \n",
      "\t\tacc =  89.4444444444 \n",
      "\t\tmcc =  nan\n",
      "train ( 181 ):    loss =  0.180430188303 \n",
      "\t\tacc =  89.4552831492 \n",
      "\t\tmcc =  nan\n",
      "train ( 182 ):    loss =  0.17964888864 \n",
      "\t\tacc =  89.4574175824 \n",
      "\t\tmcc =  nan\n",
      "train ( 183 ):    loss =  0.179482294775 \n",
      "\t\tacc =  89.4467213115 \n",
      "\t\tmcc =  nan\n",
      "train ( 184 ):    loss =  0.179573353702 \n",
      "\t\tacc =  89.4828464674 \n",
      "\t\tmcc =  nan\n",
      "train ( 185 ):    loss =  0.179388072485 \n",
      "\t\tacc =  89.5101351351 \n",
      "\t\tmcc =  nan\n",
      "train ( 186 ):    loss =  0.178576369356 \n",
      "\t\tacc =  89.5623319892 \n",
      "\t\tmcc =  nan\n",
      "train ( 187 ):    loss =  0.178730857537 \n",
      "\t\tacc =  89.6139705882 \n",
      "\t\tmcc =  nan\n",
      "train ( 188 ):    loss =  0.178957435594 \n",
      "\t\tacc =  89.6484375 \n",
      "\t\tmcc =  nan\n",
      "train ( 189 ):    loss =  0.178631778692 \n",
      "\t\tacc =  89.6618716931 \n",
      "\t\tmcc =  nan\n",
      "train ( 190 ):    loss =  0.177884228085 \n",
      "\t\tacc =  89.6587171053 \n",
      "\t\tmcc =  nan\n",
      "train ( 191 ):    loss =  0.177669233803 \n",
      "\t\tacc =  89.6474149215 \n",
      "\t\tmcc =  nan\n",
      "train ( 192 ):    loss =  0.17804992372 \n",
      "\t\tacc =  89.6525065104 \n",
      "\t\tmcc =  nan\n",
      "train ( 193 ):    loss =  0.177735056384 \n",
      "\t\tacc =  89.6777849741 \n",
      "\t\tmcc =  nan\n",
      "train ( 194 ):    loss =  0.177064137925 \n",
      "\t\tacc =  89.7269652062 \n",
      "\t\tmcc =  nan\n",
      "train ( 195 ):    loss =  0.176477019964 \n",
      "\t\tacc =  89.7716346154 \n",
      "\t\tmcc =  nan\n",
      "train ( 196 ):    loss =  0.176507676257 \n",
      "\t\tacc =  89.8078762755 \n",
      "\t\tmcc =  nan\n",
      "train ( 197 ):    loss =  0.176118297912 \n",
      "\t\tacc =  89.8516814721 \n",
      "\t\tmcc =  nan\n",
      "train ( 198 ):    loss =  0.175586173056 \n",
      "\t\tacc =  89.898989899 \n",
      "\t\tmcc =  nan\n",
      "train ( 199 ):    loss =  0.175219745433 \n",
      "\t\tacc =  89.9301193467 \n",
      "\t\tmcc =  nan\n",
      "train ( 200 ):    loss =  0.174899886507 \n",
      "\t\tacc =  89.953125 \n",
      "\t\tmcc =  nan\n",
      "train ( 201 ):    loss =  0.174219035356 \n",
      "\t\tacc =  89.9525808458 \n",
      "\t\tmcc =  nan\n",
      "train ( 202 ):    loss =  0.174066174561 \n",
      "\t\tacc =  89.9443069307 \n",
      "\t\tmcc =  nan\n",
      "train ( 203 ):    loss =  0.173728213812 \n",
      "\t\tacc =  89.9745997537 \n",
      "\t\tmcc =  nan\n",
      "train ( 204 ):    loss =  0.173530482882 \n",
      "\t\tacc =  90.0084252451 \n",
      "\t\tmcc =  nan\n",
      "train ( 205 ):    loss =  0.173168392658 \n",
      "\t\tacc =  90.0495426829 \n",
      "\t\tmcc =  nan\n",
      "train ( 206 ):    loss =  0.172635687412 \n",
      "\t\tacc =  90.0864684466 \n",
      "\t\tmcc =  nan\n",
      "train ( 207 ):    loss =  0.172260241404 \n",
      "\t\tacc =  90.1268115942 \n",
      "\t\tmcc =  nan\n",
      "train ( 208 ):    loss =  0.172126325704 \n",
      "\t\tacc =  90.1630108173 \n",
      "\t\tmcc =  nan\n",
      "train ( 209 ):    loss =  0.171519914871 \n",
      "\t\tacc =  90.2063397129 \n",
      "\t\tmcc =  nan\n",
      "train ( 210 ):    loss =  0.170874865148 \n",
      "\t\tacc =  90.2232142857 \n",
      "\t\tmcc =  nan\n",
      "train ( 211 ):    loss =  0.170304037704 \n",
      "\t\tacc =  90.2621445498 \n",
      "\t\tmcc =  nan\n",
      "train ( 212 ):    loss =  0.169687338094 \n",
      "\t\tacc =  90.3007075472 \n",
      "\t\tmcc =  nan\n",
      "train ( 213 ):    loss =  0.169065784387 \n",
      "\t\tacc =  90.33157277 \n",
      "\t\tmcc =  nan\n",
      "train ( 214 ):    loss =  0.168397316487 \n",
      "\t\tacc =  90.3767523364 \n",
      "\t\tmcc =  nan\n",
      "train ( 215 ):    loss =  0.167832765612 \n",
      "\t\tacc =  90.4106104651 \n",
      "\t\tmcc =  nan\n",
      "train ( 216 ):    loss =  0.167408450957 \n",
      "\t\tacc =  90.4369212963 \n",
      "\t\tmcc =  nan\n",
      "train ( 217 ):    loss =  0.166868901507 \n",
      "\t\tacc =  90.4737903226 \n",
      "\t\tmcc =  nan\n",
      "train ( 218 ):    loss =  0.166391963108 \n",
      "\t\tacc =  90.5067373853 \n",
      "\t\tmcc =  nan\n",
      "train ( 219 ):    loss =  0.166013342358 \n",
      "\t\tacc =  90.53581621 \n",
      "\t\tmcc =  nan\n",
      "train ( 220 ):    loss =  0.165604626972 \n",
      "\t\tacc =  90.5646306818 \n",
      "\t\tmcc =  nan\n",
      "train ( 221 ):    loss =  0.165119912304 \n",
      "\t\tacc =  90.6002545249 \n",
      "\t\tmcc =  nan\n",
      "train ( 222 ):    loss =  0.164829835784 \n",
      "\t\tacc =  90.6214808559 \n",
      "\t\tmcc =  nan\n",
      "train ( 223 ):    loss =  0.16467137956 \n",
      "\t\tacc =  90.6390134529 \n",
      "\t\tmcc =  nan\n",
      "train ( 224 ):    loss =  0.164262809542 \n",
      "\t\tacc =  90.6494140625 \n",
      "\t\tmcc =  nan\n",
      "train ( 225 ):    loss =  0.163683671559 \n",
      "\t\tacc =  90.6736111111 \n",
      "\t\tmcc =  nan\n",
      "train ( 226 ):    loss =  0.163356278074 \n",
      "\t\tacc =  90.6975940265 \n",
      "\t\tmcc =  nan\n",
      "train ( 227 ):    loss =  0.162903597614 \n",
      "\t\tacc =  90.7316905286 \n",
      "\t\tmcc =  nan\n",
      "train ( 228 ):    loss =  0.162387285934 \n",
      "\t\tacc =  90.7620614035 \n",
      "\t\tmcc =  nan\n",
      "train ( 229 ):    loss =  0.161803869014 \n",
      "\t\tacc =  90.7853438865 \n",
      "\t\tmcc =  nan\n",
      "train ( 230 ):    loss =  0.161483402896 \n",
      "\t\tacc =  90.8016304348 \n",
      "\t\tmcc =  nan\n",
      "train ( 231 ):    loss =  0.161392179368 \n",
      "\t\tacc =  90.8110119048 \n",
      "\t\tmcc =  nan\n",
      "train ( 232 ):    loss =  0.160778410338 \n",
      "\t\tacc =  90.8438846983 \n",
      "\t\tmcc =  nan\n",
      "train ( 233 ):    loss =  0.160307989352 \n",
      "\t\tacc =  90.866416309 \n",
      "\t\tmcc =  nan\n",
      "train ( 234 ):    loss =  0.159683639479 \n",
      "\t\tacc =  90.8954326923 \n",
      "\t\tmcc =  nan\n",
      "train ( 235 ):    loss =  0.159187814814 \n",
      "\t\tacc =  90.9208776596 \n",
      "\t\tmcc =  nan\n",
      "train ( 236 ):    loss =  0.158856516565 \n",
      "\t\tacc =  90.9527277542 \n",
      "\t\tmcc =  nan\n",
      "train ( 237 ):    loss =  0.158483730415 \n",
      "\t\tacc =  90.9744198312 \n",
      "\t\tmcc =  nan\n",
      "train ( 238 ):    loss =  0.158278276274 \n",
      "\t\tacc =  90.9827993697 \n",
      "\t\tmcc =  nan\n",
      "train ( 239 ):    loss =  0.157708318063 \n",
      "\t\tacc =  91.0074529289 \n",
      "\t\tmcc =  nan\n",
      "train ( 240 ):    loss =  0.15742938197 \n",
      "\t\tacc =  91.0384114583 \n",
      "\t\tmcc =  nan\n",
      "train ( 241 ):    loss =  0.157001889666 \n",
      "\t\tacc =  91.0593879668 \n",
      "\t\tmcc =  nan\n",
      "train ( 242 ):    loss =  0.156558896306 \n",
      "\t\tacc =  91.0866477273 \n",
      "\t\tmcc =  nan\n",
      "train ( 243 ):    loss =  0.156094877323 \n",
      "\t\tacc =  91.110468107 \n",
      "\t\tmcc =  nan\n",
      "train ( 244 ):    loss =  0.155679278621 \n",
      "\t\tacc =  91.1148821721 \n",
      "\t\tmcc =  nan\n",
      "train ( 245 ):    loss =  0.155181156209 \n",
      "\t\tacc =  91.1383928571 \n",
      "\t\tmcc =  nan\n",
      "train ( 246 ):    loss =  0.154681687783 \n",
      "\t\tacc =  91.1648882114 \n",
      "\t\tmcc =  nan\n",
      "train ( 247 ):    loss =  0.154153948309 \n",
      "\t\tacc =  91.1974949393 \n",
      "\t\tmcc =  nan\n",
      "train ( 248 ):    loss =  0.153743184402 \n",
      "\t\tacc =  91.2266885081 \n",
      "\t\tmcc =  nan\n",
      "train ( 249 ):    loss =  0.153364983487 \n",
      "\t\tacc =  91.2587851406 \n",
      "\t\tmcc =  nan\n",
      "Epoch 1 of 10 took 4936.528s\n",
      "  training loss:\t\t0.153365\n",
      "  training accuracy:\t\t91.26 %\n",
      "  training mcc:\t\tnan\n",
      "  validation loss:\t\t0.110993\n",
      "  validation accuracy:\t\t98.29 %\n",
      "  validation mcc:\t\t0.97\n",
      "train ( 1 ):    loss =  0.0257427291298 \n",
      "\t\tacc =  100.0 \n",
      "\t\tmcc =  1.0\n",
      "train ( 2 ):    loss =  0.11124038666 \n",
      "\t\tacc =  97.265625 \n",
      "\t\tmcc =  0.946499022652\n",
      "train ( 3 ):    loss =  0.0945595589702 \n",
      "\t\tacc =  97.3958333333 \n",
      "\t\tmcc =  0.949042697272\n",
      "train ( 4 ):    loss =  0.0869397662987 \n",
      "\t\tacc =  97.65625 \n",
      "\t\tmcc =  0.954060007923\n",
      "train ( 5 ):    loss =  0.0864129529793 \n",
      "\t\tacc =  97.03125 \n",
      "\t\tmcc =  0.942506061653\n",
      "train ( 6 ):    loss =  0.078049653293 \n",
      "\t\tacc =  96.875 \n",
      "\t\tmcc =  0.939520827877\n",
      "train ( 7 ):    loss =  0.0823955862781 \n",
      "\t\tacc =  96.9866071429 \n",
      "\t\tmcc =  0.941271926887\n",
      "train ( 8 ):    loss =  0.0736807191332 \n",
      "\t\tacc =  96.97265625 \n",
      "\t\tmcc =  0.941008344913\n",
      "train ( 9 ):    loss =  0.0721058243852 \n",
      "\t\tacc =  96.3541666667 \n",
      "\t\tmcc =  0.929750078053\n",
      "train ( 10 ):    loss =  0.0695497800303 \n",
      "\t\tacc =  96.25 \n",
      "\t\tmcc =  0.927817752839\n",
      "train ( 11 ):    loss =  0.0729204306307 \n",
      "\t\tacc =  96.2357954545 \n",
      "\t\tmcc =  0.927283569207\n",
      "train ( 12 ):    loss =  0.0724549468226 \n",
      "\t\tacc =  96.4192708333 \n",
      "\t\tmcc =  0.930723112926\n",
      "train ( 13 ):    loss =  0.0711472537086 \n",
      "\t\tacc =  96.5745192308 \n",
      "\t\tmcc =  0.933642964566\n",
      "train ( 14 ):    loss =  0.0681701659324 \n",
      "\t\tacc =  96.7633928571 \n",
      "\t\tmcc =  0.937273653765\n",
      "train ( 15 ):    loss =  0.0690544405406 \n",
      "\t\tacc =  96.7708333333 \n",
      "\t\tmcc =  0.937270580737\n",
      "train ( 16 ):    loss =  0.0725479699252 \n",
      "\t\tacc =  96.826171875 \n",
      "\t\tmcc =  0.938268755039\n",
      "train ( 17 ):    loss =  0.0695321875489 \n",
      "\t\tacc =  96.9209558824 \n",
      "\t\tmcc =  0.940054099125\n",
      "train ( 18 ):    loss =  0.0758043643753 \n",
      "\t\tacc =  96.5711805556 \n",
      "\t\tmcc =  0.933754342918\n",
      "train ( 19 ):    loss =  0.0748953667523 \n",
      "\t\tacc =  96.5049342105 \n",
      "\t\tmcc =  0.932527680722\n",
      "train ( 20 ):    loss =  0.0750894809385 \n",
      "\t\tacc =  96.5625 \n",
      "\t\tmcc =  0.933536575305\n",
      "train ( 21 ):    loss =  0.0723430672702 \n",
      "\t\tacc =  96.6889880952 \n",
      "\t\tmcc =  0.935963175766\n",
      "train ( 22 ):    loss =  0.0729213141962 \n",
      "\t\tacc =  96.7329545455 \n",
      "\t\tmcc =  0.936742318005\n",
      "train ( 23 ):    loss =  0.0719103303612 \n",
      "\t\tacc =  96.8410326087 \n",
      "\t\tmcc =  0.938818202804\n",
      "train ( 24 ):    loss =  0.0723228670918 \n",
      "\t\tacc =  96.97265625 \n",
      "\t\tmcc =  0.941367444354\n",
      "train ( 25 ):    loss =  0.0724347523322 \n",
      "\t\tacc =  97.0 \n",
      "\t\tmcc =  0.941875313143\n",
      "train ( 26 ):    loss =  0.0734135666323 \n",
      "\t\tacc =  97.0252403846 \n",
      "\t\tmcc =  0.942307197447\n",
      "train ( 27 ):    loss =  0.0720593245607 \n",
      "\t\tacc =  96.9039351852 \n",
      "\t\tmcc =  0.940086669437\n",
      "train ( 28 ):    loss =  0.0720734894915 \n",
      "\t\tacc =  96.7075892857 \n",
      "\t\tmcc =  0.936202137839\n",
      "train ( 29 ):    loss =  0.0727159065142 \n",
      "\t\tacc =  96.6594827586 \n",
      "\t\tmcc =  0.935254464376\n",
      "train ( 30 ):    loss =  0.0743601020897 \n",
      "\t\tacc =  96.640625 \n",
      "\t\tmcc =  0.934906235779\n",
      "train ( 31 ):    loss =  0.0738871784433 \n",
      "\t\tacc =  96.6985887097 \n",
      "\t\tmcc =  0.935997970109\n",
      "train ( 32 ):    loss =  0.0751652055959 \n",
      "\t\tacc =  96.728515625 \n",
      "\t\tmcc =  0.936510744423\n",
      "train ( 33 ):    loss =  0.075035398387 \n",
      "\t\tacc =  96.7803030303 \n",
      "\t\tmcc =  0.937472660297\n",
      "train ( 34 ):    loss =  0.0796352862296 \n",
      "\t\tacc =  96.7601102941 \n",
      "\t\tmcc =  0.937010483057\n",
      "train ( 35 ):    loss =  0.0783782576547 \n",
      "\t\tacc =  96.71875 \n",
      "\t\tmcc =  0.936250949996\n",
      "train ( 36 ):    loss =  0.0770727512911 \n",
      "\t\tacc =  96.7013888889 \n",
      "\t\tmcc =  0.935918228133\n",
      "train ( 37 ):    loss =  0.0774655825711 \n",
      "\t\tacc =  96.6005067568 \n",
      "\t\tmcc =  0.934074374855\n",
      "train ( 38 ):    loss =  0.0769026785951 \n",
      "\t\tacc =  96.5460526316 \n",
      "\t\tmcc =  0.933053178459\n",
      "train ( 39 ):    loss =  0.076327921047 \n",
      "\t\tacc =  96.5144230769 \n",
      "\t\tmcc =  0.932438682279\n",
      "train ( 40 ):    loss =  0.0768198361088 \n",
      "\t\tacc =  96.50390625 \n",
      "\t\tmcc =  0.932234539295\n",
      "train ( 41 ):    loss =  0.0761394959035 \n",
      "\t\tacc =  96.5320121951 \n",
      "\t\tmcc =  0.932741318369\n",
      "train ( 42 ):    loss =  0.0761405945099 \n",
      "\t\tacc =  96.5773809524 \n",
      "\t\tmcc =  0.933586857652\n",
      "train ( 43 ):    loss =  0.0753741151994 \n",
      "\t\tacc =  96.6569767442 \n",
      "\t\tmcc =  0.935131349334\n",
      "train ( 44 ):    loss =  0.0763800913909 \n",
      "\t\tacc =  96.7151988636 \n",
      "\t\tmcc =  0.936252227211\n",
      "train ( 45 ):    loss =  0.0784412861497 \n",
      "\t\tacc =  96.71875 \n",
      "\t\tmcc =  0.936276897017\n",
      "train ( 46 ):    loss =  0.0775967397334 \n",
      "\t\tacc =  96.7561141304 \n",
      "\t\tmcc =  0.936993285544\n",
      "train ( 47 ):    loss =  0.0772641758053 \n",
      "\t\tacc =  96.6921542553 \n",
      "\t\tmcc =  0.935821451887\n",
      "train ( 48 ):    loss =  0.0766374210481 \n",
      "\t\tacc =  96.6471354167 \n",
      "\t\tmcc =  0.934994871485\n",
      "train ( 49 ):    loss =  0.0768501353413 \n",
      "\t\tacc =  96.6198979592 \n",
      "\t\tmcc =  0.934478401843\n",
      "train ( 50 ):    loss =  0.0766735447512 \n",
      "\t\tacc =  96.625 \n",
      "\t\tmcc =  0.93456247226\n",
      "train ( 51 ):    loss =  0.0779040896782 \n",
      "\t\tacc =  96.5992647059 \n",
      "\t\tmcc =  0.934005271653\n",
      "train ( 52 ):    loss =  0.0776864139196 \n",
      "\t\tacc =  96.6195913462 \n",
      "\t\tmcc =  0.934362762287\n",
      "train ( 53 ):    loss =  0.077722925941 \n",
      "\t\tacc =  96.6244103774 \n",
      "\t\tmcc =  0.934412440763\n",
      "train ( 54 ):    loss =  0.0769622744035 \n",
      "\t\tacc =  96.6724537037 \n",
      "\t\tmcc =  0.93533850093\n",
      "train ( 55 ):    loss =  0.0767654641603 \n",
      "\t\tacc =  96.7045454545 \n",
      "\t\tmcc =  0.935945982731\n",
      "train ( 56 ):    loss =  0.0760722514513 \n",
      "\t\tacc =  96.7215401786 \n",
      "\t\tmcc =  0.936260789799\n",
      "train ( 57 ):    loss =  0.0757982302653 \n",
      "\t\tacc =  96.7379385965 \n",
      "\t\tmcc =  0.936575438051\n",
      "train ( 58 ):    loss =  0.0747709945479 \n",
      "\t\tacc =  96.7941810345 \n",
      "\t\tmcc =  0.937668964981\n",
      "train ( 59 ):    loss =  0.0745246786697 \n",
      "\t\tacc =  96.8220338983 \n",
      "\t\tmcc =  0.938203785391\n",
      "train ( 60 ):    loss =  0.0740646396309 \n",
      "\t\tacc =  96.8619791667 \n",
      "\t\tmcc =  0.938972652156\n",
      "train ( 61 ):    loss =  0.0750231478029 \n",
      "\t\tacc =  96.875 \n",
      "\t\tmcc =  0.939205443721\n",
      "train ( 62 ):    loss =  0.0750663975152 \n",
      "\t\tacc =  96.8371975806 \n",
      "\t\tmcc =  0.938508589838\n",
      "train ( 63 ):    loss =  0.074400188104 \n",
      "\t\tacc =  96.7385912698 \n",
      "\t\tmcc =  0.93674131368\n",
      "train ( 64 ):    loss =  0.0745651287294 \n",
      "\t\tacc =  96.6796875 \n",
      "\t\tmcc =  0.935676693289\n",
      "train ( 65 ):    loss =  0.0753845429496 \n",
      "\t\tacc =  96.6586538462 \n",
      "\t\tmcc =  0.935276865069\n",
      "train ( 66 ):    loss =  0.0749956585568 \n",
      "\t\tacc =  96.6856060606 \n",
      "\t\tmcc =  0.935777348151\n",
      "train ( 67 ):    loss =  0.0746036961691 \n",
      "\t\tacc =  96.7117537313 \n",
      "\t\tmcc =  0.936268450182\n",
      "train ( 68 ):    loss =  0.0748644125116 \n",
      "\t\tacc =  96.7256433824 \n",
      "\t\tmcc =  0.936517716814\n",
      "train ( 69 ):    loss =  0.075935135758 \n",
      "\t\tacc =  96.7164855072 \n",
      "\t\tmcc =  0.936312607549\n",
      "train ( 70 ):    loss =  0.075850457516 \n",
      "\t\tacc =  96.7075892857 \n",
      "\t\tmcc =  0.936119784714\n",
      "train ( 71 ):    loss =  0.0749864451874 \n",
      "\t\tacc =  96.7209507042 \n",
      "\t\tmcc =  0.936372522452\n",
      "train ( 72 ):    loss =  0.0749957925183 \n",
      "\t\tacc =  96.6796875 \n",
      "\t\tmcc =  0.93562225049\n",
      "train ( 73 ):    loss =  0.0752491485616 \n",
      "\t\tacc =  96.6716609589 \n",
      "\t\tmcc =  0.935451815544\n",
      "train ( 74 ):    loss =  0.0747620736656 \n",
      "\t\tacc =  96.6955236486 \n",
      "\t\tmcc =  0.935901791009\n",
      "train ( 75 ):    loss =  0.0751108778978 \n",
      "\t\tacc =  96.7291666667 \n",
      "\t\tmcc =  0.936549602708\n",
      "train ( 76 ):    loss =  0.0748178539034 \n",
      "\t\tacc =  96.7722039474 \n",
      "\t\tmcc =  0.937384476356\n",
      "train ( 77 ):    loss =  0.0743370023009 \n",
      "\t\tacc =  96.8141233766 \n",
      "\t\tmcc =  0.938197664975\n",
      "train ( 78 ):    loss =  0.0746784873951 \n",
      "\t\tacc =  96.8249198718 \n",
      "\t\tmcc =  0.93836272669\n",
      "train ( 79 ):    loss =  0.0747299860476 \n",
      "\t\tacc =  96.855221519 \n",
      "\t\tmcc =  0.938932835679\n",
      "train ( 80 ):    loss =  0.0742233490538 \n",
      "\t\tacc =  96.85546875 \n",
      "\t\tmcc =  0.93891623493\n",
      "train ( 81 ):    loss =  0.0735009227085 \n",
      "\t\tacc =  96.8557098765 \n",
      "\t\tmcc =  0.938917673339\n",
      "train ( 82 ):    loss =  0.0731070902392 \n",
      "\t\tacc =  96.865472561 \n",
      "\t\tmcc =  0.939102386641\n",
      "train ( 83 ):    loss =  0.0733329019821 \n",
      "\t\tacc =  96.8561746988 \n",
      "\t\tmcc =  0.938906153177\n",
      "train ( 84 ):    loss =  0.0732038147715 \n",
      "\t\tacc =  96.8470982143 \n",
      "\t\tmcc =  0.938704741284\n",
      "train ( 85 ):    loss =  0.0729003255837 \n",
      "\t\tacc =  96.8566176471 \n",
      "\t\tmcc =  0.938874955358\n",
      "train ( 86 ):    loss =  0.0728107795635 \n",
      "\t\tacc =  96.8568313953 \n",
      "\t\tmcc =  0.938858967505\n",
      "train ( 87 ):    loss =  0.0737132594193 \n",
      "\t\tacc =  96.875 \n",
      "\t\tmcc =  0.939201751606\n",
      "train ( 88 ):    loss =  0.0740155502916 \n",
      "\t\tacc =  96.8838778409 \n",
      "\t\tmcc =  0.939365086003\n",
      "train ( 89 ):    loss =  0.0739852747935 \n",
      "\t\tacc =  96.8486657303 \n",
      "\t\tmcc =  0.938693283803\n",
      "train ( 90 ):    loss =  0.0738255197356 \n",
      "\t\tacc =  96.8229166667 \n",
      "\t\tmcc =  0.938191933819\n",
      "train ( 91 ):    loss =  0.0739387406744 \n",
      "\t\tacc =  96.7977335165 \n",
      "\t\tmcc =  0.937723481622\n",
      "train ( 92 ):    loss =  0.0738102744226 \n",
      "\t\tacc =  96.8070652174 \n",
      "\t\tmcc =  0.937890664485\n",
      "train ( 93 ):    loss =  0.0741744573336 \n",
      "\t\tacc =  96.7993951613 \n",
      "\t\tmcc =  0.937729761135\n",
      "train ( 94 ):    loss =  0.0739267973076 \n",
      "\t\tacc =  96.8168218085 \n",
      "\t\tmcc =  0.93806456312\n",
      "train ( 95 ):    loss =  0.073528667635 \n",
      "\t\tacc =  96.8421052632 \n",
      "\t\tmcc =  0.938552830623\n",
      "train ( 96 ):    loss =  0.0739481185399 \n",
      "\t\tacc =  96.8343098958 \n",
      "\t\tmcc =  0.938374287842\n",
      "train ( 97 ):    loss =  0.0740574191453 \n",
      "\t\tacc =  96.8508376289 \n",
      "\t\tmcc =  0.938686730073\n",
      "train ( 98 ):    loss =  0.0738291231161 \n",
      "\t\tacc =  96.875 \n",
      "\t\tmcc =  0.939154086586\n",
      "train ( 99 ):    loss =  0.0733953191039 \n",
      "\t\tacc =  96.8907828283 \n",
      "\t\tmcc =  0.93945517769\n",
      "train ( 100 ):    loss =  0.0734436817282 \n",
      "\t\tacc =  96.90625 \n",
      "\t\tmcc =  0.939752266318\n",
      "train ( 101 ):    loss =  0.0735838312919 \n",
      "\t\tacc =  96.9059405941 \n",
      "\t\tmcc =  0.939746434683\n",
      "train ( 102 ):    loss =  0.0732715467445 \n",
      "\t\tacc =  96.9056372549 \n",
      "\t\tmcc =  0.939742415094\n",
      "train ( 103 ):    loss =  0.0737348654831 \n",
      "\t\tacc =  96.9205097087 \n",
      "\t\tmcc =  0.940028637949\n",
      "train ( 104 ):    loss =  0.0733171912279 \n",
      "\t\tacc =  96.9200721154 \n",
      "\t\tmcc =  0.940015688764\n",
      "train ( 105 ):    loss =  0.072987836385 \n",
      "\t\tacc =  96.9419642857 \n",
      "\t\tmcc =  0.940438872451\n",
      "train ( 106 ):    loss =  0.0728991055738 \n",
      "\t\tacc =  96.9413325472 \n",
      "\t\tmcc =  0.940428472113\n",
      "train ( 107 ):    loss =  0.0728109211633 \n",
      "\t\tacc =  96.9553154206 \n",
      "\t\tmcc =  0.940689623152\n",
      "train ( 108 ):    loss =  0.0724079719686 \n",
      "\t\tacc =  96.9545717593 \n",
      "\t\tmcc =  0.940676426682\n",
      "train ( 109 ):    loss =  0.072097494272 \n",
      "\t\tacc =  96.9753440367 \n",
      "\t\tmcc =  0.941074538905\n",
      "train ( 110 ):    loss =  0.0719455825341 \n",
      "\t\tacc =  96.9957386364 \n",
      "\t\tmcc =  0.941469272051\n",
      "train ( 111 ):    loss =  0.0716493551649 \n",
      "\t\tacc =  97.0157657658 \n",
      "\t\tmcc =  0.94185668954\n",
      "train ( 112 ):    loss =  0.0718054970162 \n",
      "\t\tacc =  97.021484375 \n",
      "\t\tmcc =  0.941955265479\n",
      "train ( 113 ):    loss =  0.071929477705 \n",
      "\t\tacc =  97.0201880531 \n",
      "\t\tmcc =  0.941930557564\n",
      "train ( 114 ):    loss =  0.0714164902588 \n",
      "\t\tacc =  97.0463267544 \n",
      "\t\tmcc =  0.942439938638\n",
      "train ( 115 ):    loss =  0.0710204090934 \n",
      "\t\tacc =  97.0720108696 \n",
      "\t\tmcc =  0.942940460911\n",
      "train ( 116 ):    loss =  0.0706928796588 \n",
      "\t\tacc =  97.0837823276 \n",
      "\t\tmcc =  0.943166526252\n",
      "train ( 117 ):    loss =  0.0706480794533 \n",
      "\t\tacc =  97.0886752137 \n",
      "\t\tmcc =  0.943260231553\n",
      "train ( 118 ):    loss =  0.070445832326 \n",
      "\t\tacc =  97.1133474576 \n",
      "\t\tmcc =  0.943741077049\n",
      "train ( 119 ):    loss =  0.070225728178 \n",
      "\t\tacc =  97.1244747899 \n",
      "\t\tmcc =  0.943954277577\n",
      "train ( 120 ):    loss =  0.0698256428396 \n",
      "\t\tacc =  97.1484375 \n",
      "\t\tmcc =  0.944421325263\n",
      "train ( 121 ):    loss =  0.0697329704792 \n",
      "\t\tacc =  97.1655475207 \n",
      "\t\tmcc =  0.944752140558\n",
      "train ( 122 ):    loss =  0.0692437268751 \n",
      "\t\tacc =  97.1823770492 \n",
      "\t\tmcc =  0.945077841606\n",
      "train ( 123 ):    loss =  0.0690527259776 \n",
      "\t\tacc =  97.1862296748 \n",
      "\t\tmcc =  0.945151970862\n",
      "train ( 124 ):    loss =  0.0685917161465 \n",
      "\t\tacc =  97.1963205645 \n",
      "\t\tmcc =  0.945342031199\n",
      "train ( 125 ):    loss =  0.0683301813175 \n",
      "\t\tacc =  97.20625 \n",
      "\t\tmcc =  0.945533141103\n",
      "train ( 126 ):    loss =  0.0678768325218 \n",
      "\t\tacc =  97.2160218254 \n",
      "\t\tmcc =  0.945719740729\n",
      "train ( 127 ):    loss =  0.0677171859036 \n",
      "\t\tacc =  97.2317913386 \n",
      "\t\tmcc =  0.946024881459\n",
      "train ( 128 ):    loss =  0.0676467904637 \n",
      "\t\tacc =  97.2351074219 \n",
      "\t\tmcc =  0.946066969929\n",
      "train ( 129 ):    loss =  0.067380425683 \n",
      "\t\tacc =  97.2504844961 \n",
      "\t\tmcc =  0.946364863069\n",
      "train ( 130 ):    loss =  0.0673880728276 \n",
      "\t\tacc =  97.2536057692 \n",
      "\t\tmcc =  0.946403553042\n",
      "train ( 131 ):    loss =  0.0670965377306 \n",
      "\t\tacc =  97.2566793893 \n",
      "\t\tmcc =  0.946449378959\n",
      "train ( 132 ):    loss =  0.066847813361 \n",
      "\t\tacc =  97.2537878788 \n",
      "\t\tmcc =  0.946394181172\n",
      "train ( 133 ):    loss =  0.0670427800514 \n",
      "\t\tacc =  97.2509398496 \n",
      "\t\tmcc =  0.946341439764\n",
      "train ( 134 ):    loss =  0.0667070838088 \n",
      "\t\tacc =  97.2481343284 \n",
      "\t\tmcc =  0.946289485542\n",
      "train ( 135 ):    loss =  0.0662905519402 \n",
      "\t\tacc =  97.2627314815 \n",
      "\t\tmcc =  0.946572435044\n",
      "train ( 136 ):    loss =  0.0659955163644 \n",
      "\t\tacc =  97.265625 \n",
      "\t\tmcc =  0.946625078102\n",
      "train ( 137 ):    loss =  0.0657105509074 \n",
      "\t\tacc =  97.2798813869 \n",
      "\t\tmcc =  0.946901447375\n",
      "train ( 138 ):    loss =  0.0656922677338 \n",
      "\t\tacc =  97.2769474638 \n",
      "\t\tmcc =  0.946835766785\n",
      "train ( 139 ):    loss =  0.0656818441712 \n",
      "\t\tacc =  97.2740557554 \n",
      "\t\tmcc =  0.946779629946\n",
      "train ( 140 ):    loss =  0.0655538539599 \n",
      "\t\tacc =  97.2544642857 \n",
      "\t\tmcc =  0.946385840779\n",
      "train ( 141 ):    loss =  0.0652178494713 \n",
      "\t\tacc =  97.2628546099 \n",
      "\t\tmcc =  0.946546541866\n",
      "train ( 142 ):    loss =  0.0650505640901 \n",
      "\t\tacc =  97.2546214789 \n",
      "\t\tmcc =  0.946393201423\n",
      "train ( 143 ):    loss =  0.0647462190892 \n",
      "\t\tacc =  97.2519667832 \n",
      "\t\tmcc =  0.946344155077\n",
      "train ( 144 ):    loss =  0.0645486593571 \n",
      "\t\tacc =  97.265625 \n",
      "\t\tmcc =  0.946609040586\n",
      "train ( 145 ):    loss =  0.0641977130505 \n",
      "\t\tacc =  97.2737068966 \n",
      "\t\tmcc =  0.946765052507\n",
      "train ( 146 ):    loss =  0.0641073616573 \n",
      "\t\tacc =  97.2870291096 \n",
      "\t\tmcc =  0.947023323472\n",
      "train ( 147 ):    loss =  0.0639355273164 \n",
      "\t\tacc =  97.2948554422 \n",
      "\t\tmcc =  0.94717256689\n",
      "train ( 148 ):    loss =  0.0636361745297 \n",
      "\t\tacc =  97.3131334459 \n",
      "\t\tmcc =  0.947529509006\n",
      "train ( 149 ):    loss =  0.0633143894227 \n",
      "\t\tacc =  97.3259228188 \n",
      "\t\tmcc =  0.947777297374\n",
      "train ( 150 ):    loss =  0.0631305492738 \n",
      "\t\tacc =  97.34375 \n",
      "\t\tmcc =  0.948125448725\n",
      "train ( 151 ):    loss =  0.062815436681 \n",
      "\t\tacc =  97.3613410596 \n",
      "\t\tmcc =  0.9484689888\n",
      "train ( 152 ):    loss =  0.0624911460902 \n",
      "\t\tacc =  97.3581414474 \n",
      "\t\tmcc =  0.948409284527\n",
      "train ( 153 ):    loss =  0.06242907353 \n",
      "\t\tacc =  97.3549836601 \n",
      "\t\tmcc =  0.948344260434\n",
      "train ( 154 ):    loss =  0.0622652416511 \n",
      "\t\tacc =  97.3569399351 \n",
      "\t\tmcc =  0.948382257055\n",
      "train ( 155 ):    loss =  0.0626896034007 \n",
      "\t\tacc =  97.3286290323 \n",
      "\t\tmcc =  0.947822010681\n",
      "train ( 156 ):    loss =  0.0623453078395 \n",
      "\t\tacc =  97.3457532051 \n",
      "\t\tmcc =  0.948156484971\n",
      "train ( 157 ):    loss =  0.0624139214862 \n",
      "\t\tacc =  97.3527070064 \n",
      "\t\tmcc =  0.94828588893\n",
      "train ( 158 ):    loss =  0.0625136384752 \n",
      "\t\tacc =  97.3595727848 \n",
      "\t\tmcc =  0.948418263985\n",
      "train ( 159 ):    loss =  0.0624518819377 \n",
      "\t\tacc =  97.3614386792 \n",
      "\t\tmcc =  0.948453218461\n",
      "train ( 160 ):    loss =  0.0622174335941 \n",
      "\t\tacc =  97.3486328125 \n",
      "\t\tmcc =  0.948180058938\n",
      "train ( 161 ):    loss =  0.062230721523 \n",
      "\t\tacc =  97.3456909938 \n",
      "\t\tmcc =  0.94811437395\n",
      "train ( 162 ):    loss =  0.0624195201045 \n",
      "\t\tacc =  97.337962963 \n",
      "\t\tmcc =  0.947952814437\n",
      "train ( 163 ):    loss =  0.0626124563682 \n",
      "\t\tacc =  97.3399156442 \n",
      "\t\tmcc =  0.94799098038\n",
      "train ( 164 ):    loss =  0.0624926357897 \n",
      "\t\tacc =  97.3466082317 \n",
      "\t\tmcc =  0.948119766718\n",
      "train ( 165 ):    loss =  0.0623685088266 \n",
      "\t\tacc =  97.353219697 \n",
      "\t\tmcc =  0.948241792182\n",
      "train ( 166 ):    loss =  0.0622526717146 \n",
      "\t\tacc =  97.3550451807 \n",
      "\t\tmcc =  0.948270531273\n",
      "train ( 167 ):    loss =  0.0624720830183 \n",
      "\t\tacc =  97.3568488024 \n",
      "\t\tmcc =  0.948295299039\n",
      "train ( 168 ):    loss =  0.0626671791937 \n",
      "\t\tacc =  97.3586309524 \n",
      "\t\tmcc =  0.948327659705\n",
      "train ( 169 ):    loss =  0.0624888919373 \n",
      "\t\tacc =  97.3650147929 \n",
      "\t\tmcc =  0.948451302957\n",
      "train ( 170 ):    loss =  0.062698124383 \n",
      "\t\tacc =  97.3621323529 \n",
      "\t\tmcc =  0.948389578229\n",
      "train ( 171 ):    loss =  0.0630418820383 \n",
      "\t\tacc =  97.3547149123 \n",
      "\t\tmcc =  0.948233841714\n",
      "train ( 172 ):    loss =  0.0630358763351 \n",
      "\t\tacc =  97.3610101744 \n",
      "\t\tmcc =  0.94835587385\n",
      "train ( 173 ):    loss =  0.0632030610568 \n",
      "\t\tacc =  97.3717485549 \n",
      "\t\tmcc =  0.948564335214\n",
      "train ( 174 ):    loss =  0.0631636738116 \n",
      "\t\tacc =  97.3733836207 \n",
      "\t\tmcc =  0.948596322061\n",
      "train ( 175 ):    loss =  0.0631378562789 \n",
      "\t\tacc =  97.375 \n",
      "\t\tmcc =  0.948618195347\n",
      "train ( 176 ):    loss =  0.0629062102037 \n",
      "\t\tacc =  97.3721590909 \n",
      "\t\tmcc =  0.948561739253\n",
      "train ( 177 ):    loss =  0.063041286192 \n",
      "\t\tacc =  97.3693502825 \n",
      "\t\tmcc =  0.948509618899\n",
      "train ( 178 ):    loss =  0.0631102480916 \n",
      "\t\tacc =  97.3577949438 \n",
      "\t\tmcc =  0.948281557956\n",
      "train ( 179 ):    loss =  0.0630060897265 \n",
      "\t\tacc =  97.3594622905 \n",
      "\t\tmcc =  0.948303411911\n",
      "train ( 180 ):    loss =  0.0629971945936 \n",
      "\t\tacc =  97.3524305556 \n",
      "\t\tmcc =  0.948169909416\n",
      "train ( 181 ):    loss =  0.062794425213 \n",
      "\t\tacc =  97.367058011 \n",
      "\t\tmcc =  0.948456263508\n",
      "train ( 182 ):    loss =  0.0627969245474 \n",
      "\t\tacc =  97.3729395604 \n",
      "\t\tmcc =  0.948569756235\n",
      "train ( 183 ):    loss =  0.0625605889338 \n",
      "\t\tacc =  97.3787568306 \n",
      "\t\tmcc =  0.948680653369\n",
      "train ( 184 ):    loss =  0.0624046635387 \n",
      "\t\tacc =  97.3802649457 \n",
      "\t\tmcc =  0.948710626666\n",
      "train ( 185 ):    loss =  0.0624488085545 \n",
      "\t\tacc =  97.3859797297 \n",
      "\t\tmcc =  0.948820094121\n",
      "train ( 186 ):    loss =  0.0627006541295 \n",
      "\t\tacc =  97.3706317204 \n",
      "\t\tmcc =  0.948537675748\n",
      "train ( 187 ):    loss =  0.062715560349 \n",
      "\t\tacc =  97.3805147059 \n",
      "\t\tmcc =  0.948729841189\n",
      "train ( 188 ):    loss =  0.062683737546 \n",
      "\t\tacc =  97.3861369681 \n",
      "\t\tmcc =  0.948838533739\n",
      "train ( 189 ):    loss =  0.0629092020403 \n",
      "\t\tacc =  97.3958333333 \n",
      "\t\tmcc =  0.949027153499\n",
      "train ( 190 ):    loss =  0.0627196993268 \n",
      "\t\tacc =  97.4054276316 \n",
      "\t\tmcc =  0.949213229879\n",
      "train ( 191 ):    loss =  0.0626879431042 \n",
      "\t\tacc =  97.4067408377 \n",
      "\t\tmcc =  0.949238165983\n",
      "train ( 192 ):    loss =  0.0626458556234 \n",
      "\t\tacc =  97.4039713542 \n",
      "\t\tmcc =  0.94917878765\n",
      "train ( 193 ):    loss =  0.0625210361734 \n",
      "\t\tacc =  97.4133743523 \n",
      "\t\tmcc =  0.949361382998\n",
      "train ( 194 ):    loss =  0.0625386194429 \n",
      "\t\tacc =  97.4226804124 \n",
      "\t\tmcc =  0.949542252033\n",
      "train ( 195 ):    loss =  0.0625260075083 \n",
      "\t\tacc =  97.4278846154 \n",
      "\t\tmcc =  0.94963286897\n",
      "train ( 196 ):    loss =  0.0624757849061 \n",
      "\t\tacc =  97.4290497449 \n",
      "\t\tmcc =  0.949655813754\n",
      "train ( 197 ):    loss =  0.0623459029958 \n",
      "\t\tacc =  97.4341687817 \n",
      "\t\tmcc =  0.949749577139\n",
      "train ( 198 ):    loss =  0.0621183618115 \n",
      "\t\tacc =  97.4431818182 \n",
      "\t\tmcc =  0.949924678718\n",
      "train ( 199 ):    loss =  0.0619141750029 \n",
      "\t\tacc =  97.4560301508 \n",
      "\t\tmcc =  0.950176313499\n",
      "train ( 200 ):    loss =  0.0621369357578 \n",
      "\t\tacc =  97.453125 \n",
      "\t\tmcc =  0.950105111611\n",
      "train ( 201 ):    loss =  0.0620624738446 \n",
      "\t\tacc =  97.461909204 \n",
      "\t\tmcc =  0.950276169107\n",
      "train ( 202 ):    loss =  0.0624876563216 \n",
      "\t\tacc =  97.4551361386 \n",
      "\t\tmcc =  0.950138152413\n",
      "train ( 203 ):    loss =  0.0623355684163 \n",
      "\t\tacc =  97.4291871921 \n",
      "\t\tmcc =  0.94966575854\n",
      "train ( 204 ):    loss =  0.0623154189786 \n",
      "\t\tacc =  97.4149816176 \n",
      "\t\tmcc =  0.94940054788\n",
      "train ( 205 ):    loss =  0.0622797121572 \n",
      "\t\tacc =  97.4085365854 \n",
      "\t\tmcc =  0.949255494889\n",
      "train ( 206 ):    loss =  0.0624089932754 \n",
      "\t\tacc =  97.4097390777 \n",
      "\t\tmcc =  0.949279370464\n",
      "train ( 207 ):    loss =  0.0623417978053 \n",
      "\t\tacc =  97.4147041063 \n",
      "\t\tmcc =  0.949373099033\n",
      "train ( 208 ):    loss =  0.0621977848072 \n",
      "\t\tacc =  97.4196213942 \n",
      "\t\tmcc =  0.94946842619\n",
      "train ( 209 ):    loss =  0.0621208464006 \n",
      "\t\tacc =  97.4282296651 \n",
      "\t\tmcc =  0.949635475183\n",
      "train ( 210 ):    loss =  0.0623351894913 \n",
      "\t\tacc =  97.4404761905 \n",
      "\t\tmcc =  0.949875306254\n",
      "train ( 211 ):    loss =  0.0622879201639 \n",
      "\t\tacc =  97.4377962085 \n",
      "\t\tmcc =  0.94981536432\n",
      "train ( 212 ):    loss =  0.0621158435927 \n",
      "\t\tacc =  97.4204009434 \n",
      "\t\tmcc =  0.949488829177\n",
      "train ( 213 ):    loss =  0.0627506821681 \n",
      "\t\tacc =  97.3958333333 \n",
      "\t\tmcc =  0.949045404796\n",
      "train ( 214 ):    loss =  0.0627376847975 \n",
      "\t\tacc =  97.4007009346 \n",
      "\t\tmcc =  0.949139417112\n",
      "train ( 215 ):    loss =  0.0632152015986 \n",
      "\t\tacc =  97.394622093 \n",
      "\t\tmcc =  0.949015760491\n",
      "train ( 216 ):    loss =  0.0631614572486 \n",
      "\t\tacc =  97.4066840278 \n",
      "\t\tmcc =  0.949251798637\n",
      "train ( 217 ):    loss =  0.0632647779991 \n",
      "\t\tacc =  97.404233871 \n",
      "\t\tmcc =  0.949193071161\n",
      "train ( 218 ):    loss =  0.0635911661058 \n",
      "\t\tacc =  97.4089736239 \n",
      "\t\tmcc =  0.949284851329\n",
      "train ( 219 ):    loss =  0.0637021917761 \n",
      "\t\tacc =  97.4172374429 \n",
      "\t\tmcc =  0.949443290129\n",
      "train ( 220 ):    loss =  0.0637669352482 \n",
      "\t\tacc =  97.4183238636 \n",
      "\t\tmcc =  0.949464294101\n",
      "train ( 221 ):    loss =  0.0637200568481 \n",
      "\t\tacc =  97.3875848416 \n",
      "\t\tmcc =  0.948910927123\n",
      "train ( 222 ):    loss =  0.0638460740775 \n",
      "\t\tacc =  97.3782376126 \n",
      "\t\tmcc =  0.948736819297\n",
      "train ( 223 ):    loss =  0.0641573940557 \n",
      "\t\tacc =  97.365470852 \n",
      "\t\tmcc =  0.948488472472\n",
      "train ( 224 ):    loss =  0.0641126888433 \n",
      "\t\tacc =  97.3702566964 \n",
      "\t\tmcc =  0.948580774115\n",
      "train ( 225 ):    loss =  0.0639252782675 \n",
      "\t\tacc =  97.375 \n",
      "\t\tmcc =  0.948669183592\n",
      "train ( 226 ):    loss =  0.0638441842632 \n",
      "\t\tacc =  97.3727876106 \n",
      "\t\tmcc =  0.948621255904\n",
      "train ( 227 ):    loss =  0.0641099734995 \n",
      "\t\tacc =  97.3705947137 \n",
      "\t\tmcc =  0.948558585175\n",
      "train ( 228 ):    loss =  0.0639671391679 \n",
      "\t\tacc =  97.3787006579 \n",
      "\t\tmcc =  0.948714339885\n",
      "train ( 229 ):    loss =  0.0641822223735 \n",
      "\t\tacc =  97.3662663755 \n",
      "\t\tmcc =  0.948484148457\n",
      "train ( 230 ):    loss =  0.0640312056806 \n",
      "\t\tacc =  97.3539402174 \n",
      "\t\tmcc =  0.94825721858\n",
      "train ( 231 ):    loss =  0.0639972060159 \n",
      "\t\tacc =  97.3383387446 \n",
      "\t\tmcc =  0.94793552883\n",
      "train ( 232 ):    loss =  0.0642822066761 \n",
      "\t\tacc =  97.3127693966 \n",
      "\t\tmcc =  0.947474588313\n",
      "train ( 233 ):    loss =  0.0644738644764 \n",
      "\t\tacc =  97.3041845494 \n",
      "\t\tmcc =  0.94731241351\n",
      "train ( 234 ):    loss =  0.0642815250572 \n",
      "\t\tacc =  97.312366453 \n",
      "\t\tmcc =  0.947471217783\n",
      "train ( 235 ):    loss =  0.0645283480302 \n",
      "\t\tacc =  97.303856383 \n",
      "\t\tmcc =  0.947302892477\n",
      "train ( 236 ):    loss =  0.0643715155797 \n",
      "\t\tacc =  97.311970339 \n",
      "\t\tmcc =  0.947460005924\n",
      "train ( 237 ):    loss =  0.0644188842001 \n",
      "\t\tacc =  97.3200158228 \n",
      "\t\tmcc =  0.947615793518\n",
      "train ( 238 ):    loss =  0.0642501121307 \n",
      "\t\tacc =  97.3214285714 \n",
      "\t\tmcc =  0.947642052266\n",
      "train ( 239 ):    loss =  0.0640752977773 \n",
      "\t\tacc =  97.3064853556 \n",
      "\t\tmcc =  0.947354740036\n",
      "train ( 240 ):    loss =  0.0641819708698 \n",
      "\t\tacc =  97.28515625 \n",
      "\t\tmcc =  0.946956149094\n",
      "train ( 241 ):    loss =  0.0642985692402 \n",
      "\t\tacc =  97.2834543568 \n",
      "\t\tmcc =  0.946915781497\n",
      "train ( 242 ):    loss =  0.0642426234487 \n",
      "\t\tacc =  97.2849948347 \n",
      "\t\tmcc =  0.9469366011\n",
      "train ( 243 ):    loss =  0.0641296188454 \n",
      "\t\tacc =  97.2929526749 \n",
      "\t\tmcc =  0.947091070286\n",
      "train ( 244 ):    loss =  0.0643577851642 \n",
      "\t\tacc =  97.2944415984 \n",
      "\t\tmcc =  0.947114865878\n",
      "train ( 245 ):    loss =  0.0642422782717 \n",
      "\t\tacc =  97.2959183673 \n",
      "\t\tmcc =  0.947137685088\n",
      "train ( 246 ):    loss =  0.0644282926998 \n",
      "\t\tacc =  97.2942073171 \n",
      "\t\tmcc =  0.947106206447\n",
      "train ( 247 ):    loss =  0.0645255819036 \n",
      "\t\tacc =  97.2861842105 \n",
      "\t\tmcc =  0.946942589536\n",
      "train ( 248 ):    loss =  0.0644130700884 \n",
      "\t\tacc =  97.2908266129 \n",
      "\t\tmcc =  0.947028522514\n",
      "train ( 249 ):    loss =  0.0645851212355 \n",
      "\t\tacc =  97.2922941767 \n",
      "\t\tmcc =  0.947048304052\n",
      "Epoch 2 of 10 took 4891.418s\n",
      "  training loss:\t\t0.064585\n",
      "  training accuracy:\t\t97.29 %\n",
      "  training mcc:\t\t0.95\n",
      "  validation loss:\t\t0.137413\n",
      "  validation accuracy:\t\t98.49 %\n",
      "  validation mcc:\t\t0.97\n",
      "train ( 1 ):    loss =  0.048887410768 \n",
      "\t\tacc =  97.65625 \n",
      "\t\tmcc =  0.953241369575\n",
      "train ( 2 ):    loss =  0.0515823696354 \n",
      "\t\tacc =  97.65625 \n",
      "\t\tmcc =  0.953069282142\n",
      "train ( 3 ):    loss =  0.0417405716208 \n",
      "\t\tacc =  98.4375 \n",
      "\t\tmcc =  0.968712854762\n",
      "train ( 4 ):    loss =  0.0395776369448 \n",
      "\t\tacc =  98.4375 \n",
      "\t\tmcc =  0.968722110788\n",
      "train ( 5 ):    loss =  0.0364325993808 \n",
      "\t\tacc =  98.75 \n",
      "\t\tmcc =  0.97497768863\n",
      "train ( 6 ):    loss =  0.0447947075178 \n",
      "\t\tacc =  98.4375 \n",
      "\t\tmcc =  0.968807757062\n",
      "train ( 7 ):    loss =  0.0477788492211 \n",
      "\t\tacc =  98.3258928571 \n",
      "\t\tmcc =  0.966375009046\n",
      "train ( 8 ):    loss =  0.0479742480832 \n",
      "\t\tacc =  98.33984375 \n",
      "\t\tmcc =  0.966647894645\n",
      "train ( 9 ):    loss =  0.0469865160895 \n",
      "\t\tacc =  98.4375 \n",
      "\t\tmcc =  0.968625903674\n",
      "train ( 10 ):    loss =  0.0535278351704 \n",
      "\t\tacc =  98.203125 \n",
      "\t\tmcc =  0.963787039862\n",
      "train ( 11 ):    loss =  0.0528188531323 \n",
      "\t\tacc =  98.2954545455 \n",
      "\t\tmcc =  0.965644574165\n",
      "train ( 12 ):    loss =  0.0530262318552 \n",
      "\t\tacc =  98.1770833333 \n",
      "\t\tmcc =  0.963257169777\n",
      "train ( 13 ):    loss =  0.0561307128852 \n",
      "\t\tacc =  98.1370192308 \n",
      "\t\tmcc =  0.962484970652\n",
      "train ( 14 ):    loss =  0.0546968971357 \n",
      "\t\tacc =  98.2142857143 \n",
      "\t\tmcc =  0.964053899599\n",
      "train ( 15 ):    loss =  0.0546459056381 \n",
      "\t\tacc =  98.1770833333 \n",
      "\t\tmcc =  0.963343291159\n",
      "train ( 16 ):    loss =  0.052150435298 \n",
      "\t\tacc =  98.193359375 \n",
      "\t\tmcc =  0.963688331188\n",
      "train ( 17 ):    loss =  0.0519784408009 \n",
      "\t\tacc =  98.2077205882 \n",
      "\t\tmcc =  0.96399855323\n",
      "train ( 18 ):    loss =  0.0518106300986 \n",
      "\t\tacc =  98.2638888889 \n",
      "\t\tmcc =  0.965133060815\n",
      "train ( 19 ):    loss =  0.0499329068647 \n",
      "\t\tacc =  98.3552631579 \n",
      "\t\tmcc =  0.966968162877\n",
      "train ( 20 ):    loss =  0.0499999836035 \n",
      "\t\tacc =  98.3203125 \n",
      "\t\tmcc =  0.966240092141\n",
      "train ( 21 ):    loss =  0.0482220416387 \n",
      "\t\tacc =  98.3630952381 \n",
      "\t\tmcc =  0.967109382276\n",
      "train ( 22 ):    loss =  0.048198338303 \n",
      "\t\tacc =  98.2599431818 \n",
      "\t\tmcc =  0.965170411335\n",
      "train ( 23 ):    loss =  0.049155008751 \n",
      "\t\tacc =  98.2336956522 \n",
      "\t\tmcc =  0.964678888046\n",
      "train ( 24 ):    loss =  0.0479908235204 \n",
      "\t\tacc =  98.2096354167 \n",
      "\t\tmcc =  0.964199473445\n",
      "train ( 25 ):    loss =  0.0466543612053 \n",
      "\t\tacc =  98.25 \n",
      "\t\tmcc =  0.964986940748\n",
      "train ( 26 ):    loss =  0.0463198760717 \n",
      "\t\tacc =  98.2572115385 \n",
      "\t\tmcc =  0.965126960373\n",
      "train ( 27 ):    loss =  0.0452415615154 \n",
      "\t\tacc =  98.2928240741 \n",
      "\t\tmcc =  0.965836403663\n",
      "train ( 28 ):    loss =  0.0447589369611 \n",
      "\t\tacc =  98.2700892857 \n",
      "\t\tmcc =  0.965404099637\n",
      "train ( 29 ):    loss =  0.0443259904934 \n",
      "\t\tacc =  98.3297413793 \n",
      "\t\tmcc =  0.966597061718\n",
      "train ( 30 ):    loss =  0.0464503268864 \n",
      "\t\tacc =  98.28125 \n",
      "\t\tmcc =  0.965678251198\n",
      "train ( 31 ):    loss =  0.045443075998 \n",
      "\t\tacc =  98.3366935484 \n",
      "\t\tmcc =  0.966785404385\n",
      "train ( 32 ):    loss =  0.0495756552711 \n",
      "\t\tacc =  98.2421875 \n",
      "\t\tmcc =  0.96492691407\n",
      "train ( 33 ):    loss =  0.04838262126 \n",
      "\t\tacc =  98.2954545455 \n",
      "\t\tmcc =  0.965989734856\n",
      "train ( 34 ):    loss =  0.0490521681437 \n",
      "\t\tacc =  98.2996323529 \n",
      "\t\tmcc =  0.966068895132\n",
      "train ( 35 ):    loss =  0.0490181654273 \n",
      "\t\tacc =  98.3482142857 \n",
      "\t\tmcc =  0.967038355271\n",
      "train ( 36 ):    loss =  0.0486975032764 \n",
      "\t\tacc =  98.3289930556 \n",
      "\t\tmcc =  0.966675512783\n",
      "train ( 37 ):    loss =  0.0499492916015 \n",
      "\t\tacc =  98.2685810811 \n",
      "\t\tmcc =  0.965541074155\n",
      "train ( 38 ):    loss =  0.0507278944475 \n",
      "\t\tacc =  98.2319078947 \n",
      "\t\tmcc =  0.964775301423\n",
      "train ( 39 ):    loss =  0.0508263602565 \n",
      "\t\tacc =  98.1770833333 \n",
      "\t\tmcc =  0.96373124157\n",
      "train ( 40 ):    loss =  0.0499698295007 \n",
      "\t\tacc =  98.203125 \n",
      "\t\tmcc =  0.964245008761\n",
      "train ( 41 ):    loss =  0.049790889956 \n",
      "\t\tacc =  98.1707317073 \n",
      "\t\tmcc =  0.963586046556\n",
      "train ( 42 ):    loss =  0.0501383726619 \n",
      "\t\tacc =  98.1956845238 \n",
      "\t\tmcc =  0.96408208568\n",
      "train ( 43 ):    loss =  0.0502618173237 \n",
      "\t\tacc =  98.2194767442 \n",
      "\t\tmcc =  0.964556634116\n",
      "train ( 44 ):    loss =  0.0502174148111 \n",
      "\t\tacc =  98.2244318182 \n",
      "\t\tmcc =  0.964651764448\n",
      "train ( 45 ):    loss =  0.0502826905075 \n",
      "\t\tacc =  98.1944444444 \n",
      "\t\tmcc =  0.964078890842\n",
      "train ( 46 ):    loss =  0.0496447698898 \n",
      "\t\tacc =  98.1827445652 \n",
      "\t\tmcc =  0.96386403974\n",
      "train ( 47 ):    loss =  0.0494812201491 \n",
      "\t\tacc =  98.1382978723 \n",
      "\t\tmcc =  0.963021676404\n",
      "train ( 48 ):    loss =  0.050376815214 \n",
      "\t\tacc =  98.0794270833 \n",
      "\t\tmcc =  0.961925950352\n",
      "train ( 49 ):    loss =  0.0501555336829 \n",
      "\t\tacc =  98.0707908163 \n",
      "\t\tmcc =  0.96176819096\n",
      "train ( 50 ):    loss =  0.050407729929 \n",
      "\t\tacc =  98.078125 \n",
      "\t\tmcc =  0.961907216192\n",
      "train ( 51 ):    loss =  0.0508861015496 \n",
      "\t\tacc =  98.0851715686 \n",
      "\t\tmcc =  0.962040039095\n",
      "train ( 52 ):    loss =  0.0511707131688 \n",
      "\t\tacc =  98.0919471154 \n",
      "\t\tmcc =  0.96216893005\n",
      "train ( 53 ):    loss =  0.0510515841337 \n",
      "\t\tacc =  98.0100235849 \n",
      "\t\tmcc =  0.960610548167\n",
      "train ( 54 ):    loss =  0.0506057705818 \n",
      "\t\tacc =  97.974537037 \n",
      "\t\tmcc =  0.959947530728\n",
      "train ( 55 ):    loss =  0.0502888162994 \n",
      "\t\tacc =  97.8835227273 \n",
      "\t\tmcc =  0.958280178208\n",
      "train ( 56 ):    loss =  0.050308629813 \n",
      "\t\tacc =  97.8376116071 \n",
      "\t\tmcc =  0.957424531475\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-cc62ba845b43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#print accurarcy on training sample:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfnn_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtrain_mcc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmcc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/cori/software/python/2.7-anaconda/envs/deeplearning/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs=10\n",
    "batchsize=128\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0.\n",
    "    train_acc = 0.\n",
    "    train_mcc = 0.\n",
    "    train_batches = 0.\n",
    "    start_time = time.time()\n",
    "    for batch in hditer_train.next_batch(batchsize):\n",
    "        inputs, targets = batch\n",
    "        train_err += fnn_train(inputs, targets)\n",
    "        train_batches += 1.\n",
    "        \n",
    "        #print accurarcy on training sample:\n",
    "        _, acc, mcc = fnn_validate(inputs, targets)\n",
    "        train_acc += acc\n",
    "        train_mcc += mcc\n",
    "        \n",
    "        #debugging output\n",
    "        print 'train (',int(train_batches),'):    loss = ', train_err/train_batches,'\\n',\\\n",
    "                                            '\\t\\tacc = ', train_acc/train_batches*100.,'\\n', \\\n",
    "                                            '\\t\\tmcc = ',train_mcc/train_batches\n",
    "        \n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0.\n",
    "    val_acc = 0.\n",
    "    val_mcc = 0.\n",
    "    val_batches = 0.\n",
    "    for batch in hditer_validation.next_batch(batchsize):\n",
    "        inputs, targets = batch            \n",
    "        err, acc, mcc = fnn_validate(inputs, targets)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_mcc += mcc\n",
    "        val_batches += 1.\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  training accuracy:\\t\\t{:.2f} %\".format(train_acc / train_batches * 100.))\n",
    "    print(\"  training mcc:\\t\\t{:.2f}\".format(train_mcc / train_batches ))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(val_acc / val_batches * 100.))\n",
    "    print(\"  validation mcc:\\t\\t{:.2f}\".format(val_mcc / val_batches ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d32a70f85f9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtest_batches\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtargets_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_batches\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatchsize_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest_batches\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatchsize_test\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfnn_det\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mtargets_gt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_batches\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatchsize_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest_batches\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatchsize_test\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/cori/software/python/2.7-anaconda/envs/deeplearning/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "#run on test data and compute ROC:\n",
    "test_err = 0.\n",
    "test_acc = 0.\n",
    "test_batches = 0\n",
    "batchsize_test=100\n",
    "\n",
    "targets_pred = np.zeros((hditer_test.num_examples,))\n",
    "targets_gt = np.zeros((hditer_test.num_examples,))\n",
    "\n",
    "for batch in hditer_test.next_batch(batchsize_test):\n",
    "    inputs, targets = batch\n",
    "    err, acc, _ = fnn_validate(inputs,targets)\n",
    "    test_err+=err\n",
    "    test_acc+=acc\n",
    "    test_batches+=1\n",
    "    \n",
    "    targets_pred[(test_batches-1)*batchsize_test:test_batches*batchsize_test] = fnn_det(inputs)[:,1]\n",
    "    targets_gt[(test_batches-1)*batchsize_test:test_batches*batchsize_test] = targets[:]\n",
    "\n",
    "#accuracies\n",
    "print(\"  test loss:\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(test_acc / test_batches * 100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ROC curve\n",
    "#ROC\n",
    "fpr, tpr, thresholds = metrics.roc_curve(targets_gt, targets_pred, pos_label=1)\n",
    "plt.figure()\n",
    "lw = 2\n",
    "#full curve\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % metrics.auc(fpr,tpr))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ROC_1400_850.png',dpi=300)\n",
    "\n",
    "#zoomed-in\n",
    "#plt.plot(fpr, tpr, color='darkorange',\n",
    "#         lw=lw, label='ROC curve (area = %0.2f)' % metrics.auc(fpr,tpr))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 0.01])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"center right\")\n",
    "plt.savefig('ROC_1400_850_zoom.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_example(x):\n",
    "    plt.imshow(np.log10(x).T,extent=[-3.15, 3.15, -5, 5], interpolation='none',aspect='auto', origin='low')\n",
    "    plt.colorbar()\n",
    "\n",
    "#for batch in hditer_validation:\n",
    "#    inputs,targets=batch\n",
    "#    plot_example(inputs[0,0,:,:])\n",
    "#    break\n",
    "\n",
    "for batch in hditer_train:\n",
    "    inputs,targets=batch\n",
    "    plot_example(inputs[0,0,:,:])\n",
    "    break;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [deeplearning]",
   "language": "python",
   "name": "Python [deeplearning]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
