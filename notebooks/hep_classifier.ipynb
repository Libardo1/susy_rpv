{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__author__ = 'tkurth'\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from nbfinder import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "%matplotlib inline\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theano modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import theano.sandbox.rng_mrg\n",
    "Trng = theano.sandbox.rng_mrg.MRG_RandomStreams(9)\n",
    "import lasagne as ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROOT stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "require(['codemirror/mode/clike/clike'], function(Clike) { console.log('ROOTaaS - C++ CodeMirror module loaded'); });"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "IPython.CodeCell.config_defaults.highlight_modes['magic_text/x-c++src'] = {'reg':[/^%%cpp/]};"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to ROOTaaS 6.06/06\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('/global/homes/w/wbhimji/cori-envs/nersc-rootpy/lib/python2.7/site-packages/')\n",
    "sys.path.append('/global/common/cori/software/root/6.06.06/lib/root')\n",
    "import ROOT\n",
    "import rootpy\n",
    "import root_numpy as rnp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a context manager to suppress stdout and stderr.\n",
    "class suppress_stdout_stderr(object):\n",
    "    '''\n",
    "    A context manager for doing a \"deep suppression\" of stdout and stderr in \n",
    "    Python, i.e. will suppress all print, even if the print originates in a \n",
    "    compiled C/Fortran sub-function.\n",
    "       This will not suppress raised exceptions, since exceptions are printed\n",
    "    to stderr just before a script exits, and after the context manager has\n",
    "    exited (at least, I think that is why it lets exceptions through).      \n",
    "\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # Open a pair of null files\n",
    "        self.null_fds =  [os.open(os.devnull,os.O_RDWR) for x in range(2)]\n",
    "        # Save the actual stdout (1) and stderr (2) file descriptors.\n",
    "        self.save_fds = (os.dup(1), os.dup(2))\n",
    "\n",
    "    def __enter__(self):\n",
    "        # Assign the null pointers to stdout and stderr.\n",
    "        os.dup2(self.null_fds[0],1)\n",
    "        os.dup2(self.null_fds[1],2)\n",
    "\n",
    "    def __exit__(self, *_):\n",
    "        # Re-assign the real stdout/stderr back to (1) and (2)\n",
    "        os.dup2(self.save_fds[0],1)\n",
    "        os.dup2(self.save_fds[1],2)\n",
    "        # Close the null files\n",
    "        os.close(self.null_fds[0])\n",
    "        os.close(self.null_fds[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader and preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(bg_cfg_file = '../config/BgFileListAug16.txt',\n",
    "                sig_cfg_file='../config/SignalFileListAug16.txt',\n",
    "                num_files=10,  \n",
    "                group_name='CollectionTree',\n",
    "                branches=['CaloCalTopoClustersAuxDyn.calPhi', \\\n",
    "                          'CaloCalTopoClustersAuxDyn.calEta', \\\n",
    "                          'CaloCalTopoClustersAuxDyn.calE'],\n",
    "                dataset_name='histo',\n",
    "                type_='root'):\n",
    "\n",
    "    #get list of files\n",
    "    bg_files = [line.rstrip() for line in open(bg_cfg_file)]\n",
    "    sig_files = [line.rstrip() for line in open(sig_cfg_file)]\n",
    "    \n",
    "    #so we don't have annoying stderr messages\n",
    "    with suppress_stdout_stderr():\n",
    "            \n",
    "        #bgarray has n_events groups of 3 parallel numpy arrays \n",
    "        #(each numpy within a group is of equal length and each array corresponds to phi, eta and the corresponding energy)\n",
    "        bgarray = rnp.root2array(bg_files[:num_files], \\\n",
    "                                treename=group_name, \\\n",
    "                                branches=branches, \\\n",
    "                                start=0, \\\n",
    "                                warn_missing_tree=True)\n",
    "\n",
    "        sigarray = rnp.root2array(sig_files[:num_files],\\\n",
    "                                treename=group_name,\\\n",
    "                                branches=branches,\\\n",
    "                                start=0, \\\n",
    "                                warn_missing_tree=True)\n",
    "        \n",
    "    #create dataframe with all entries\n",
    "    #store in dataframe\n",
    "    bgdf = pd.DataFrame.from_records(bgarray)\n",
    "    bgdf['label']=0\n",
    "    sigdf = pd.DataFrame.from_records(sigarray)\n",
    "    sigdf['label']=1\n",
    "    \n",
    "    #concat\n",
    "    return pd.concat([bgdf,sigdf])\n",
    "\n",
    "\n",
    "#preprocessor\n",
    "def preprocess_data(df,eta_range,phi_range,eta_bins,phi_bins):\n",
    "    #empty array\n",
    "    xvals = np.zeros((df.shape[0], 1, phi_bins, eta_bins ),dtype='float32')\n",
    "    yvals = np.zeros((df.shape[0],),dtype='int32')\n",
    "    \n",
    "    for i in range(df.shape[0]):        \n",
    "        phi, eta, E =  df.iloc[i]['CaloCalTopoClustersAuxDyn.calPhi'],\\\n",
    "                       df.iloc[i]['CaloCalTopoClustersAuxDyn.calEta'],\\\n",
    "                       df.iloc[i]['CaloCalTopoClustersAuxDyn.calE']\n",
    "        \n",
    "        xvals[i]=np.histogram2d(phi,eta,\n",
    "                                bins=(phi_bins, eta_bins), \\\n",
    "                                weights=E,\n",
    "                                range=[phi_range,eta_range])[0]\n",
    "        yvals[i]=df.iloc[i]['label']\n",
    "        \n",
    "    return xvals, yvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class hep_data_iterator:\n",
    "    \n",
    "    #class constructor\n",
    "    def __init__(self,\n",
    "                 datadf,\n",
    "                 max_frequency=None,\n",
    "                 shuffle=True,\n",
    "                 bin_size=0.025,\n",
    "                 eta_range = [-5,5],\n",
    "                 phi_range = [-3.14, 3.14]\n",
    "                ):\n",
    "\n",
    "        #set parameters\n",
    "        self.shuffle = shuffle\n",
    "        self.bin_size = bin_size\n",
    "        self.eta_range = eta_range\n",
    "        self.phi_range = phi_range\n",
    "        \n",
    "        #compute bins\n",
    "        self.eta_bins = int(np.floor((self.eta_range[1] - self.eta_range[0]) / self.bin_size))\n",
    "        self.phi_bins = int(np.floor((self.phi_range[1] - self.phi_range[0]) / self.bin_size))\n",
    "        \n",
    "        #dataframe\n",
    "        self.df = datadf\n",
    "        self.df.sort_values(by='label',inplace=True)\n",
    "        \n",
    "        #make class frequencies even:\n",
    "        tmpdf=self.df.groupby('label').count().reset_index()\n",
    "        self.num_classes=tmpdf.shape[0]\n",
    "        \n",
    "        #determine minimum frequency\n",
    "        min_frequency=tmpdf['CaloCalTopoClustersAuxDyn.calE'].min()\n",
    "        if max_frequency:\n",
    "            min_frequency=np.min([min_frequency,max_frequency])\n",
    "        tmpdf=self.df.groupby(['label']).apply(lambda x: x[['CaloCalTopoClustersAuxDyn.calPhi', \\\n",
    "                                                            'CaloCalTopoClustersAuxDyn.calEta', \\\n",
    "                                                            'CaloCalTopoClustersAuxDyn.calE']].iloc[:min_frequency,:]).copy()\n",
    "        tmpdf.reset_index(inplace=True)\n",
    "        del tmpdf['level_1']\n",
    "        self.df=tmpdf.copy()\n",
    "        \n",
    "        #compute max:\n",
    "        self.compute_data_max()\n",
    "        \n",
    "        #shuffle if wanted (highly recommended)\n",
    "        if self.shuffle:\n",
    "            self.df=self.df.reindex(np.random.permutation(self.df.index))\n",
    "        \n",
    "        #number of examples\n",
    "        self.num_examples=self.df.shape[0]\n",
    "        \n",
    "        #shapes:\n",
    "        self.xshape=(1, self.phi_bins, self.eta_bins)\n",
    "        \n",
    "    \n",
    "    #compute max over all data\n",
    "    def compute_data_max(self):\n",
    "        '''compute the maximum over all event entries for rescaling data between -1 and 1'''\n",
    "        self.max_abs=(self.df['CaloCalTopoClustersAuxDyn.calE'].abs()).apply(lambda x: np.max(x)).max()\n",
    "    \n",
    "    \n",
    "    #this is the batch iterator:\n",
    "    def next_batch(self,batchsize):\n",
    "        '''batch iterator'''\n",
    "        \n",
    "        #shuffle:\n",
    "        if self.shuffle:\n",
    "            self.df=self.df.reindex(np.random.permutation(self.df.index))\n",
    "        \n",
    "        #iterate\n",
    "        for idx in range(0,self.num_examples-batchsize,batchsize):\n",
    "            #yield next batch\n",
    "            x,y=preprocess_data(self.df.iloc[idx:idx+batchsize,:],\\\n",
    "                             self.eta_range,\n",
    "                             self.phi_range,\n",
    "                             self.eta_bins,self.phi_bins)\n",
    "            #rescale x:\n",
    "            x/=self.max_abs\n",
    "        \n",
    "            #return result\n",
    "            yield x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct data iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#parameters\n",
    "train_fraction=0.8\n",
    "binsize=0.1\n",
    "numfiles=2\n",
    "\n",
    "#load data\n",
    "datadf=load_data(num_files=numfiles)\n",
    "\n",
    "#create views for different labels\n",
    "sigdf=datadf[ datadf.label==1 ]\n",
    "bgdf=datadf[ datadf.label==0 ]\n",
    "\n",
    "#split the sets\n",
    "num_sig_train=int(np.floor(sigdf.shape[0]*train_fraction))\n",
    "num_bg_train=int(np.floor(bgdf.shape[0]*train_fraction))\n",
    "traindf=pd.concat([bgdf.iloc[:num_bg_train],sigdf.iloc[:num_sig_train]])\n",
    "validdf=pd.concat([bgdf.iloc[num_bg_train:],sigdf.iloc[num_sig_train:]])\n",
    "\n",
    "#create iterators\n",
    "hditer_train=hep_data_iterator(traindf,max_frequency=2000,bin_size=binsize)\n",
    "hditer_validation=hep_data_iterator(validdf,max_frequency=500,bin_size=binsize)\n",
    "\n",
    "#the preprocessing for the validation iterator has to be taken from the training iterator\n",
    "hditer_validation.max_abs=hditer_train.max_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print hditer_train.num_examples\n",
    "print hditer_validation.num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct classification network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#some parameters\n",
    "keep_prob=0.5\n",
    "num_filters=128\n",
    "num_units_dense=1024\n",
    "initial_learning_rate=0.001\n",
    "\n",
    "#input layer\n",
    "l_inp_data = ls.layers.InputLayer((None,hditer_train.xshape[0],hditer_train.xshape[1],hditer_train.xshape[2]))\n",
    "l_inp_label = ls.layers.InputLayer((None,1))\n",
    "\n",
    "#conv layers\n",
    "#first layer\n",
    "l_conv1 = ls.layers.Conv2DLayer(incoming=l_inp_data,\n",
    "                                num_filters=num_filters,\n",
    "                                filter_size=3,\n",
    "                                stride=(1,1),\n",
    "                                pad=0,\n",
    "                                W=ls.init.HeUniform(),\n",
    "                                b=ls.init.Constant(0.),\n",
    "                                nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                               )\n",
    "l_drop1 = ls.layers.DropoutLayer(incoming=l_conv1,\n",
    "                       p=keep_prob,\n",
    "                       rescale=True\n",
    "                      )\n",
    "l_pool1 = ls.layers.MaxPool2DLayer(incoming=l_drop1,\n",
    "                                   pool_size=(2,2),\n",
    "                                   stride=2,\n",
    "                                   pad=0                                   \n",
    "                                  )\n",
    "\n",
    "#second layer:\n",
    "l_conv2 = ls.layers.Conv2DLayer(incoming=l_pool1,\n",
    "                                num_filters=num_filters,\n",
    "                                filter_size=3,\n",
    "                                stride=(1,1),\n",
    "                                pad=0,\n",
    "                                W=ls.init.HeUniform(),\n",
    "                                b=ls.init.Constant(0.),\n",
    "                                nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                               )\n",
    "l_drop2 = ls.layers.DropoutLayer(incoming=l_conv2,\n",
    "                       p=keep_prob,\n",
    "                       rescale=True\n",
    "                      )\n",
    "l_pool2 = ls.layers.MaxPool2DLayer(incoming=l_drop2,\n",
    "                                   pool_size=(2,2),\n",
    "                                   stride=2,\n",
    "                                   pad=0                                   \n",
    "                                  )\n",
    "\n",
    "#third layer:\n",
    "l_conv3 = ls.layers.Conv2DLayer(incoming=l_pool2,\n",
    "                                num_filters=num_filters,\n",
    "                                filter_size=3,\n",
    "                                stride=(1,1),\n",
    "                                pad=0,\n",
    "                                W=ls.init.HeUniform(),\n",
    "                                b=ls.init.Constant(0.),\n",
    "                                nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                               )\n",
    "\n",
    "l_drop3 = ls.layers.DropoutLayer(incoming=l_conv3,\n",
    "                       p=keep_prob,\n",
    "                       rescale=True\n",
    "                      )\n",
    "l_pool3 = ls.layers.MaxPool2DLayer(incoming=l_drop3,\n",
    "                                   pool_size=(2,2),\n",
    "                                   stride=2,\n",
    "                                   pad=0                                   \n",
    "                                  )\n",
    "\n",
    "#fourth layer:\n",
    "l_conv4 = ls.layers.Conv2DLayer(incoming=l_pool3,\n",
    "                                num_filters=num_filters,\n",
    "                                filter_size=3,\n",
    "                                stride=(1,1),\n",
    "                                pad=0,\n",
    "                                W=ls.init.HeUniform(),\n",
    "                                b=ls.init.Constant(0.),\n",
    "                                nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                               )\n",
    "l_drop4 = ls.layers.DropoutLayer(incoming=l_conv4,\n",
    "                       p=keep_prob,\n",
    "                       rescale=True\n",
    "                      )\n",
    "l_pool4 = ls.layers.MaxPool2DLayer(incoming=l_drop4,\n",
    "                                   pool_size=(2,2),\n",
    "                                   stride=2,\n",
    "                                   pad=0                                   \n",
    "                                  )\n",
    "\n",
    "#flatten\n",
    "l_flat = ls.layers.FlattenLayer(incoming=l_pool4, \n",
    "                                outdim=2)\n",
    "\n",
    "#crossfire\n",
    "l_fc1 = ls.layers.DenseLayer(incoming=l_flat, \n",
    "                             num_units=num_units_dense, \n",
    "                             W=ls.init.GlorotUniform(np.sqrt(2./(1+0.01**2))), \n",
    "                             b=ls.init.Constant(0.0),\n",
    "                             nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                            )\n",
    "\n",
    "l_drop5 = ls.layers.DropoutLayer(incoming=l_fc1,\n",
    "                       p=keep_prob,\n",
    "                       rescale=True\n",
    "                      )\n",
    "\n",
    "l_fc2 = ls.layers.DenseLayer(incoming=l_drop5, \n",
    "                             num_units=num_units_dense, \n",
    "                             W=ls.init.GlorotUniform(np.sqrt(2./(1+0.01**2))), \n",
    "                             b=ls.init.Constant(0.0),\n",
    "                             nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                            )\n",
    "\n",
    "l_drop6 = ls.layers.DropoutLayer(incoming=l_fc2,\n",
    "                       p=keep_prob,\n",
    "                       rescale=True\n",
    "                      )\n",
    "\n",
    "#output layer\n",
    "l_out = ls.layers.DenseLayer(incoming=l_drop6, \n",
    "                             num_units=hditer_train.num_classes, \n",
    "                             W=ls.init.GlorotUniform(np.sqrt(2./(1+0.01**2))), \n",
    "                             b=ls.init.Constant(0.0),\n",
    "                             nonlinearity=ls.nonlinearities.softmax\n",
    "                            )\n",
    "\n",
    "#network\n",
    "network = [l_inp_data, l_inp_label,\n",
    "           l_conv1, l_pool1, l_drop1,\n",
    "           l_conv2, l_pool2, l_drop2,\n",
    "           l_conv3, l_pool3, l_drop3,\n",
    "           l_conv4, l_pool4, l_drop4,\n",
    "           l_flat, \n",
    "           l_fc1, l_drop5,\n",
    "           l_fc2, l_drop6,\n",
    "           l_out\n",
    "          ]\n",
    "\n",
    "#variables\n",
    "inp = l_inp_data.input_var\n",
    "lab = T.ivector('lab')\n",
    "\n",
    "#output\n",
    "lab_pred = ls.layers.get_output(l_out, {l_inp_data: inp})\n",
    "lab_pred_det = ls.layers.get_output(l_out, {l_inp_data: inp}, deterministic=True)\n",
    "\n",
    "#loss functions:\n",
    "loss = ls.objectives.categorical_crossentropy(lab_pred,lab).mean()\n",
    "loss_det = ls.objectives.categorical_crossentropy(lab_pred_det,lab).mean()\n",
    "\n",
    "#accuracy\n",
    "acc_det = ls.objectives.categorical_accuracy(lab_pred_det, lab, top_k=1).mean()\n",
    "\n",
    "#parameters\n",
    "params = ls.layers.get_all_params(network, trainable=True)\n",
    "\n",
    "#updates\n",
    "updates = ls.updates.adam(loss, params, learning_rate=initial_learning_rate)\n",
    "\n",
    "#compile network function\n",
    "fnn = theano.function([inp], lab_pred)\n",
    "#training function to minimize\n",
    "fnn_train = theano.function([inp,lab], loss, updates=updates)\n",
    "#validation function with accuracy\n",
    "fnn_validate = theano.function([inp,lab], [loss_det,acc_det])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  0.854359550711 45.3125\n",
      "train:  0.782403738396 51.953125\n",
      "train:  0.685646399585 67.96875\n",
      "train:  0.622731683213 74.8046875\n",
      "train:  0.584275504751 77.1875\n",
      "train:  0.554349988524 79.9479166667\n",
      "train:  0.525538357265 82.7008928571\n",
      "train:  0.496564382816 84.86328125\n",
      "train:  0.46795178187 86.5451388889\n",
      "train:  0.437657944332 87.65625\n",
      "train:  0.404696759609 88.28125\n",
      "train:  0.375400879578 89.2578125\n",
      "train:  0.346973025819 90.0841346154\n",
      "train:  0.322288511504 90.7924107143\n",
      "train:  0.300828393725 91.40625\n",
      "train:  0.282065372324 91.845703125\n",
      "train:  0.265485904139 92.2794117647\n",
      "train:  0.253942789474 92.6649305556\n",
      "train:  0.240765811252 93.0509868421\n",
      "train:  0.230311852725 93.3984375\n",
      "train:  0.219344622115 93.6755952381\n",
      "train:  0.20937455789 93.9275568182\n",
      "train:  0.213049573047 94.1576086957\n",
      "train:  0.204172507503 94.4010416667\n",
      "train:  0.197861171316 94.625\n",
      "train:  0.190251126266 94.8317307692\n",
      "train:  0.183204969927 94.994212963\n",
      "train:  0.17668089892 95.1171875\n",
      "train:  0.172719714047 95.2586206897\n",
      "train:  0.167228705645 95.4166666667\n",
      "train:  0.16183423158 95.564516129\n",
      "Epoch 1 of 10 took 298.102s\n",
      "  training loss:\t\t0.161834\n",
      "  training accuracy:\t\t95.56 %\n",
      "  validation loss:\t\t0.003678\n",
      "  validation accuracy:\t\t99.89 %\n",
      "train:  7.47720907579e-09 100.0\n",
      "train:  1.58917178586e-07 100.0\n",
      "train:  2.00211330305e-05 100.0\n",
      "train:  0.0322414090326 99.8046875\n",
      "train:  0.0696987764417 99.84375\n",
      "train:  0.0616545775128 99.4791666667\n",
      "train:  0.0614007426491 99.21875\n",
      "train:  0.0838511837163 99.12109375\n",
      "train:  0.079590832258 99.21875\n",
      "train:  0.0775522870953 99.296875\n",
      "train:  0.0841114998199 99.2897727273\n",
      "train:  0.078880318762 99.3489583333\n",
      "train:  0.0896192782508 99.3389423077\n",
      "train:  0.084019124331 99.3861607143\n",
      "train:  0.0794273660033 99.4270833333\n",
      "train:  0.0745686176564 99.462890625\n",
      "train:  0.0701956040872 99.4944852941\n",
      "train:  0.0663331239652 99.5225694444\n",
      "train:  0.0628522651942 99.5476973684\n",
      "train:  0.0597316022548 99.5703125\n",
      "train:  0.0569617271075 99.5907738095\n",
      "train:  0.0559952906034 99.609375\n",
      "train:  0.0547948469204 99.6263586957\n",
      "train:  0.0574794109982 99.609375\n",
      "train:  0.0586844249232 99.625\n",
      "train:  0.0564470112333 99.6394230769\n",
      "train:  0.0543615824148 99.6527777778\n",
      "train:  0.052421989323 99.6651785714\n",
      "train:  0.050686219315 99.5959051724\n",
      "train:  0.0491459711486 99.5833333333\n",
      "train:  0.0475629926591 99.5967741935\n",
      "Epoch 2 of 10 took 296.214s\n",
      "  training loss:\t\t0.047563\n",
      "  training accuracy:\t\t99.60 %\n",
      "  validation loss:\t\t0.068068\n",
      "  validation accuracy:\t\t97.88 %\n",
      "train:  0.0717968112081 97.65625\n",
      "train:  0.0362236318577 98.828125\n",
      "train:  0.0243345085646 99.21875\n",
      "train:  0.0182996075495 99.4140625\n",
      "train:  0.0146567553171 99.53125\n",
      "train:  0.0122503418166 99.609375\n",
      "train:  0.0105244315271 99.6651785714\n",
      "train:  0.00923277730349 99.70703125\n",
      "train:  0.00821887616923 99.7395833333\n",
      "train:  0.00742643622204 99.765625\n",
      "train:  0.0067806906963 99.7159090909\n",
      "train:  0.00624358729097 99.7395833333\n",
      "train:  0.00577845655927 99.7596153846\n",
      "train:  0.00544511932723 99.7767857143\n",
      "train:  0.00510869055264 99.7916666667\n",
      "train:  0.00486014233746 99.8046875\n",
      "train:  0.00459835179112 99.8161764706\n",
      "train:  0.00436167913229 99.8263888889\n",
      "train:  0.0100169711613 99.7944078947\n",
      "train:  0.00960038687768 99.8046875\n",
      "train:  0.00933765864722 99.8139880952\n",
      "train:  0.00893243291715 99.8224431818\n",
      "train:  0.00859297675385 99.8301630435\n",
      "train:  0.00824314537832 99.8372395833\n",
      "train:  0.00792624378215 99.84375\n",
      "train:  0.00762817293667 99.8497596154\n",
      "train:  0.00735427135845 99.8553240741\n",
      "train:  0.0070964165518 99.8604910714\n",
      "train:  0.0068806896437 99.8653017241\n",
      "train:  0.00666084145987 99.84375\n",
      "train:  0.00645591334404 99.8487903226\n",
      "Epoch 3 of 10 took 295.665s\n",
      "  training loss:\t\t0.006456\n",
      "  training accuracy:\t\t99.85 %\n",
      "  validation loss:\t\t0.041968\n",
      "  validation accuracy:\t\t99.33 %\n",
      "train:  0.00116697281216 100.0\n",
      "train:  0.000855906098654 100.0\n",
      "train:  0.000783020138837 99.7395833333\n",
      "train:  0.00136311952742 99.609375\n",
      "train:  0.00117439052596 99.53125\n",
      "train:  0.00123426347098 99.609375\n",
      "train:  0.00124357838363 99.4419642857\n",
      "train:  0.00110195853861 99.51171875\n",
      "train:  0.000987878907664 99.5659722222\n",
      "train:  0.000901182238752 99.609375\n",
      "train:  0.000828684832916 99.6448863636\n",
      "train:  0.000766589307151 99.6744791667\n",
      "train:  0.000715461214778 99.6995192308\n",
      "train:  0.000671179052428 99.7209821429\n",
      "train:  0.000633662710719 99.7395833333\n",
      "train:  0.000687057909374 99.755859375\n",
      "train:  0.000652276537399 99.7702205882\n",
      "train:  0.000631064724024 99.7395833333\n",
      "train:  0.000685686880166 99.7532894737\n",
      "train:  0.000655032197035 99.765625\n",
      "train:  0.00482008518438 99.7395833333\n",
      "train:  0.00463051866676 99.7159090909\n",
      "train:  0.00443386283072 99.7282608696\n",
      "train:  0.00425390011604 99.7395833333\n",
      "train:  0.00408764077982 99.75\n",
      "train:  0.00393327136291 99.7295673077\n",
      "train:  0.00379107075081 99.7395833333\n",
      "train:  0.00366405793863 99.7488839286\n",
      "train:  0.00354018594699 99.7575431034\n",
      "train:  0.00348979630955 99.7135416667\n",
      "train:  0.00340842287424 99.6723790323\n",
      "Epoch 4 of 10 took 295.369s\n",
      "  training loss:\t\t0.003408\n",
      "  training accuracy:\t\t99.67 %\n",
      "  validation loss:\t\t0.035245\n",
      "  validation accuracy:\t\t99.55 %\n",
      "train:  0.0241108615679 100.0\n",
      "train:  0.0125247269076 99.21875\n",
      "train:  0.00842753553543 99.4791666667\n",
      "train:  0.00682822170378 99.21875\n",
      "train:  0.00585466438666 99.21875\n",
      "train:  0.00504275945386 99.21875\n",
      "train:  0.00632635528381 99.1071428571\n",
      "train:  0.00554932215733 99.21875\n",
      "train:  0.0049490352207 99.3055555556\n",
      "train:  0.0044619590883 99.375\n",
      "train:  0.00407100158339 99.4318181818\n",
      "train:  0.00386852882016 99.4791666667\n",
      "train:  0.00357611625282 99.5192307692\n",
      "train:  0.00333950953804 99.5535714286\n",
      "train:  0.00313389973088 99.5833333333\n",
      "train:  0.00294453692668 99.609375\n",
      "train:  0.00278135404693 99.6323529412\n",
      "train:  0.00267866826256 99.6527777778\n",
      "train:  0.00267679692362 99.6710526316\n",
      "train:  0.00256470250028 99.6484375\n",
      "train:  0.00245104601409 99.6651785714\n",
      "train:  0.0023608872161 99.6803977273\n",
      "train:  0.00226140307685 99.6942934783\n",
      "train:  0.00217727220999 99.70703125\n",
      "train:  0.00210739407983 99.71875\n",
      "train:  0.00202998179799 99.6995192308\n",
      "train:  0.00196707019063 99.7106481481\n",
      "train:  0.00190138020583 99.7209821429\n",
      "train:  0.00184009972711 99.7306034483\n",
      "train:  0.00178409347984 99.7395833333\n",
      "train:  0.00173823030298 99.747983871\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3121409e3b42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhditer_validation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfnn_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mval_err\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mval_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/cori/software/python/2.7-anaconda/envs/deeplearning/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs=10\n",
    "batchsize=128\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0.\n",
    "    train_acc = 0.\n",
    "    train_batches = 0.\n",
    "    start_time = time.time()\n",
    "    for batch in hditer_train.next_batch(batchsize):\n",
    "        inputs, targets = batch\n",
    "        train_err += fnn_train(inputs, targets)\n",
    "        train_batches += 1.\n",
    "        \n",
    "        #print accurarcy on training sample:\n",
    "        _, acc = fnn_validate(inputs, targets)\n",
    "        train_acc += acc\n",
    "        \n",
    "        #debugging output\n",
    "        print 'train: ', train_err/train_batches, train_acc/train_batches*100.\n",
    "        \n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0.\n",
    "    val_acc = 0.\n",
    "    val_batches = 0.\n",
    "    for batch in hditer_validation.next_batch(batchsize):\n",
    "        inputs, targets = batch            \n",
    "        err, acc = fnn_validate(inputs, targets)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1.\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  training accuracy:\\t\\t{:.2f} %\".format(train_acc / train_batches * 100.))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(val_acc / val_batches * 100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_example(x):\n",
    "    plt.imshow(np.log10(x).T,extent=[-3.15, 3.15, -5, 5], interpolation='none',aspect='auto', origin='low')\n",
    "    plt.colorbar()\n",
    "\n",
    "#for batch in hditer_validation:\n",
    "#    inputs,targets=batch\n",
    "#    plot_example(inputs[0,0,:,:])\n",
    "#    break\n",
    "\n",
    "for batch in hditer_train:\n",
    "    inputs,targets=batch\n",
    "    plot_example(inputs[0,0,:,:])\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [deeplearning]",
   "language": "python",
   "name": "Python [deeplearning]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
