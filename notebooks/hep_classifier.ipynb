{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__author__ = 'tkurth'\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from nbfinder import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "%matplotlib inline\n",
    "import time\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theano modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import theano.sandbox.rng_mrg\n",
    "Trng = theano.sandbox.rng_mrg.MRG_RandomStreams(9)\n",
    "import lasagne as ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROOT stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "require(['codemirror/mode/clike/clike'], function(Clike) { console.log('ROOTaaS - C++ CodeMirror module loaded'); });"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "IPython.CodeCell.config_defaults.highlight_modes['magic_text/x-c++src'] = {'reg':[/^%%cpp/]};"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to ROOTaaS 6.06/06\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('/global/homes/w/wbhimji/cori-envs/nersc-rootpy/lib/python2.7/site-packages/')\n",
    "sys.path.append('/global/common/cori/software/root/6.06.06/lib/root')\n",
    "import ROOT\n",
    "import rootpy\n",
    "import root_numpy as rnp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a context manager to suppress stdout and stderr.\n",
    "class suppress_stdout_stderr(object):\n",
    "    '''\n",
    "    A context manager for doing a \"deep suppression\" of stdout and stderr in \n",
    "    Python, i.e. will suppress all print, even if the print originates in a \n",
    "    compiled C/Fortran sub-function.\n",
    "       This will not suppress raised exceptions, since exceptions are printed\n",
    "    to stderr just before a script exits, and after the context manager has\n",
    "    exited (at least, I think that is why it lets exceptions through).      \n",
    "\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # Open a pair of null files\n",
    "        self.null_fds =  [os.open(os.devnull,os.O_RDWR) for x in range(2)]\n",
    "        # Save the actual stdout (1) and stderr (2) file descriptors.\n",
    "        self.save_fds = (os.dup(1), os.dup(2))\n",
    "\n",
    "    def __enter__(self):\n",
    "        # Assign the null pointers to stdout and stderr.\n",
    "        os.dup2(self.null_fds[0],1)\n",
    "        os.dup2(self.null_fds[1],2)\n",
    "\n",
    "    def __exit__(self, *_):\n",
    "        # Re-assign the real stdout/stderr back to (1) and (2)\n",
    "        os.dup2(self.save_fds[0],1)\n",
    "        os.dup2(self.save_fds[1],2)\n",
    "        # Close the null files\n",
    "        os.close(self.null_fds[0])\n",
    "        os.close(self.null_fds[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader and preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_dicts(dict1,dict2):\n",
    "    tmp = dict1.copy()\n",
    "    tmp.update(dict2)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#file string parser\n",
    "def parse_filename(fname,directory='.'):\n",
    "    directory=re.sub(r'^(.*?)(/+)$',r'\\1',directory)\n",
    "    \n",
    "    #signal file?\n",
    "    smatch=re.compile(r'GG_RPV(.*?)_(.*?)_(.*?)\\.merge')\n",
    "    tmpres=smatch.findall(fname)\n",
    "    if tmpres:\n",
    "        tmpres=tmpres[0]\n",
    "        return {'rpv':int(tmpres[0]), 'mass1':int(tmpres[1]), 'mass2':int(tmpres[2]), 'name':directory+'/'+fname}\n",
    "\n",
    "    #background file?\n",
    "    smatch=re.compile(r'JZ(.*?)\\.merge')\n",
    "    tmpres=smatch.findall(fname)\n",
    "    if tmpres:\n",
    "        return {'jz':int(tmpres[0]), 'name':directory+'/'+fname}\n",
    "\n",
    "    #nothing at all\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(filelists,\n",
    "                group_name='CollectionTree',\n",
    "                branches=['CaloCalTopoClustersAuxDyn.calPhi', \\\n",
    "                          'CaloCalTopoClustersAuxDyn.calEta', \\\n",
    "                          'CaloCalTopoClustersAuxDyn.calE'],\n",
    "                dataset_name='histo',\n",
    "                type_='root'):\n",
    "    \n",
    "    #iterate over elements in the filelists\n",
    "    records=[]\n",
    "    for fname in filelists:\n",
    "        #read specifics of that list\n",
    "        masterrec=parse_filename(fname.split('/')[-1])\n",
    "        #determine if it is label or background\n",
    "        if 'jz' in masterrec.keys():\n",
    "            masterrec['label']=0\n",
    "        else:\n",
    "            masterrec['label']=1\n",
    "        \n",
    "        #read the files in the filelist\n",
    "        files = [line.rstrip() for line in open(fname)]\n",
    "        \n",
    "        #we don't want annoying stderr messages\n",
    "        with suppress_stdout_stderr():\n",
    "            \n",
    "            #bgarray has n_events groups of 3 parallel numpy arrays \n",
    "            #(each numpy within a group is of equal length and each array corresponds to phi, eta and the corresponding energy)\n",
    "            try:\n",
    "                datarec = rnp.root2array(files, \\\n",
    "                                        treename=group_name, \\\n",
    "                                        branches=branches, \\\n",
    "                                        start=0, \\\n",
    "                                        warn_missing_tree=True)\n",
    "                tmpdf=pd.DataFrame.from_records(datarec)\n",
    "                reclist=tmpdf[['CaloCalTopoClustersAuxDyn.calPhi', \\\n",
    "                                'CaloCalTopoClustersAuxDyn.calEta', \\\n",
    "                                'CaloCalTopoClustersAuxDyn.calE']].to_dict('records')\n",
    "                reclist=[merge_dicts(masterrec,rec) for rec in reclist]\n",
    "                    \n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "        #append to records\n",
    "        records+=reclist\n",
    "            \n",
    "    #return dataframe\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "#preprocessor\n",
    "def preprocess_data(df,eta_range,phi_range,eta_bins,phi_bins):\n",
    "    #empty array\n",
    "    xvals = np.zeros((df.shape[0], 1, phi_bins, eta_bins ),dtype='float32')\n",
    "    yvals = np.zeros((df.shape[0],),dtype='int32')\n",
    "    \n",
    "    for i in range(df.shape[0]):        \n",
    "        phi, eta, E =  df.iloc[i]['CaloCalTopoClustersAuxDyn.calPhi'],\\\n",
    "                       df.iloc[i]['CaloCalTopoClustersAuxDyn.calEta'],\\\n",
    "                       df.iloc[i]['CaloCalTopoClustersAuxDyn.calE']\n",
    "        \n",
    "        xvals[i]=np.histogram2d(phi,eta,\n",
    "                                bins=(phi_bins, eta_bins), \\\n",
    "                                weights=E,\n",
    "                                range=[phi_range,eta_range])[0]\n",
    "        yvals[i]=df.iloc[i]['label']\n",
    "        \n",
    "    return xvals, yvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class hep_data_iterator:\n",
    "    \n",
    "    #class constructor\n",
    "    def __init__(self,\n",
    "                 datadf,\n",
    "                 max_frequency=None,\n",
    "                 even_frequencies=True,\n",
    "                 shuffle=True,\n",
    "                 nbins=(100,100),\n",
    "                 eta_range = [-5,5],\n",
    "                 phi_range = [-3.1416, 3.1416]\n",
    "                ):\n",
    "\n",
    "        #set parameters\n",
    "        self.shuffle = shuffle\n",
    "        self.nbins = nbins\n",
    "        self.eta_range = eta_range\n",
    "        self.phi_range = phi_range\n",
    "        self.even_frequencies = even_frequencies\n",
    "        \n",
    "        #compute bins\n",
    "        self.eta_bins = self.nbins[0]\n",
    "        self.phi_bins = self.nbins[1]\n",
    "        \n",
    "        #dataframe\n",
    "        self.df = datadf\n",
    "        self.df.sort_values(by='label',inplace=True)\n",
    "        \n",
    "        #make class frequencies even:\n",
    "        tmpdf=self.df.groupby('label').count().reset_index()\n",
    "        self.num_classes=tmpdf.shape[0]\n",
    "        \n",
    "        #determine minimum frequency\n",
    "        min_frequency=tmpdf['CaloCalTopoClustersAuxDyn.calE'].min()\n",
    "        if max_frequency:\n",
    "            min_frequency=np.min([min_frequency,max_frequency])\n",
    "        elif not even_frequencies:\n",
    "            min_frequency=-1\n",
    "        tmpdf=self.df.groupby(['label']).apply(lambda x: x[['CaloCalTopoClustersAuxDyn.calPhi', \\\n",
    "                                                            'CaloCalTopoClustersAuxDyn.calEta', \\\n",
    "                                                            'CaloCalTopoClustersAuxDyn.calE']].iloc[:min_frequency,:]).copy()\n",
    "        tmpdf.reset_index(inplace=True)\n",
    "        del tmpdf['level_1']\n",
    "        \n",
    "        #copy tmpdf into self.df\n",
    "        self.df=tmpdf.copy()\n",
    "        \n",
    "        #compute max:\n",
    "        self.compute_data_max()\n",
    "        \n",
    "        #shuffle if wanted (highly recommended)\n",
    "        if self.shuffle:\n",
    "            self.df=self.df.reindex(np.random.permutation(self.df.index))\n",
    "        \n",
    "        #number of examples\n",
    "        self.num_examples=self.df.shape[0]\n",
    "        \n",
    "        #shapes:\n",
    "        self.xshape=(1, self.phi_bins, self.eta_bins)\n",
    "        \n",
    "    \n",
    "    #compute max over all data\n",
    "    def compute_data_max(self):\n",
    "        '''compute the maximum over all event entries for rescaling data between -1 and 1'''\n",
    "        self.max_abs=(self.df['CaloCalTopoClustersAuxDyn.calE'].abs()).apply(lambda x: np.max(x)).max()\n",
    "    \n",
    "    \n",
    "    #this is the batch iterator:\n",
    "    def next_batch(self,batchsize):\n",
    "        '''batch iterator'''\n",
    "        \n",
    "        #shuffle:\n",
    "        if self.shuffle:\n",
    "            self.df=self.df.reindex(np.random.permutation(self.df.index))\n",
    "        \n",
    "        #iterate\n",
    "        for idx in range(0,self.num_examples-batchsize,batchsize):\n",
    "            #yield next batch\n",
    "            x,y=preprocess_data(self.df.iloc[idx:idx+batchsize,:],\\\n",
    "                             self.eta_range,\n",
    "                             self.phi_range,\n",
    "                             self.eta_bins,self.phi_bins)\n",
    "            #rescale x:\n",
    "            x/=self.max_abs\n",
    "        \n",
    "            #return result\n",
    "            yield x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curate File list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directory='/project/projectdirs/das/wbhimji/RPVSusyJetLearn/atlas_dl/config/'\n",
    "filelists=[parse_filename(x,directory) for x in os.listdir(directory) if x.startswith('mc')]\n",
    "filenamedf=pd.DataFrame(filelists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#select signal configuration\n",
    "mass1=1400\n",
    "mass2=850\n",
    "sig_cfg_files=list(filenamedf[ (filenamedf['mass1']==1400) & (filenamedf['mass2']==850) ]['name'])\n",
    "\n",
    "#select background configuration\n",
    "jzmin=4\n",
    "jzmax=5\n",
    "bg_cfg_files=list(filenamedf[ (filenamedf['jz']>=jzmin) & (filenamedf['jz']<=jzmax) ]['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct data iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load background files\n",
    "bgdf=load_data(bg_cfg_files)\n",
    "bgdf=bgdf.reindex(np.random.permutation(bgdf.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load signal data\n",
    "sigdf=load_data(sig_cfg_files)\n",
    "sigdf=sigdf.reindex(np.random.permutation(sigdf.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#parameters\n",
    "train_fraction=0.8\n",
    "validation_fraction=0.1\n",
    "nbins=(100,100)\n",
    "\n",
    "#create sizes:\n",
    "num_sig_train=int(np.floor(sigdf.shape[0]*train_fraction))\n",
    "#num_bg_train=int(np.floor(bgdf.shape[0]*train_fraction))\n",
    "num_bg_train=num_sig_train\n",
    "num_sig_validation=int(np.floor(sigdf.shape[0]*validation_fraction))\n",
    "#num_bg_validation=int(np.floor(bgdf.shape[0]*validation_fraction))\n",
    "num_bg_validation=num_sig_validation\n",
    "\n",
    "#split the sets\n",
    "traindf=pd.concat([bgdf.iloc[:num_bg_train],sigdf.iloc[:num_sig_train]])\n",
    "validdf=pd.concat([bgdf.iloc[num_bg_train:num_bg_train+num_bg_validation], \\\n",
    "                   sigdf.iloc[num_sig_train:num_sig_train+num_sig_validation]])\n",
    "testdf=pd.concat([bgdf.iloc[num_bg_train+num_bg_validation:], \\\n",
    "                   sigdf.iloc[num_sig_train+num_sig_validation:]])\n",
    "\n",
    "#create iterators\n",
    "hditer_train=hep_data_iterator(traindf,nbins=nbins)\n",
    "hditer_validation=hep_data_iterator(validdf,nbins=nbins)\n",
    "hditer_test=hep_data_iterator(testdf,nbins=nbins,even_frequencies=False)\n",
    "\n",
    "#the preprocessing for the validation iterator has to be taken from the training iterator\n",
    "hditer_validation.max_abs=hditer_train.max_abs\n",
    "hditer_test.max_abs=hditer_train.max_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n",
      "4000\n",
      "3165384\n"
     ]
    }
   ],
   "source": [
    "print hditer_train.num_examples\n",
    "print hditer_validation.num_examples\n",
    "print hditer_test.num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Matthews correlation coefficient objective, only for binary classifications\n",
    "def matthews_correlation_coefficient(predictions, targets):\n",
    "    #preprocess\n",
    "    if targets.ndim == predictions.ndim:\n",
    "        targets = T.argmax(targets, axis=-1)\n",
    "    #make predictions flat as well:\n",
    "    predictions = T.argmax(predictions, axis=-1)\n",
    "    \n",
    "    #true predictions\n",
    "    true_pred=T.eq(predictions, targets)\n",
    "    #false predictions\n",
    "    false_pred=T.neq(predictions, targets)\n",
    "    \n",
    "    #true positives:\n",
    "    tp=(true_pred*predictions).sum()\n",
    "    #false positives:\n",
    "    fp=(false_pred*predictions).sum()\n",
    "    #true negatives\n",
    "    tn=(true_pred*(1-predictions)).sum()\n",
    "    #false negatives\n",
    "    fn=(false_pred*(1-predictions)).sum()\n",
    "    \n",
    "    #now, assemble ratio\n",
    "    mcc=(tp*tn-fp*fn)/T.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "    \n",
    "    return mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#debug\n",
    "ypred=T.matrix('ypred')\n",
    "ytrue=T.imatrix('ytrue')\n",
    "mccdummy=matthews_correlation_coefficient(ypred,ytrue)\n",
    "mcc=theano.function([ypred,ytrue],mccdummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1.0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcc(np.asarray([[0.3,0.7],[0.8,0.2],[0.56,0.44],[0.1,0.9]],dtype=np.float32),np.asarray([[0,1],[1,0],[1,0],[0,1]],dtype=np.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct classification network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#some parameters\n",
    "keep_prob=0.5\n",
    "num_filters=128\n",
    "num_units_dense=1024\n",
    "initial_learning_rate=0.001\n",
    "\n",
    "#input layer\n",
    "l_inp_data = ls.layers.InputLayer((None,hditer_train.xshape[0],hditer_train.xshape[1],hditer_train.xshape[2]))\n",
    "l_inp_label = ls.layers.InputLayer((None,1))\n",
    "\n",
    "#conv layers\n",
    "#first layer\n",
    "l_conv1 = ls.layers.Conv2DLayer(incoming=l_inp_data,\n",
    "                                num_filters=num_filters,\n",
    "                                filter_size=3,\n",
    "                                stride=(1,1),\n",
    "                                pad=0,\n",
    "                                W=ls.init.HeUniform(),\n",
    "                                b=ls.init.Constant(0.),\n",
    "                                nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                               )\n",
    "l_drop1 = ls.layers.DropoutLayer(incoming=l_conv1,\n",
    "                       p=keep_prob,\n",
    "                       rescale=True\n",
    "                      )\n",
    "l_pool1 = ls.layers.MaxPool2DLayer(incoming=l_drop1,\n",
    "                                   pool_size=(2,2),\n",
    "                                   stride=2,\n",
    "                                   pad=0                                   \n",
    "                                  )\n",
    "\n",
    "#second layer:\n",
    "l_conv2 = ls.layers.Conv2DLayer(incoming=l_pool1,\n",
    "                                num_filters=num_filters,\n",
    "                                filter_size=3,\n",
    "                                stride=(1,1),\n",
    "                                pad=0,\n",
    "                                W=ls.init.HeUniform(),\n",
    "                                b=ls.init.Constant(0.),\n",
    "                                nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                               )\n",
    "l_drop2 = ls.layers.DropoutLayer(incoming=l_conv2,\n",
    "                       p=keep_prob,\n",
    "                       rescale=True\n",
    "                      )\n",
    "l_pool2 = ls.layers.MaxPool2DLayer(incoming=l_drop2,\n",
    "                                   pool_size=(2,2),\n",
    "                                   stride=2,\n",
    "                                   pad=0                                   \n",
    "                                  )\n",
    "\n",
    "#third layer:\n",
    "l_conv3 = ls.layers.Conv2DLayer(incoming=l_pool2,\n",
    "                                num_filters=num_filters,\n",
    "                                filter_size=3,\n",
    "                                stride=(1,1),\n",
    "                                pad=0,\n",
    "                                W=ls.init.HeUniform(),\n",
    "                                b=ls.init.Constant(0.),\n",
    "                                nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                               )\n",
    "l_drop3 = ls.layers.DropoutLayer(incoming=l_conv3,\n",
    "                       p=keep_prob,\n",
    "                       rescale=True\n",
    "                      )\n",
    "l_pool3 = ls.layers.MaxPool2DLayer(incoming=l_drop3,\n",
    "                                   pool_size=(2,2),\n",
    "                                   stride=2,\n",
    "                                   pad=0                                   \n",
    "                                  )\n",
    "\n",
    "#fourth layer:\n",
    "l_conv4 = ls.layers.Conv2DLayer(incoming=l_pool3,\n",
    "                                num_filters=num_filters,\n",
    "                                filter_size=3,\n",
    "                                stride=(1,1),\n",
    "                                pad=0,\n",
    "                                W=ls.init.HeUniform(),\n",
    "                                b=ls.init.Constant(0.),\n",
    "                                nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                               )\n",
    "l_drop4 = ls.layers.DropoutLayer(incoming=l_conv4,\n",
    "                       p=keep_prob,\n",
    "                       rescale=True\n",
    "                      )\n",
    "l_pool4 = ls.layers.MaxPool2DLayer(incoming=l_drop4,\n",
    "                                   pool_size=(2,2),\n",
    "                                   stride=2,\n",
    "                                   pad=0                                   \n",
    "                                  )\n",
    "\n",
    "#flatten\n",
    "l_flat = ls.layers.FlattenLayer(incoming=l_pool4, \n",
    "                                outdim=2)\n",
    "\n",
    "#crossfire\n",
    "l_fc1 = ls.layers.DenseLayer(incoming=l_flat, \n",
    "                             num_units=num_units_dense, \n",
    "                             W=ls.init.GlorotUniform(np.sqrt(2./(1+0.01**2))), \n",
    "                             b=ls.init.Constant(0.0),\n",
    "                             nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                            )\n",
    "\n",
    "l_drop5 = ls.layers.DropoutLayer(incoming=l_fc1,\n",
    "                       p=keep_prob,\n",
    "                       rescale=True\n",
    "                      )\n",
    "\n",
    "l_fc2 = ls.layers.DenseLayer(incoming=l_drop5, \n",
    "                             num_units=num_units_dense, \n",
    "                             W=ls.init.GlorotUniform(np.sqrt(2./(1+0.01**2))), \n",
    "                             b=ls.init.Constant(0.0),\n",
    "                             nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                            )\n",
    "\n",
    "l_drop6 = ls.layers.DropoutLayer(incoming=l_fc2,\n",
    "                       p=keep_prob,\n",
    "                       rescale=True\n",
    "                      )\n",
    "\n",
    "#output layer\n",
    "l_out = ls.layers.DenseLayer(incoming=l_drop6, \n",
    "                             num_units=hditer_train.num_classes, \n",
    "                             W=ls.init.GlorotUniform(np.sqrt(2./(1+0.01**2))), \n",
    "                             b=ls.init.Constant(0.0),\n",
    "                             nonlinearity=ls.nonlinearities.softmax\n",
    "                            )\n",
    "\n",
    "#network\n",
    "network = [l_inp_data, l_inp_label,\n",
    "           l_conv1, l_pool1, l_drop1,\n",
    "           l_conv2, l_pool2, l_drop2,\n",
    "           l_conv3, l_pool3, l_drop3,\n",
    "           l_conv4, l_pool4, l_drop4,\n",
    "           l_flat, \n",
    "           l_fc1, l_drop5,\n",
    "           l_fc2, l_drop6,\n",
    "           l_out\n",
    "          ]\n",
    "\n",
    "#variables\n",
    "inp = l_inp_data.input_var\n",
    "lab = T.ivector('lab')\n",
    "\n",
    "#output\n",
    "lab_pred = ls.layers.get_output(l_out, {l_inp_data: inp})\n",
    "lab_pred_det = ls.layers.get_output(l_out, {l_inp_data: inp}, deterministic=True)\n",
    "\n",
    "#loss functions:\n",
    "loss = ls.objectives.categorical_crossentropy(lab_pred,lab).mean()\n",
    "loss_det = ls.objectives.categorical_crossentropy(lab_pred_det,lab).mean()\n",
    "\n",
    "#accuracy\n",
    "acc_det = ls.objectives.categorical_accuracy(lab_pred_det, lab, top_k=1).mean()\n",
    "\n",
    "#MCC\n",
    "mcc_det = matthews_correlation_coefficient(lab_pred_det,lab)\n",
    "\n",
    "#parameters\n",
    "params = ls.layers.get_all_params(network, trainable=True)\n",
    "\n",
    "#updates\n",
    "updates = ls.updates.adam(loss, params, learning_rate=initial_learning_rate)\n",
    "\n",
    "#compile network function\n",
    "fnn = theano.function([inp], lab_pred)\n",
    "fnn_det = theano.function([inp], lab_pred_det)\n",
    "#training function to minimize\n",
    "fnn_train = theano.function([inp,lab], loss, updates=updates)\n",
    "#validation function with accuracy\n",
    "fnn_validate = theano.function([inp,lab], [loss_det,acc_det,mcc_det])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load from which files\n",
    "load_model=False\n",
    "paramsfile_load=\"model_parameters.pick\"\n",
    "updatesfile_load=\"model_updates.pick\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if load_model:\n",
    "    #load updates\n",
    "    with open(updatesfile_load,'rb') as f:\n",
    "        values1=pickle.load(f)\n",
    "        f.close()\n",
    "        for p, value in zip(updates.keys(), values1):\n",
    "            p.set_value(value)\n",
    "    \n",
    "    #load parameters\n",
    "    with open(paramsfile_load,'rb') as f:\n",
    "        values2=pickle.load(f)\n",
    "        f.close()\n",
    "        ls.layers.set_all_param_values(network, values2, trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_model=True\n",
    "num_epochs=10\n",
    "batchsize=128\n",
    "paramsfile_savebest=\"model_parameters_best.pick\"\n",
    "updatesfile_savebest=\"model_updates_best.pick\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train ( 1 ):    loss =  0.96379935226 \n",
      "\t\tacc =  65.625 \n",
      "\t\tmcc =  0.334967162775\n",
      "train ( 2 ):    loss =  0.888624478457 \n",
      "\t\tacc =  58.984375 \n",
      "\t\tmcc =  0.153595091038\n",
      "train ( 3 ):    loss =  0.860940022202 \n",
      "\t\tacc =  53.3854166667 \n",
      "\t\tmcc =  0.127261482562\n",
      "train ( 4 ):    loss =  0.777782839709 \n",
      "\t\tacc =  53.515625 \n",
      "\t\tmcc =  0.119062690466\n",
      "train ( 5 ):    loss =  0.727395518277 \n",
      "\t\tacc =  53.4375 \n",
      "\t\tmcc =  0.113849624516\n",
      "train ( 6 ):    loss =  0.68872341148 \n",
      "\t\tacc =  52.734375 \n",
      "\t\tmcc =  nan\n",
      "train ( 7 ):    loss =  0.660796649795 \n",
      "\t\tacc =  52.5669642857 \n",
      "\t\tmcc =  nan\n",
      "train ( 8 ):    loss =  0.638084911948 \n",
      "\t\tacc =  52.34375 \n",
      "\t\tmcc =  nan\n",
      "train ( 9 ):    loss =  0.616621912827 \n",
      "\t\tacc =  53.0381944444 \n",
      "\t\tmcc =  nan\n",
      "train ( 10 ):    loss =  0.595859858057 \n",
      "\t\tacc =  53.75 \n",
      "\t\tmcc =  nan\n",
      "train ( 11 ):    loss =  0.581060501551 \n",
      "\t\tacc =  53.8352272727 \n",
      "\t\tmcc =  nan\n",
      "train ( 12 ):    loss =  0.562785103939 \n",
      "\t\tacc =  53.515625 \n",
      "\t\tmcc =  nan\n",
      "train ( 13 ):    loss =  0.546734454823 \n",
      "\t\tacc =  53.4855769231 \n",
      "\t\tmcc =  nan\n",
      "train ( 14 ):    loss =  0.533559778372 \n",
      "\t\tacc =  53.7388392857 \n",
      "\t\tmcc =  nan\n",
      "train ( 15 ):    loss =  0.520483249146 \n",
      "\t\tacc =  53.90625 \n",
      "\t\tmcc =  nan\n",
      "train ( 16 ):    loss =  0.50674673958 \n",
      "\t\tacc =  54.1015625 \n",
      "\t\tmcc =  nan\n",
      "train ( 17 ):    loss =  0.491234995281 \n",
      "\t\tacc =  54.1360294118 \n",
      "\t\tmcc =  nan\n",
      "train ( 18 ):    loss =  0.482045763403 \n",
      "\t\tacc =  53.6892361111 \n",
      "\t\tmcc =  nan\n",
      "train ( 19 ):    loss =  0.469732399554 \n",
      "\t\tacc =  53.3717105263 \n",
      "\t\tmcc =  nan\n",
      "train ( 20 ):    loss =  0.46065509533 \n",
      "\t\tacc =  53.4375 \n",
      "\t\tmcc =  nan\n",
      "train ( 21 ):    loss =  0.451715261183 \n",
      "\t\tacc =  53.3482142857 \n",
      "\t\tmcc =  nan\n",
      "train ( 22 ):    loss =  0.443819434063 \n",
      "\t\tacc =  53.6931818182 \n",
      "\t\tmcc =  nan\n",
      "train ( 23 ):    loss =  0.436914522862 \n",
      "\t\tacc =  54.0421195652 \n",
      "\t\tmcc =  nan\n",
      "train ( 24 ):    loss =  0.42973712793 \n",
      "\t\tacc =  53.9713541667 \n",
      "\t\tmcc =  nan\n",
      "train ( 25 ):    loss =  0.423163377572 \n",
      "\t\tacc =  53.9375 \n",
      "\t\tmcc =  nan\n",
      "train ( 26 ):    loss =  0.415796548514 \n",
      "\t\tacc =  53.6358173077 \n",
      "\t\tmcc =  nan\n",
      "train ( 27 ):    loss =  0.410838523662 \n",
      "\t\tacc =  53.587962963 \n",
      "\t\tmcc =  nan\n",
      "train ( 28 ):    loss =  0.405286947218 \n",
      "\t\tacc =  53.2924107143 \n",
      "\t\tmcc =  nan\n",
      "train ( 29 ):    loss =  0.400544370142 \n",
      "\t\tacc =  53.0172413793 \n",
      "\t\tmcc =  nan\n",
      "train ( 30 ):    loss =  0.394870700487 \n",
      "\t\tacc =  53.0989583333 \n",
      "\t\tmcc =  nan\n",
      "train ( 31 ):    loss =  0.390323223474 \n",
      "\t\tacc =  52.9485887097 \n",
      "\t\tmcc =  nan\n",
      "train ( 32 ):    loss =  0.385103216191 \n",
      "\t\tacc =  52.83203125 \n",
      "\t\tmcc =  nan\n",
      "train ( 33 ):    loss =  0.381307459759 \n",
      "\t\tacc =  52.7225378788 \n",
      "\t\tmcc =  nan\n",
      "train ( 34 ):    loss =  0.376133061607 \n",
      "\t\tacc =  52.7803308824 \n",
      "\t\tmcc =  nan\n",
      "train ( 35 ):    loss =  0.372027171077 \n",
      "\t\tacc =  53.0357142857 \n",
      "\t\tmcc =  nan\n",
      "train ( 36 ):    loss =  0.367880721756 \n",
      "\t\tacc =  53.0164930556 \n",
      "\t\tmcc =  nan\n",
      "train ( 37 ):    loss =  0.363866604353 \n",
      "\t\tacc =  53.3783783784 \n",
      "\t\tmcc =  nan\n",
      "train ( 38 ):    loss =  0.359814928648 \n",
      "\t\tacc =  53.6595394737 \n",
      "\t\tmcc =  nan\n",
      "train ( 39 ):    loss =  0.356175410502 \n",
      "\t\tacc =  54.0865384615 \n",
      "\t\tmcc =  nan\n",
      "train ( 40 ):    loss =  0.352500146968 \n",
      "\t\tacc =  54.453125 \n",
      "\t\tmcc =  nan\n",
      "train ( 41 ):    loss =  0.349670620404 \n",
      "\t\tacc =  54.8208841463 \n",
      "\t\tmcc =  nan\n",
      "train ( 42 ):    loss =  0.346359259882 \n",
      "\t\tacc =  55.3013392857 \n",
      "\t\tmcc =  nan\n",
      "train ( 43 ):    loss =  0.343677461762 \n",
      "\t\tacc =  55.6140988372 \n",
      "\t\tmcc =  nan\n",
      "train ( 44 ):    loss =  0.340672757681 \n",
      "\t\tacc =  56.1612215909 \n",
      "\t\tmcc =  nan\n",
      "train ( 45 ):    loss =  0.337504706434 \n",
      "\t\tacc =  56.8576388889 \n",
      "\t\tmcc =  nan\n",
      "train ( 46 ):    loss =  0.334552679672 \n",
      "\t\tacc =  57.5747282609 \n",
      "\t\tmcc =  nan\n",
      "train ( 47 ):    loss =  0.331209397486 \n",
      "\t\tacc =  58.1449468085 \n",
      "\t\tmcc =  nan\n",
      "train ( 48 ):    loss =  0.32830144017 \n",
      "\t\tacc =  58.8704427083 \n",
      "\t\tmcc =  nan\n",
      "train ( 49 ):    loss =  0.325599249739 \n",
      "\t\tacc =  59.518494898 \n",
      "\t\tmcc =  nan\n",
      "train ( 50 ):    loss =  0.322940161855 \n",
      "\t\tacc =  60.09375 \n",
      "\t\tmcc =  nan\n",
      "train ( 51 ):    loss =  0.321649585713 \n",
      "\t\tacc =  60.6158088235 \n",
      "\t\tmcc =  nan\n",
      "train ( 52 ):    loss =  0.319241995092 \n",
      "\t\tacc =  61.1177884615 \n",
      "\t\tmcc =  nan\n",
      "train ( 53 ):    loss =  0.317080048938 \n",
      "\t\tacc =  61.4976415094 \n",
      "\t\tmcc =  nan\n",
      "train ( 54 ):    loss =  0.315233217498 \n",
      "\t\tacc =  61.6898148148 \n",
      "\t\tmcc =  nan\n",
      "train ( 55 ):    loss =  0.313232445215 \n",
      "\t\tacc =  61.9034090909 \n",
      "\t\tmcc =  nan\n",
      "train ( 56 ):    loss =  0.311644638815 \n",
      "\t\tacc =  61.9559151786 \n",
      "\t\tmcc =  nan\n",
      "train ( 57 ):    loss =  0.309280072173 \n",
      "\t\tacc =  62.0614035088 \n",
      "\t\tmcc =  nan\n",
      "train ( 58 ):    loss =  0.307365015056 \n",
      "\t\tacc =  62.0420258621 \n",
      "\t\tmcc =  nan\n",
      "train ( 59 ):    loss =  0.305509727951 \n",
      "\t\tacc =  62.0762711864 \n",
      "\t\tmcc =  nan\n",
      "train ( 60 ):    loss =  0.303453170725 \n",
      "\t\tacc =  61.9921875 \n",
      "\t\tmcc =  nan\n",
      "train ( 61 ):    loss =  0.301427463586 \n",
      "\t\tacc =  61.9108606557 \n",
      "\t\tmcc =  nan\n",
      "train ( 62 ):    loss =  0.299763362989 \n",
      "\t\tacc =  61.7565524194 \n",
      "\t\tmcc =  nan\n",
      "train ( 63 ):    loss =  0.298259574376 \n",
      "\t\tacc =  61.6939484127 \n",
      "\t\tmcc =  nan\n",
      "train ( 64 ):    loss =  0.296645925572 \n",
      "\t\tacc =  61.474609375 \n",
      "\t\tmcc =  nan\n",
      "train ( 65 ):    loss =  0.295095496653 \n",
      "\t\tacc =  61.2379807692 \n",
      "\t\tmcc =  nan\n",
      "train ( 66 ):    loss =  0.293351225363 \n",
      "\t\tacc =  61.1505681818 \n",
      "\t\tmcc =  nan\n",
      "train ( 67 ):    loss =  0.291819086467 \n",
      "\t\tacc =  60.9258395522 \n",
      "\t\tmcc =  nan\n",
      "train ( 68 ):    loss =  0.290921222812 \n",
      "\t\tacc =  60.5813419118 \n",
      "\t\tmcc =  nan\n",
      "train ( 69 ):    loss =  0.289406568983 \n",
      "\t\tacc =  60.3940217391 \n",
      "\t\tmcc =  nan\n",
      "train ( 70 ):    loss =  0.288150082329 \n",
      "\t\tacc =  60.3459821429 \n",
      "\t\tmcc =  nan\n",
      "train ( 71 ):    loss =  0.287046281697 \n",
      "\t\tacc =  60.1782570423 \n",
      "\t\tmcc =  nan\n",
      "train ( 72 ):    loss =  0.28567287001 \n",
      "\t\tacc =  60.0368923611 \n",
      "\t\tmcc =  nan\n",
      "train ( 73 ):    loss =  0.284292597598 \n",
      "\t\tacc =  59.8886986301 \n",
      "\t\tmcc =  nan\n",
      "train ( 74 ):    loss =  0.283110722089 \n",
      "\t\tacc =  59.7972972973 \n",
      "\t\tmcc =  nan\n",
      "train ( 75 ):    loss =  0.281731475184 \n",
      "\t\tacc =  59.6458333333 \n",
      "\t\tmcc =  nan\n",
      "train ( 76 ):    loss =  0.280661885156 \n",
      "\t\tacc =  59.5600328947 \n",
      "\t\tmcc =  nan\n",
      "train ( 77 ):    loss =  0.279801202658 \n",
      "\t\tacc =  59.4054383117 \n",
      "\t\tmcc =  nan\n",
      "train ( 78 ):    loss =  0.278799916097 \n",
      "\t\tacc =  59.2648237179 \n",
      "\t\tmcc =  nan\n",
      "train ( 79 ):    loss =  0.277980208739 \n",
      "\t\tacc =  59.1574367089 \n",
      "\t\tmcc =  nan\n",
      "train ( 80 ):    loss =  0.276734604363 \n",
      "\t\tacc =  59.08203125 \n",
      "\t\tmcc =  nan\n",
      "train ( 81 ):    loss =  0.275680777252 \n",
      "\t\tacc =  58.9023919753 \n",
      "\t\tmcc =  nan\n",
      "train ( 82 ):    loss =  0.274793049563 \n",
      "\t\tacc =  58.6985518293 \n",
      "\t\tmcc =  nan\n",
      "train ( 83 ):    loss =  0.273653774411 \n",
      "\t\tacc =  58.6031626506 \n",
      "\t\tmcc =  nan\n",
      "train ( 84 ):    loss =  0.272615867516 \n",
      "\t\tacc =  58.4914434524 \n",
      "\t\tmcc =  nan\n",
      "train ( 85 ):    loss =  0.271496702687 \n",
      "\t\tacc =  58.3823529412 \n",
      "\t\tmcc =  nan\n",
      "train ( 86 ):    loss =  0.270337294336 \n",
      "\t\tacc =  58.3484738372 \n",
      "\t\tmcc =  nan\n",
      "train ( 87 ):    loss =  0.269078497615 \n",
      "\t\tacc =  58.2614942529 \n",
      "\t\tmcc =  nan\n",
      "train ( 88 ):    loss =  0.268032380057 \n",
      "\t\tacc =  58.1676136364 \n",
      "\t\tmcc =  nan\n",
      "train ( 89 ):    loss =  0.267358107439 \n",
      "\t\tacc =  58.1197331461 \n",
      "\t\tmcc =  nan\n",
      "train ( 90 ):    loss =  0.266710867229 \n",
      "\t\tacc =  57.9600694444 \n",
      "\t\tmcc =  nan\n",
      "train ( 91 ):    loss =  0.265894825906 \n",
      "\t\tacc =  57.8983516484 \n",
      "\t\tmcc =  nan\n",
      "train ( 92 ):    loss =  0.264882877292 \n",
      "\t\tacc =  57.8040081522 \n",
      "\t\tmcc =  nan\n",
      "train ( 93 ):    loss =  0.263875215733 \n",
      "\t\tacc =  57.6780913978 \n",
      "\t\tmcc =  nan\n",
      "train ( 94 ):    loss =  0.263268590536 \n",
      "\t\tacc =  57.6462765957 \n",
      "\t\tmcc =  nan\n",
      "train ( 95 ):    loss =  0.262426232676 \n",
      "\t\tacc =  57.5904605263 \n",
      "\t\tmcc =  nan\n",
      "train ( 96 ):    loss =  0.261701591225 \n",
      "\t\tacc =  57.4869791667 \n",
      "\t\tmcc =  nan\n",
      "train ( 97 ):    loss =  0.260903573937 \n",
      "\t\tacc =  57.4420103093 \n",
      "\t\tmcc =  nan\n",
      "train ( 98 ):    loss =  0.260263631727 \n",
      "\t\tacc =  57.2544642857 \n",
      "\t\tmcc =  nan\n",
      "train ( 99 ):    loss =  0.259519329879 \n",
      "\t\tacc =  57.1101641414 \n",
      "\t\tmcc =  nan\n",
      "train ( 100 ):    loss =  0.258928767555 \n",
      "\t\tacc =  57.09375 \n",
      "\t\tmcc =  nan\n",
      "train ( 101 ):    loss =  0.258109743442 \n",
      "\t\tacc =  57.03125 \n",
      "\t\tmcc =  nan\n",
      "train ( 102 ):    loss =  0.257115623155 \n",
      "\t\tacc =  56.9240196078 \n",
      "\t\tmcc =  nan\n",
      "train ( 103 ):    loss =  0.256491335707 \n",
      "\t\tacc =  56.8264563107 \n",
      "\t\tmcc =  nan\n",
      "train ( 104 ):    loss =  0.255516094822 \n",
      "\t\tacc =  56.7007211538 \n",
      "\t\tmcc =  nan\n",
      "train ( 105 ):    loss =  0.254689930548 \n",
      "\t\tacc =  56.6071428571 \n",
      "\t\tmcc =  nan\n",
      "train ( 106 ):    loss =  0.253787587841 \n",
      "\t\tacc =  56.5300707547 \n",
      "\t\tmcc =  nan\n",
      "train ( 107 ):    loss =  0.253125441138 \n",
      "\t\tacc =  56.4763434579 \n",
      "\t\tmcc =  nan\n",
      "train ( 108 ):    loss =  0.252487377555 \n",
      "\t\tacc =  56.3946759259 \n",
      "\t\tmcc =  nan\n",
      "train ( 109 ):    loss =  0.251751166632 \n",
      "\t\tacc =  56.3073394495 \n",
      "\t\tmcc =  nan\n",
      "train ( 110 ):    loss =  0.251012973659 \n",
      "\t\tacc =  56.2357954545 \n",
      "\t\tmcc =  nan\n",
      "train ( 111 ):    loss =  0.250345447307 \n",
      "\t\tacc =  56.1725788288 \n",
      "\t\tmcc =  nan\n",
      "train ( 112 ):    loss =  0.249563130677 \n",
      "\t\tacc =  56.0686383929 \n",
      "\t\tmcc =  nan\n",
      "train ( 113 ):    loss =  0.248872593071 \n",
      "\t\tacc =  55.9872787611 \n",
      "\t\tmcc =  nan\n",
      "train ( 114 ):    loss =  0.248303959573 \n",
      "\t\tacc =  55.8799342105 \n",
      "\t\tmcc =  nan\n",
      "train ( 115 ):    loss =  0.247721446664 \n",
      "\t\tacc =  55.8355978261 \n",
      "\t\tmcc =  nan\n",
      "train ( 116 ):    loss =  0.247188624923 \n",
      "\t\tacc =  55.825700431 \n",
      "\t\tmcc =  nan\n",
      "train ( 117 ):    loss =  0.246452940355 \n",
      "\t\tacc =  55.7291666667 \n",
      "\t\tmcc =  nan\n",
      "train ( 118 ):    loss =  0.245706396704 \n",
      "\t\tacc =  55.6408898305 \n",
      "\t\tmcc =  nan\n",
      "train ( 119 ):    loss =  0.245257631455 \n",
      "\t\tacc =  55.6328781513 \n",
      "\t\tmcc =  nan\n",
      "train ( 120 ):    loss =  0.24468364927 \n",
      "\t\tacc =  55.5208333333 \n",
      "\t\tmcc =  nan\n",
      "train ( 121 ):    loss =  0.244019581558 \n",
      "\t\tacc =  55.5010330579 \n",
      "\t\tmcc =  nan\n",
      "train ( 122 ):    loss =  0.243439446241 \n",
      "\t\tacc =  55.4495389344 \n",
      "\t\tmcc =  nan\n",
      "train ( 123 ):    loss =  0.243001409311 \n",
      "\t\tacc =  55.456046748 \n",
      "\t\tmcc =  nan\n",
      "train ( 124 ):    loss =  0.242421038904 \n",
      "\t\tacc =  55.3742439516 \n",
      "\t\tmcc =  nan\n",
      "train ( 125 ):    loss =  0.241790902677 \n",
      "\t\tacc =  55.35 \n",
      "\t\tmcc =  nan\n",
      "train ( 126 ):    loss =  0.241193626612 \n",
      "\t\tacc =  55.2517361111 \n",
      "\t\tmcc =  nan\n",
      "train ( 127 ):    loss =  0.240670255416 \n",
      "\t\tacc =  55.1673228346 \n",
      "\t\tmcc =  nan\n",
      "train ( 128 ):    loss =  0.239964786831 \n",
      "\t\tacc =  55.1025390625 \n",
      "\t\tmcc =  nan\n",
      "train ( 129 ):    loss =  0.239329143342 \n",
      "\t\tacc =  55.0932655039 \n",
      "\t\tmcc =  nan\n",
      "train ( 130 ):    loss =  0.238608241405 \n",
      "\t\tacc =  55.0240384615 \n",
      "\t\tmcc =  nan\n",
      "train ( 131 ):    loss =  0.237933749887 \n",
      "\t\tacc =  54.9677958015 \n",
      "\t\tmcc =  nan\n",
      "train ( 132 ):    loss =  0.237244953861 \n",
      "\t\tacc =  54.9597537879 \n",
      "\t\tmcc =  nan\n",
      "train ( 133 ):    loss =  0.236590840035 \n",
      "\t\tacc =  54.9165883459 \n",
      "\t\tmcc =  nan\n",
      "train ( 134 ):    loss =  0.236110713982 \n",
      "\t\tacc =  54.9207089552 \n",
      "\t\tmcc =  nan\n",
      "train ( 135 ):    loss =  0.235462228957 \n",
      "\t\tacc =  54.8900462963 \n",
      "\t\tmcc =  nan\n",
      "train ( 136 ):    loss =  0.234812171255 \n",
      "\t\tacc =  54.8081341912 \n",
      "\t\tmcc =  nan\n",
      "train ( 137 ):    loss =  0.234394950892 \n",
      "\t\tacc =  54.7445255474 \n",
      "\t\tmcc =  nan\n",
      "train ( 138 ):    loss =  0.233882750524 \n",
      "\t\tacc =  54.7327898551 \n",
      "\t\tmcc =  nan\n",
      "train ( 139 ):    loss =  0.233279035194 \n",
      "\t\tacc =  54.6931205036 \n",
      "\t\tmcc =  nan\n",
      "train ( 140 ):    loss =  0.232822263973 \n",
      "\t\tacc =  54.6316964286 \n",
      "\t\tmcc =  nan\n",
      "train ( 141 ):    loss =  0.232371412802 \n",
      "\t\tacc =  54.6320921986 \n",
      "\t\tmcc =  nan\n",
      "train ( 142 ):    loss =  0.23174648322 \n",
      "\t\tacc =  54.6159771127 \n",
      "\t\tmcc =  nan\n",
      "train ( 143 ):    loss =  0.231242833293 \n",
      "\t\tacc =  54.611013986 \n",
      "\t\tmcc =  nan\n",
      "train ( 144 ):    loss =  0.230787054997 \n",
      "\t\tacc =  54.5844184028 \n",
      "\t\tmcc =  nan\n",
      "train ( 145 ):    loss =  0.23049904902 \n",
      "\t\tacc =  54.4773706897 \n",
      "\t\tmcc =  nan\n",
      "train ( 146 ):    loss =  0.230149139357 \n",
      "\t\tacc =  54.4788099315 \n",
      "\t\tmcc =  nan\n",
      "train ( 147 ):    loss =  0.229605812973 \n",
      "\t\tacc =  54.4802295918 \n",
      "\t\tmcc =  nan\n",
      "train ( 148 ):    loss =  0.229221536902 \n",
      "\t\tacc =  54.418285473 \n",
      "\t\tmcc =  nan\n",
      "train ( 149 ):    loss =  0.228956323562 \n",
      "\t\tacc =  54.4200922819 \n",
      "\t\tmcc =  nan\n",
      "train ( 150 ):    loss =  0.228335738369 \n",
      "\t\tacc =  54.390625 \n",
      "\t\tmcc =  nan\n",
      "train ( 151 ):    loss =  0.22799886078 \n",
      "\t\tacc =  54.3460264901 \n",
      "\t\tmcc =  nan\n",
      "train ( 152 ):    loss =  0.227606969033 \n",
      "\t\tacc =  54.3328536184 \n",
      "\t\tmcc =  nan\n",
      "train ( 153 ):    loss =  0.227099808452 \n",
      "\t\tacc =  54.2994281046 \n",
      "\t\tmcc =  nan\n",
      "train ( 154 ):    loss =  0.226551456423 \n",
      "\t\tacc =  54.2664366883 \n",
      "\t\tmcc =  nan\n",
      "train ( 155 ):    loss =  0.226085153448 \n",
      "\t\tacc =  54.2540322581 \n",
      "\t\tmcc =  nan\n",
      "train ( 156 ):    loss =  0.225563676387 \n",
      "\t\tacc =  54.1866987179 \n",
      "\t\tmcc =  nan\n",
      "train ( 157 ):    loss =  0.225240749336 \n",
      "\t\tacc =  54.1451035032 \n",
      "\t\tmcc =  nan\n",
      "train ( 158 ):    loss =  0.224857130985 \n",
      "\t\tacc =  54.1139240506 \n",
      "\t\tmcc =  nan\n",
      "train ( 159 ):    loss =  0.224496331184 \n",
      "\t\tacc =  54.1077044025 \n",
      "\t\tmcc =  nan\n",
      "train ( 160 ):    loss =  0.223972693372 \n",
      "\t\tacc =  54.072265625 \n",
      "\t\tmcc =  nan\n",
      "train ( 161 ):    loss =  0.223558628612 \n",
      "\t\tacc =  54.080939441 \n",
      "\t\tmcc =  nan\n",
      "train ( 162 ):    loss =  0.223079158419 \n",
      "\t\tacc =  54.0702160494 \n",
      "\t\tmcc =  nan\n",
      "train ( 163 ):    loss =  0.222576448892 \n",
      "\t\tacc =  54.0308665644 \n",
      "\t\tmcc =  nan\n",
      "train ( 164 ):    loss =  0.222048557046 \n",
      "\t\tacc =  54.0158155488 \n",
      "\t\tmcc =  nan\n",
      "train ( 165 ):    loss =  0.221523695882 \n",
      "\t\tacc =  54.0056818182 \n",
      "\t\tmcc =  nan\n",
      "train ( 166 ):    loss =  0.22113977967 \n",
      "\t\tacc =  53.953313253 \n",
      "\t\tmcc =  nan\n",
      "train ( 167 ):    loss =  0.220707830284 \n",
      "\t\tacc =  53.9296407186 \n",
      "\t\tmcc =  nan\n",
      "train ( 168 ):    loss =  0.220336263342 \n",
      "\t\tacc =  53.9295014881 \n",
      "\t\tmcc =  nan\n",
      "train ( 169 ):    loss =  0.219896192065 \n",
      "\t\tacc =  53.8970044379 \n",
      "\t\tmcc =  nan\n",
      "train ( 170 ):    loss =  0.219417960789 \n",
      "\t\tacc =  53.8419117647 \n",
      "\t\tmcc =  nan\n",
      "train ( 171 ):    loss =  0.218957644558 \n",
      "\t\tacc =  53.8468567251 \n",
      "\t\tmcc =  nan\n",
      "train ( 172 ):    loss =  0.218571980231 \n",
      "\t\tacc =  53.851744186 \n",
      "\t\tmcc =  nan\n",
      "train ( 173 ):    loss =  0.218149258057 \n",
      "\t\tacc =  53.8159320809 \n",
      "\t\tmcc =  nan\n",
      "train ( 174 ):    loss =  0.217654097972 \n",
      "\t\tacc =  53.7940014368 \n",
      "\t\tmcc =  nan\n",
      "train ( 175 ):    loss =  0.217109361748 \n",
      "\t\tacc =  53.7767857143 \n",
      "\t\tmcc =  nan\n",
      "train ( 176 ):    loss =  0.216651468862 \n",
      "\t\tacc =  53.7331321023 \n",
      "\t\tmcc =  nan\n",
      "train ( 177 ):    loss =  0.216257361844 \n",
      "\t\tacc =  53.7252824859 \n",
      "\t\tmcc =  nan\n",
      "train ( 178 ):    loss =  0.215785440346 \n",
      "\t\tacc =  53.7262991573 \n",
      "\t\tmcc =  nan\n",
      "train ( 179 ):    loss =  0.215396383266 \n",
      "\t\tacc =  53.7403980447 \n",
      "\t\tmcc =  nan\n",
      "train ( 180 ):    loss =  0.215018132941 \n",
      "\t\tacc =  53.7326388889 \n",
      "\t\tmcc =  nan\n",
      "train ( 181 ):    loss =  0.214736666307 \n",
      "\t\tacc =  53.7077002762 \n",
      "\t\tmcc =  nan\n",
      "train ( 182 ):    loss =  0.214503605886 \n",
      "\t\tacc =  53.6873282967 \n",
      "\t\tmcc =  nan\n",
      "train ( 183 ):    loss =  0.214119719872 \n",
      "\t\tacc =  53.6842554645 \n",
      "\t\tmcc =  nan\n",
      "train ( 184 ):    loss =  0.213707327912 \n",
      "\t\tacc =  53.659986413 \n",
      "\t\tmcc =  nan\n",
      "train ( 185 ):    loss =  0.213339804843 \n",
      "\t\tacc =  53.6486486486 \n",
      "\t\tmcc =  nan\n",
      "train ( 186 ):    loss =  0.212982212862 \n",
      "\t\tacc =  53.6458333333 \n",
      "\t\tmcc =  nan\n",
      "train ( 187 ):    loss =  0.212670321616 \n",
      "\t\tacc =  53.605447861 \n",
      "\t\tmcc =  nan\n",
      "train ( 188 ):    loss =  0.212356533064 \n",
      "\t\tacc =  53.6444481383 \n",
      "\t\tmcc =  nan\n",
      "train ( 189 ):    loss =  0.211917965507 \n",
      "\t\tacc =  53.6375661376 \n",
      "\t\tmcc =  nan\n",
      "train ( 190 ):    loss =  0.211590495468 \n",
      "\t\tacc =  53.6636513158 \n",
      "\t\tmcc =  nan\n",
      "train ( 191 ):    loss =  0.211389203184 \n",
      "\t\tacc =  53.6731020942 \n",
      "\t\tmcc =  nan\n",
      "train ( 192 ):    loss =  0.211141857804 \n",
      "\t\tacc =  53.6336263021 \n",
      "\t\tmcc =  nan\n",
      "train ( 193 ):    loss =  0.21074906798 \n",
      "\t\tacc =  53.6147992228 \n",
      "\t\tmcc =  nan\n",
      "train ( 194 ):    loss =  0.210343198519 \n",
      "\t\tacc =  53.6122744845 \n",
      "\t\tmcc =  nan\n",
      "train ( 195 ):    loss =  0.210107556018 \n",
      "\t\tacc =  53.5697115385 \n",
      "\t\tmcc =  nan\n",
      "train ( 196 ):    loss =  0.209761082666 \n",
      "\t\tacc =  53.5594706633 \n",
      "\t\tmcc =  nan\n",
      "train ( 197 ):    loss =  0.209304263882 \n",
      "\t\tacc =  53.5453680203 \n",
      "\t\tmcc =  nan\n",
      "train ( 198 ):    loss =  0.209022608937 \n",
      "\t\tacc =  53.5235164141 \n",
      "\t\tmcc =  nan\n",
      "train ( 199 ):    loss =  0.208663831453 \n",
      "\t\tacc =  53.5215138191 \n",
      "\t\tmcc =  nan\n",
      "train ( 200 ):    loss =  0.208449678771 \n",
      "\t\tacc =  53.4609375 \n",
      "\t\tmcc =  nan\n",
      "train ( 201 ):    loss =  0.208142885902 \n",
      "\t\tacc =  53.4514925373 \n",
      "\t\tmcc =  nan\n",
      "train ( 202 ):    loss =  0.207847071798 \n",
      "\t\tacc =  53.4073329208 \n",
      "\t\tmcc =  nan\n",
      "train ( 203 ):    loss =  0.207522440203 \n",
      "\t\tacc =  53.3905480296 \n",
      "\t\tmcc =  nan\n",
      "train ( 204 ):    loss =  0.207160308544 \n",
      "\t\tacc =  53.4122242647 \n",
      "\t\tmcc =  nan\n",
      "train ( 205 ):    loss =  0.20688208785 \n",
      "\t\tacc =  53.3384146341 \n",
      "\t\tmcc =  nan\n",
      "train ( 206 ):    loss =  0.206590664611 \n",
      "\t\tacc =  53.3260012136 \n",
      "\t\tmcc =  nan\n",
      "train ( 207 ):    loss =  0.206209096807 \n",
      "\t\tacc =  53.3099335749 \n",
      "\t\tmcc =  nan\n",
      "train ( 208 ):    loss =  0.20605312549 \n",
      "\t\tacc =  53.3466045673 \n",
      "\t\tmcc =  nan\n",
      "train ( 209 ):    loss =  0.205736953406 \n",
      "\t\tacc =  53.3231160287 \n",
      "\t\tmcc =  nan\n",
      "train ( 210 ):    loss =  0.205352041232 \n",
      "\t\tacc =  53.318452381 \n",
      "\t\tmcc =  nan\n",
      "train ( 211 ):    loss =  0.205005618483 \n",
      "\t\tacc =  53.2953199052 \n",
      "\t\tmcc =  nan\n",
      "train ( 212 ):    loss =  0.204756516109 \n",
      "\t\tacc =  53.2760908019 \n",
      "\t\tmcc =  nan\n",
      "train ( 213 ):    loss =  0.204385629626 \n",
      "\t\tacc =  53.2753814554 \n",
      "\t\tmcc =  nan\n",
      "train ( 214 ):    loss =  0.204077109482 \n",
      "\t\tacc =  53.2564252336 \n",
      "\t\tmcc =  nan\n",
      "train ( 215 ):    loss =  0.203750594412 \n",
      "\t\tacc =  53.2412790698 \n",
      "\t\tmcc =  nan\n",
      "train ( 216 ):    loss =  0.20342518734 \n",
      "\t\tacc =  53.2154224537 \n",
      "\t\tmcc =  nan\n",
      "train ( 217 ):    loss =  0.203126930161 \n",
      "\t\tacc =  53.1790034562 \n",
      "\t\tmcc =  nan\n",
      "train ( 218 ):    loss =  0.202797414586 \n",
      "\t\tacc =  53.2002580275 \n",
      "\t\tmcc =  nan\n",
      "train ( 219 ):    loss =  0.202429902323 \n",
      "\t\tacc =  53.196347032 \n",
      "\t\tmcc =  nan\n",
      "train ( 220 ):    loss =  0.20228385851 \n",
      "\t\tacc =  53.1995738636 \n",
      "\t\tmcc =  nan\n",
      "train ( 221 ):    loss =  0.201969039271 \n",
      "\t\tacc =  53.2063065611 \n",
      "\t\tmcc =  nan\n",
      "train ( 222 ):    loss =  0.201667491061 \n",
      "\t\tacc =  53.1707488739 \n",
      "\t\tmcc =  nan\n",
      "train ( 223 ):    loss =  0.201398865387 \n",
      "\t\tacc =  53.1144899103 \n",
      "\t\tmcc =  nan\n",
      "train ( 224 ):    loss =  0.201071726149 \n",
      "\t\tacc =  53.1145368304 \n",
      "\t\tmcc =  nan\n",
      "train ( 225 ):    loss =  0.200910717647 \n",
      "\t\tacc =  53.0798611111 \n",
      "\t\tmcc =  nan\n",
      "train ( 226 ):    loss =  0.200631699023 \n",
      "\t\tacc =  53.0662334071 \n",
      "\t\tmcc =  nan\n",
      "train ( 227 ):    loss =  0.200501776265 \n",
      "\t\tacc =  53.1009085903 \n",
      "\t\tmcc =  nan\n",
      "train ( 228 ):    loss =  0.200196725985 \n",
      "\t\tacc =  53.0941611842 \n",
      "\t\tmcc =  nan\n",
      "train ( 229 ):    loss =  0.199852080984 \n",
      "\t\tacc =  53.0840611354 \n",
      "\t\tmcc =  nan\n",
      "train ( 230 ):    loss =  0.199645364999 \n",
      "\t\tacc =  53.1046195652 \n",
      "\t\tmcc =  nan\n",
      "train ( 231 ):    loss =  0.19927059842 \n",
      "\t\tacc =  53.0979437229 \n",
      "\t\tmcc =  nan\n",
      "train ( 232 ):    loss =  0.199007172913 \n",
      "\t\tacc =  53.1148976293 \n",
      "\t\tmcc =  nan\n",
      "train ( 233 ):    loss =  0.198684892037 \n",
      "\t\tacc =  53.125 \n",
      "\t\tmcc =  nan\n",
      "train ( 234 ):    loss =  0.198529512508 \n",
      "\t\tacc =  53.1149839744 \n",
      "\t\tmcc =  nan\n",
      "train ( 235 ):    loss =  0.198217352862 \n",
      "\t\tacc =  53.0817819149 \n",
      "\t\tmcc =  nan\n",
      "train ( 236 ):    loss =  0.198002853822 \n",
      "\t\tacc =  53.1051377119 \n",
      "\t\tmcc =  nan\n",
      "train ( 237 ):    loss =  0.197746573959 \n",
      "\t\tacc =  53.098628692 \n",
      "\t\tmcc =  nan\n",
      "train ( 238 ):    loss =  0.197483226136 \n",
      "\t\tacc =  53.0757615546 \n",
      "\t\tmcc =  nan\n",
      "train ( 239 ):    loss =  0.197275058127 \n",
      "\t\tacc =  53.0661610879 \n",
      "\t\tmcc =  nan\n",
      "train ( 240 ):    loss =  0.197050129258 \n",
      "\t\tacc =  53.0403645833 \n",
      "\t\tmcc =  nan\n",
      "train ( 241 ):    loss =  0.196753010922 \n",
      "\t\tacc =  53.0115404564 \n",
      "\t\tmcc =  nan\n",
      "train ( 242 ):    loss =  0.19644606968 \n",
      "\t\tacc =  52.9990960744 \n",
      "\t\tmcc =  nan\n",
      "train ( 243 ):    loss =  0.196122182028 \n",
      "\t\tacc =  53.0092592593 \n",
      "\t\tmcc =  nan\n",
      "train ( 244 ):    loss =  0.19578060187 \n",
      "\t\tacc =  52.9969262295 \n",
      "\t\tmcc =  nan\n",
      "train ( 245 ):    loss =  0.195566466725 \n",
      "\t\tacc =  52.9974489796 \n",
      "\t\tmcc =  nan\n",
      "train ( 246 ):    loss =  0.195258403728 \n",
      "\t\tacc =  53.0011432927 \n",
      "\t\tmcc =  nan\n",
      "train ( 247 ):    loss =  0.194975615048 \n",
      "\t\tacc =  52.982667004 \n",
      "\t\tmcc =  nan\n",
      "train ( 248 ):    loss =  0.194646995358 \n",
      "\t\tacc =  52.9769405242 \n",
      "\t\tmcc =  nan\n",
      "train ( 249 ):    loss =  0.194371497838 \n",
      "\t\tacc =  52.952434739 \n",
      "\t\tmcc =  nan\n",
      "Epoch 1 of 10 took 6004.883s\n",
      "  training loss:\t\t0.194371\n",
      "  training accuracy:\t\t52.95 %\n",
      "  training mcc:\t\tnan\n",
      "  validation loss:\t\t1.290289\n",
      "  validation accuracy:\t\t50.00 %\n",
      "  validation mcc:\t\tnan\n",
      "train ( 1 ):    loss =  0.11936328849 \n",
      "\t\tacc =  45.3125 \n",
      "\t\tmcc =  nan\n",
      "train ( 2 ):    loss =  0.116053958244 \n",
      "\t\tacc =  48.046875 \n",
      "\t\tmcc =  nan\n",
      "train ( 3 ):    loss =  0.116490297041 \n",
      "\t\tacc =  49.21875 \n",
      "\t\tmcc =  nan\n",
      "train ( 4 ):    loss =  0.12104867852 \n",
      "\t\tacc =  50.0 \n",
      "\t\tmcc =  nan\n",
      "train ( 5 ):    loss =  0.127549827287 \n",
      "\t\tacc =  48.125 \n",
      "\t\tmcc =  nan\n",
      "train ( 6 ):    loss =  0.127676305052 \n",
      "\t\tacc =  47.7864583333 \n",
      "\t\tmcc =  nan\n",
      "train ( 7 ):    loss =  0.126595089766 \n",
      "\t\tacc =  48.6607142857 \n",
      "\t\tmcc =  nan\n",
      "train ( 8 ):    loss =  0.129280890766 \n",
      "\t\tacc =  48.828125 \n",
      "\t\tmcc =  nan\n",
      "train ( 9 ):    loss =  0.128666584644 \n",
      "\t\tacc =  49.4791666667 \n",
      "\t\tmcc =  nan\n",
      "train ( 10 ):    loss =  0.1275761576 \n",
      "\t\tacc =  49.296875 \n",
      "\t\tmcc =  nan\n",
      "train ( 11 ):    loss =  0.131653134294 \n",
      "\t\tacc =  49.21875 \n",
      "\t\tmcc =  nan\n",
      "train ( 12 ):    loss =  0.130448906252 \n",
      "\t\tacc =  49.4140625 \n",
      "\t\tmcc =  nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-067a2d2238f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhditer_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mtrain_err\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfnn_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mtrain_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/cori/software/python/2.7-anaconda/envs/deeplearning/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if train_model:\n",
    "    \n",
    "    #validation error\n",
    "    best_val_err=1.e6\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0.\n",
    "        train_acc = 0.\n",
    "        train_mcc = 0.\n",
    "        train_batches = 0.\n",
    "        start_time = time.time()\n",
    "        for batch in hditer_train.next_batch(batchsize):\n",
    "            inputs, targets = batch\n",
    "            train_err += fnn_train(inputs, targets)\n",
    "            train_batches += 1.\n",
    "        \n",
    "            #print accurarcy on training sample:\n",
    "            _, acc, mcc = fnn_validate(inputs, targets)\n",
    "            train_acc += acc\n",
    "            train_mcc += mcc\n",
    "        \n",
    "            #debugging output\n",
    "            print 'train (',int(train_batches),'):    loss = ', train_err/train_batches,'\\n',\\\n",
    "                                                '\\t\\tacc = ', train_acc/train_batches*100.,'\\n', \\\n",
    "                                                '\\t\\tmcc = ',train_mcc/train_batches\n",
    "        \n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0.\n",
    "        val_acc = 0.\n",
    "        val_mcc = 0.\n",
    "        val_batches = 0.\n",
    "        for batch in hditer_validation.next_batch(batchsize):\n",
    "            inputs, targets = batch            \n",
    "            err, acc, mcc = fnn_validate(inputs, targets)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_mcc += mcc\n",
    "            val_batches += 1.\n",
    "        \n",
    "        #save model if the validation error decreased\n",
    "        if best_val_err>val_err:\n",
    "            #write updates\n",
    "            values1 = [p.get_value() for p in updates.keys()]\n",
    "            with open(updatesfile_savebest,'wb+') as f:\n",
    "                pickle.dump(values1,f)\n",
    "                f.close()\n",
    "        \n",
    "            #write parameters\n",
    "            values2 = ls.layers.get_all_param_values(network,trainable=True)\n",
    "            with open(paramsfile_savebest,'wb+') as f:\n",
    "                pickle.dump(values2,f)\n",
    "                f.close()\n",
    "            #update last error:\n",
    "            best_val_err=val_err\n",
    "        \n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  training accuracy:\\t\\t{:.2f} %\".format(train_acc / train_batches * 100.))\n",
    "        print(\"  training mcc:\\t\\t{:.2f}\".format(train_mcc / train_batches ))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(val_acc / val_batches * 100.))\n",
    "        print(\"  validation mcc:\\t\\t{:.2f}\".format(val_mcc / val_batches ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_model=True\n",
    "paramsfile_save=\"model_parameters.pick\"\n",
    "updatesfile_save=\"model_updates.pick\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if save_model:\n",
    "    #write updates\n",
    "    values1 = [p.get_value() for p in updates.keys()]\n",
    "    with open(updatesfile_save,'wb+') as f:\n",
    "        pickle.dump(values1,f)\n",
    "        f.close()\n",
    "        \n",
    "    #write parameters\n",
    "    values2 = ls.layers.get_all_param_values(network,trainable=True)\n",
    "    with open(paramsfile_save,'wb+') as f:\n",
    "        pickle.dump(values2,f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "#run on test data and compute ROC:\n",
    "test_err = 0.\n",
    "test_acc = 0.\n",
    "test_batches = 0\n",
    "batchsize_test=100\n",
    "\n",
    "targets_pred = np.zeros((hditer_test.num_examples,))\n",
    "targets_gt = np.zeros((hditer_test.num_examples,))\n",
    "\n",
    "for batch in hditer_test.next_batch(batchsize_test):\n",
    "    inputs, targets = batch\n",
    "    err, acc, _ = fnn_validate(inputs,targets)\n",
    "    test_err+=err\n",
    "    test_acc+=acc\n",
    "    test_batches+=1\n",
    "    \n",
    "    targets_pred[(test_batches-1)*batchsize_test:test_batches*batchsize_test] = fnn_det(inputs)[:,1]\n",
    "    targets_gt[(test_batches-1)*batchsize_test:test_batches*batchsize_test] = targets[:]\n",
    "\n",
    "#accuracies\n",
    "print(\"  test loss:\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(test_acc / test_batches * 100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ROC curve\n",
    "#ROC\n",
    "fpr, tpr, thresholds = metrics.roc_curve(targets_gt, targets_pred, pos_label=1)\n",
    "plt.figure()\n",
    "lw = 2\n",
    "#full curve\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % metrics.auc(fpr,tpr))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ROC_1400_850.png',dpi=300)\n",
    "\n",
    "#zoomed-in\n",
    "#plt.plot(fpr, tpr, color='darkorange',\n",
    "#         lw=lw, label='ROC curve (area = %0.2f)' % metrics.auc(fpr,tpr))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 0.01])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"center right\")\n",
    "plt.savefig('ROC_1400_850_zoom.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_example(x):\n",
    "    plt.imshow(np.log10(x).T,extent=[-3.15, 3.15, -5, 5], interpolation='none',aspect='auto', origin='low')\n",
    "    plt.colorbar()\n",
    "\n",
    "#for batch in hditer_validation:\n",
    "#    inputs,targets=batch\n",
    "#    plot_example(inputs[0,0,:,:])\n",
    "#    break\n",
    "\n",
    "for batch in hditer_train:\n",
    "    inputs,targets=batch\n",
    "    plot_example(inputs[0,0,:,:])\n",
    "    break;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [deeplearning]",
   "language": "python",
   "name": "Python [deeplearning]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
