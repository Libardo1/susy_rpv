{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__author__ = 'tkurth'\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from nbfinder import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "%matplotlib inline\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theano modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne as ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROOT stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append('/global/homes/w/wbhimji/cori-envs/nersc-rootpy/lib/python2.7/site-packages/')\n",
    "import ROOT\n",
    "import rootpy\n",
    "import root_numpy as rnp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a context manager to suppress stdout and stderr.\n",
    "class suppress_stdout_stderr(object):\n",
    "    '''\n",
    "    A context manager for doing a \"deep suppression\" of stdout and stderr in \n",
    "    Python, i.e. will suppress all print, even if the print originates in a \n",
    "    compiled C/Fortran sub-function.\n",
    "       This will not suppress raised exceptions, since exceptions are printed\n",
    "    to stderr just before a script exits, and after the context manager has\n",
    "    exited (at least, I think that is why it lets exceptions through).      \n",
    "\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # Open a pair of null files\n",
    "        self.null_fds =  [os.open(os.devnull,os.O_RDWR) for x in range(2)]\n",
    "        # Save the actual stdout (1) and stderr (2) file descriptors.\n",
    "        self.save_fds = (os.dup(1), os.dup(2))\n",
    "\n",
    "    def __enter__(self):\n",
    "        # Assign the null pointers to stdout and stderr.\n",
    "        os.dup2(self.null_fds[0],1)\n",
    "        os.dup2(self.null_fds[1],2)\n",
    "\n",
    "    def __exit__(self, *_):\n",
    "        # Re-assign the real stdout/stderr back to (1) and (2)\n",
    "        os.dup2(self.save_fds[0],1)\n",
    "        os.dup2(self.save_fds[1],2)\n",
    "        # Close the null files\n",
    "        os.close(self.null_fds[0])\n",
    "        os.close(self.null_fds[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class hep_data_iterator:\n",
    "    \n",
    "    #class constructor\n",
    "    def __init__(self,\n",
    "                 bg_cfg_file = '../config/BgFileListAug16.txt',\n",
    "                 sig_cfg_file='../config/SignalFileListAug16.txt',\n",
    "                 group_name='CollectionTree',\n",
    "                 branches=['CaloCalTopoClustersAuxDyn.calPhi', 'CaloCalTopoClustersAuxDyn.calEta','CaloCalTopoClustersAuxDyn.calE'],\n",
    "                 num_events_total=8192,\n",
    "                 num_events_cached=512,\n",
    "                 num_classes=2,\n",
    "                 shuffle=True,\n",
    "                 preprocess=True,\n",
    "                 bin_size=0.025,\n",
    "                 eta_range = [-5,5],\n",
    "                 phi_range = [-3.14, 3.14],\n",
    "                 dataset_name='histo'):\n",
    "        \n",
    "        #general stuff\n",
    "        self.group_name=group_name\n",
    "        self.branches=branches\n",
    "        \n",
    "        #set some local variables:\n",
    "        self.bg_files = [line.rstrip() for line in open(bg_cfg_file)]\n",
    "        self.sig_files = [line.rstrip() for line in open(sig_cfg_file)]\n",
    "\n",
    "        #that is somehow hardcoded\n",
    "        self.events_per_sig_file = 10000\n",
    "    \n",
    "        #number of total events:\n",
    "        self.num_events_total=num_events_total\n",
    "        self.num_events_cached=num_events_cached\n",
    "        \n",
    "        #number of classes\n",
    "        self.num_classes=num_classes\n",
    "        \n",
    "        #if no total number of events is specified, take everything\n",
    "        if not self.num_events_total:\n",
    "            self.num_events_total = self.events_per_sig_file * np.min([len(self.sig_files),len(self.bg_files)])\n",
    "        \n",
    "        #we assume there are more bg per file than sig, so we bound our number of files by number of files\n",
    "        #needed for a sig event\n",
    "        if self.num_events_total % self.num_classes != 0:\n",
    "            #adjust to class frequencies\n",
    "            self.num_events_total -= self.num_events_total % self.num_classes\n",
    "        \n",
    "        #how many events to we need:\n",
    "        self.num_each = self.num_events_total / self.num_classes\n",
    "\n",
    "        #get the number of files needed in total and for caching\n",
    "        self.num_files = int(np.ceil(self.num_each / float(self.events_per_sig_file)))\n",
    "\n",
    "        #hack because rootpy does not do well with one file\n",
    "        if self.num_files == 1:\n",
    "            self.num_files = 2\n",
    "        \n",
    "        #shuffle array in cases where not all files are used:\n",
    "        self.shuffle=shuffle\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.bg_files)\n",
    "            np.random.shuffle(self.sig_files)\n",
    "        \n",
    "        #restrict the number of elements:\n",
    "        self.bg_files=self.bg_files[:self.num_files]\n",
    "        self.sig_files=self.sig_files[:self.num_files]\n",
    "        \n",
    "        #set file-counter to zero:\n",
    "        self.filecount=0\n",
    "        \n",
    "        #some other required preprocessing\n",
    "        self.phi_range=phi_range\n",
    "        self.eta_range=eta_range\n",
    "        self.bin_size=bin_size\n",
    "        self.phi_bins = int(np.floor((self.phi_range[1] - self.phi_range[0]) / self.bin_size))\n",
    "        self.eta_bins = int(np.floor((self.eta_range[1] - self.eta_range[0]) / self.bin_size))\n",
    "        \n",
    "        #compute cache size\n",
    "        self.compute_cache()\n",
    "        \n",
    "        #compute absolute max over whole set\n",
    "        self.compute_data_max()\n",
    "        \n",
    "        #prefetch the first batch\n",
    "        self.prefetch()\n",
    "        \n",
    "        #store the shapes:\n",
    "        self.xshape=self.x[0].shape\n",
    "        self.yshape=self.y[0].shape\n",
    "    \n",
    "    \n",
    "    #compute cache pars:\n",
    "    def compute_cache(self):\n",
    "        #make sure that it is an integer multiple of the number of classes\n",
    "        if self.num_events_cached % self.num_classes != 0:\n",
    "            self.num_events_cached -= self.num_events_cached % self.num_classes\n",
    "            \n",
    "        #how many per class\n",
    "        self.num_each_cached = self.num_events_cached / self.num_classes\n",
    "        \n",
    "        #how many files:\n",
    "        self.num_files_cached = int(np.ceil(self.num_each_cached / float(self.events_per_sig_file)))\n",
    "        \n",
    "        #hack because rootpy does not do well with one file\n",
    "        if self.num_files_cached == 1:\n",
    "            self.num_files_cached = 2\n",
    "    \n",
    "    \n",
    "    #compute max over all data\n",
    "    def compute_data_max(self):\n",
    "        '''compute the maximum over all event entries for rescaling data between -1 and 1'''\n",
    "        #we have to iterate through all available data at least once to compute the max\n",
    "        #to avoid memory overflows, only iterate in units of numbers of cached events:\n",
    "        self.max_abs=0.\n",
    "        for i in np.arange(0,self.num_files,self.num_files_cached):\n",
    "            #upper boundary\n",
    "            upper=np.min([self.num_files,i+self.num_files_cached])\n",
    "            \n",
    "            #so we don't have annoying stderr messages\n",
    "            with suppress_stdout_stderr():\n",
    "                #get arrays\n",
    "                bgarray = rnp.root2array(self.bg_files[i:upper], \\\n",
    "                                         treename=self.group_name, \\\n",
    "                                         branches=self.branches, \\\n",
    "                                         start=0, \\\n",
    "                                         stop=self.num_each_cached,warn_missing_tree=True)\n",
    "\n",
    "                sigarray = rnp.root2array(self.sig_files[i:upper],\\\n",
    "                                          treename=self.group_name,\\\n",
    "                                          branches=self.branches,\\\n",
    "                                          start=0, \\\n",
    "                                          stop=self.num_each_cached,warn_missing_tree=True)\n",
    "            \n",
    "            #convert to dataframe and compute max\n",
    "            df = pd.concat([pd.DataFrame.from_records(bgarray),pd.DataFrame.from_records(sigarray)])\n",
    "            tmpmax=(df['CaloCalTopoClustersAuxDyn.calE'].abs()).apply(lambda x: np.max(x)).max()\n",
    "            \n",
    "            #update max\n",
    "            self.max_abs=np.max([tmpmax,self.max_abs])\n",
    "    \n",
    "    \n",
    "    #fetch the next bunch of data and preprocess\n",
    "    def prefetch(self):\n",
    "        '''prefetch the next bunch of events'''\n",
    "        #shuffle and start from zero if the number of remaining cached files is too small:\n",
    "        if self.filecount+self.num_files_cached>self.num_files:\n",
    "            self.filecount=0\n",
    "        \n",
    "            #reshuffle data\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(self.bg_files)\n",
    "                np.random.shuffle(self.sig_files)\n",
    "        \n",
    "        #so we don't have annoying stderr messages\n",
    "        with suppress_stdout_stderr():\n",
    "            \n",
    "            #bgarray has n_events groups of 3 parallel numpy arrays \n",
    "            #(each numpy within a group is of equal length and each array corresponds to phi, eta and the corresponding energy)\n",
    "            bgarray = rnp.root2array(self.bg_files[self.filecount:self.filecount+self.num_files_cached], \\\n",
    "                                     treename=self.group_name, \\\n",
    "                                     branches=self.branches, \\\n",
    "                                     start=0, \\\n",
    "                                     stop=self.num_each_cached,warn_missing_tree=True)\n",
    "\n",
    "            sigarray = rnp.root2array(self.sig_files[self.filecount:self.filecount+self.num_files_cached],\\\n",
    "                                      treename=self.group_name,\\\n",
    "                                      branches=self.branches,\\\n",
    "                                      start=0, \\\n",
    "                                      stop=self.num_each_cached,warn_missing_tree=True)\n",
    "            \n",
    "            #update counter\n",
    "            self.filecount+=self.num_files_cached\n",
    "            \n",
    "        \n",
    "        #now preprocess\n",
    "        #store in dataframe\n",
    "        bgdf = pd.DataFrame.from_records(bgarray)\n",
    "        sigdf = pd.DataFrame.from_records(sigarray)\n",
    "\n",
    "        #create empty array\n",
    "        x_bg = np.zeros((self.num_each_cached, 1, self.phi_bins, self.eta_bins ))\n",
    "        x_sig = np.zeros((self.num_each_cached, 1, self.phi_bins, self.eta_bins ))\n",
    "        \n",
    "        #now go through all the events\n",
    "        for i in range(self.num_each_cached):\n",
    "            phi, eta, E =  bgdf['CaloCalTopoClustersAuxDyn.calPhi'][i],\\\n",
    "                           bgdf['CaloCalTopoClustersAuxDyn.calEta'][i],\\\n",
    "                           bgdf['CaloCalTopoClustersAuxDyn.calE'][i]\n",
    "\n",
    "            x_bg[i] = np.histogram2d(phi,eta, bins=(self.phi_bins, self.eta_bins), weights=E, \\\n",
    "                                     range=[self.phi_range,self.eta_range])[0]\n",
    "\n",
    "            phi, eta, E =  sigdf['CaloCalTopoClustersAuxDyn.calPhi'][i],\\\n",
    "                           sigdf['CaloCalTopoClustersAuxDyn.calEta'][i],\\\n",
    "                           sigdf['CaloCalTopoClustersAuxDyn.calE'][i]\n",
    "            x_sig[i] = np.histogram2d(phi,eta, bins=(self.phi_bins, self.eta_bins), weights=E, \\\n",
    "                                      range=[self.phi_range,self.eta_range])[0]\n",
    "\n",
    "\n",
    "        #background first\n",
    "        self.x = np.vstack((x_bg, x_sig))\n",
    "        \n",
    "        # 1 means signal, 0 means background\n",
    "        self.y = np.zeros((self.num_events_cached,self.num_classes)).astype('int32')\n",
    "        #make the last half signal label\n",
    "        for i in range(0,self.num_classes):\n",
    "            self.y[i*self.num_each_cached:(i+1)*self.num_each_cached,i] = 1.\n",
    "        \n",
    "        #shuffle the arrays\n",
    "        pivot=np.arange(0,self.num_events_cached)\n",
    "        np.random.shuffle(pivot)\n",
    "        self.x=self.x[pivot,:,:,:]\n",
    "        self.y=self.y[pivot,:]\n",
    "        \n",
    "        #apply rescaling\n",
    "        self.x /= self.max_abs\n",
    "        \n",
    "    \n",
    "    #this is the batch iterator:\n",
    "    def get_batch(self,batchsize):\n",
    "        '''batch iterator'''\n",
    "        #recompute cache if batchsize is too large\n",
    "        if batchsize>self.num_events_cached:\n",
    "            self.num_events_cached=batchsize*self.num_classes\n",
    "            compute_cache()\n",
    "        \n",
    "        #do iterator loop\n",
    "        for i in range(0,self.num_events_total,batchsize):\n",
    "            \n",
    "            idx = i % self.num_events_cached\n",
    "            \n",
    "            #do we need to prefetch?\n",
    "            if idx+batchsize>self.num_events_cached:\n",
    "                self.prefetch()\n",
    "        \n",
    "            #return the next batch\n",
    "            yield self.x[idx:idx+batchsize,:,:,:], self.y[idx:idx+batchsize,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct data iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data iterator class\n",
    "hditer=hep_data_iterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#some parameters\n",
    "keep_prob=0.5\n",
    "num_filters=128\n",
    "num_units_dense=1024\n",
    "initial_learning_rate=0.001\n",
    "\n",
    "#input layer\n",
    "l_inp_data = ls.layers.InputLayer((None,hditer.xshape[0],hditer.xshape[1],hditer.xshape[2]))\n",
    "l_inp_label = ls.layers.InputLayer((None,hditer.yshape[0]))\n",
    "\n",
    "#conv layers\n",
    "#first layer\n",
    "l_conv1 = ls.layers.Conv2DLayer(incoming=l_inp_data,\n",
    "                                num_filters=num_filters,\n",
    "                                filter_size=3,\n",
    "                                stride=(1,1),\n",
    "                                pad=0,\n",
    "                                W=ls.init.HeUniform(),\n",
    "                                b=ls.init.Constant(0.),\n",
    "                                nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                               )\n",
    "l_pool1 = ls.layers.MaxPool2DLayer(incoming=l_conv1,\n",
    "                                   pool_size=(2,2),\n",
    "                                   stride=2,\n",
    "                                   pad=0                                   \n",
    "                                  )\n",
    "\n",
    "l_drop1 = ls.layers.DropoutLayer(incoming=l_pool1,\n",
    "                       p=keep_prob,\n",
    "                       rescale=True\n",
    "                      )\n",
    "\n",
    "#second layer:\n",
    "l_conv2 = ls.layers.Conv2DLayer(incoming=l_drop1,\n",
    "                                num_filters=num_filters,\n",
    "                                filter_size=3,\n",
    "                                stride=(1,1),\n",
    "                                pad=0,\n",
    "                                W=ls.init.HeUniform(),\n",
    "                                b=ls.init.Constant(0.),\n",
    "                                nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                               )\n",
    "l_pool2 = ls.layers.MaxPool2DLayer(incoming=l_conv2,\n",
    "                                   pool_size=(2,2),\n",
    "                                   stride=2,\n",
    "                                   pad=0                                   \n",
    "                                  )\n",
    "\n",
    "l_drop2 = ls.layers.DropoutLayer(incoming=l_pool2,\n",
    "                       p=keep_prob,\n",
    "                       rescale=True\n",
    "                      )\n",
    "\n",
    "#third layer:\n",
    "l_conv3 = ls.layers.Conv2DLayer(incoming=l_drop2,\n",
    "                                num_filters=num_filters,\n",
    "                                filter_size=3,\n",
    "                                stride=(1,1),\n",
    "                                pad=0,\n",
    "                                W=ls.init.HeUniform(),\n",
    "                                b=ls.init.Constant(0.),\n",
    "                                nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                               )\n",
    "l_pool3 = ls.layers.MaxPool2DLayer(incoming=l_conv3,\n",
    "                                   pool_size=(2,2),\n",
    "                                   stride=2,\n",
    "                                   pad=0                                   \n",
    "                                  )\n",
    "\n",
    "l_drop3 = ls.layers.DropoutLayer(incoming=l_pool3,\n",
    "                       p=keep_prob,\n",
    "                       rescale=True\n",
    "                      )\n",
    "\n",
    "#fourth layer:\n",
    "l_conv4 = ls.layers.Conv2DLayer(incoming=l_drop3,\n",
    "                                num_filters=num_filters,\n",
    "                                filter_size=3,\n",
    "                                stride=(1,1),\n",
    "                                pad=0,\n",
    "                                W=ls.init.HeUniform(),\n",
    "                                b=ls.init.Constant(0.),\n",
    "                                nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                               )\n",
    "l_pool4 = ls.layers.MaxPool2DLayer(incoming=l_conv4,\n",
    "                                   pool_size=(2,2),\n",
    "                                   stride=2,\n",
    "                                   pad=0                                   \n",
    "                                  )\n",
    "\n",
    "l_drop4 = ls.layers.DropoutLayer(incoming=l_pool4,\n",
    "                       p=keep_prob,\n",
    "                       rescale=True\n",
    "                      )\n",
    "\n",
    "#flatten\n",
    "l_flat = ls.layers.FlattenLayer(incoming=l_drop4, \n",
    "                                outdim=2)\n",
    "\n",
    "#crossfire\n",
    "l_fc1 = ls.layers.DenseLayer(incoming=l_flat, \n",
    "                             num_units=num_units_dense, \n",
    "                             W=ls.init.GlorotUniform(np.sqrt(2./(1+0.01**2))), \n",
    "                             b=ls.init.Constant(0.0),\n",
    "                             nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                            )\n",
    "\n",
    "l_fc2 = ls.layers.DenseLayer(incoming=l_fc1, \n",
    "                             num_units=num_units_dense, \n",
    "                             W=ls.init.GlorotUniform(np.sqrt(2./(1+0.01**2))), \n",
    "                             b=ls.init.Constant(0.0),\n",
    "                             nonlinearity=ls.nonlinearities.LeakyRectify()\n",
    "                            )\n",
    "\n",
    "#output layer\n",
    "l_out = ls.layers.DenseLayer(incoming=l_fc2, \n",
    "                             num_units=hditer.num_classes, \n",
    "                             W=ls.init.GlorotUniform(np.sqrt(2./(1+0.01**2))), \n",
    "                             b=ls.init.Constant(0.0),\n",
    "                             nonlinearity=ls.nonlinearities.softmax\n",
    "                            )\n",
    "\n",
    "#network\n",
    "network = [l_inp_data, l_inp_label,\n",
    "           l_conv1, l_pool1, l_drop1,\n",
    "           l_conv2, l_pool2, l_drop2,\n",
    "           l_conv3, l_pool3, l_drop3,\n",
    "           l_conv4, l_pool4, l_drop4,\n",
    "           l_flat, l_fc1, l_fc2,\n",
    "           l_out\n",
    "          ]\n",
    "\n",
    "#variables\n",
    "inp = l_inp_data.input_var\n",
    "lab = T.imatrix('lab')\n",
    "\n",
    "#output\n",
    "lab_pred = ls.layers.get_output(l_out, {l_inp_data: inp})\n",
    "lab_pred_det = ls.layers.get_output(l_out, {l_inp_data: inp}, deterministic=True)\n",
    "\n",
    "#loss functions:\n",
    "loss = ls.objectives.categorical_crossentropy(lab_pred,lab).mean()\n",
    "loss_det = ls.objectives.categorical_crossentropy(lab_pred_det,lab).mean()\n",
    "\n",
    "#accuracy\n",
    "acc_det = T.mean(T.eq(T.argmax(lab_pred_det, axis=1), lab),dtype=theano.config.floatX)\n",
    "\n",
    "#parameters\n",
    "params = ls.layers.get_all_params(network, trainable=True)\n",
    "\n",
    "#updates\n",
    "updates = ls.updates.adam(loss, params, learning_rate=initial_learning_rate)\n",
    "\n",
    "#compile network function\n",
    "fnn = theano.function([inp], lab_pred)\n",
    "#training function to minimize\n",
    "fnn_train = theano.function([inp,lab], loss, updates=updates)\n",
    "#validation function with accuracy\n",
    "fnn_validate = theano.function([inp,lab], [loss_det,acc_det])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs=10\n",
    "batchsize=128\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in hditer.get_batch(batchsize):\n",
    "        inputs, targets = batch\n",
    "        train_err += fnn_train(inputs, targets)\n",
    "        train_batches += 1\n",
    "        \n",
    "        print train_err\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    #val_err = 0\n",
    "    #val_acc = 0\n",
    "    #val_batches = 0\n",
    "    #for batch in iterate_minibatches(X_val, y_val, 500, shuffle=False):\n",
    "    #    inputs, targets = batch\n",
    "    #    err, acc = val_fn(inputs, targets)\n",
    "    #    val_err += err\n",
    "    #    val_acc += acc\n",
    "    #    val_batches += 1\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    #print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    #print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "    #    val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning_plus_root",
   "language": "python",
   "name": "deeplearning_plus_root"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
