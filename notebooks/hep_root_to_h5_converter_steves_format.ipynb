{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__author__ = 'tkurth'\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from nbfinder import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "#%matplotlib inline\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "#sys.path.append('/global/homes/w/wbhimji/cori-envs/nersc-rootpy/lib/python2.7/site-packages/')\n",
    "#sys.path.append('/global/common/cori/software/root/6.06.06/lib/root')\n",
    "#import ROOT\n",
    "#import rootpy\n",
    "#import root_numpy as rnp\n",
    "import h5py as h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a context manager to suppress stdout and stderr.\n",
    "class suppress_stdout_stderr(object):\n",
    "    '''\n",
    "    A context manager for doing a \"deep suppression\" of stdout and stderr in \n",
    "    Python, i.e. will suppress all print, even if the print originates in a \n",
    "    compiled C/Fortran sub-function.\n",
    "       This will not suppress raised exceptions, since exceptions are printed\n",
    "    to stderr just before a script exits, and after the context manager has\n",
    "    exited (at least, I think that is why it lets exceptions through).      \n",
    "\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # Open a pair of null files\n",
    "        self.null_fds =  [os.open(os.devnull,os.O_RDWR) for x in range(2)]\n",
    "        # Save the actual stdout (1) and stderr (2) file descriptors.\n",
    "        self.save_fds = (os.dup(1), os.dup(2))\n",
    "\n",
    "    def __enter__(self):\n",
    "        # Assign the null pointers to stdout and stderr.\n",
    "        os.dup2(self.null_fds[0],1)\n",
    "        os.dup2(self.null_fds[1],2)\n",
    "\n",
    "    def __exit__(self, *_):\n",
    "        # Re-assign the real stdout/stderr back to (1) and (2)\n",
    "        os.dup2(self.save_fds[0],1)\n",
    "        os.dup2(self.save_fds[1],2)\n",
    "        # Close the null files\n",
    "        os.close(self.null_fds[0])\n",
    "        os.close(self.null_fds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_dicts(dict1,dict2):\n",
    "    tmp = dict1.copy()\n",
    "    tmp.update(dict2)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#file string parser\n",
    "def parse_filename(fname,directory='.'):\n",
    "    directory=re.sub(r'^(.*?)(/+)$',r'\\1',directory)\n",
    "    \n",
    "    #signal file?\n",
    "    smatch=re.compile(r'^GG_RPV(.*?)_(.*?)_(.*?)\\.h5')\n",
    "    tmpres=smatch.findall(fname)\n",
    "    if tmpres:\n",
    "        tmpres=tmpres[0]\n",
    "        return {'rpv':int(tmpres[0]), \n",
    "                'mGlu':int(tmpres[1]), \n",
    "                'mNeu':int(tmpres[2]), \n",
    "                'jz': 0, \n",
    "                'filename': directory+'/'+fname}\n",
    "\n",
    "    #background file?\n",
    "    smatch=re.compile(r'^jetjet_JZ(.*?)\\.h5')\n",
    "    tmpres=smatch.findall(fname)\n",
    "    if tmpres:\n",
    "        return {'rpv': 0., \n",
    "                'mGlu': 0.,\n",
    "                'mNeu': 0., \n",
    "                'jz': int(tmpres[0]), \n",
    "                'filename': directory+'/'+fname}\n",
    "\n",
    "    #nothing at all\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(filelists,\n",
    "                group_name='CollectionTree',\n",
    "                dataset_name='histo',\n",
    "                type_='hdf5'):\n",
    "    \n",
    "    #iterate over elements in the filelists\n",
    "    records=[]\n",
    "    \n",
    "    for fname in tqdm(filelists):\n",
    "        #read specifics of that list\n",
    "        infile=fname.split('/')[-1]\n",
    "        masterrec=parse_filename(infile)\n",
    "        #determine if it is label or background\n",
    "        if masterrec['mGlu']>0 or masterrec['mNeu']>0:\n",
    "            masterrec['label']=1\n",
    "        else:\n",
    "            masterrec['label']=0\n",
    "        \n",
    "        #open the hdf5 file\n",
    "        #we don't want annoying stderr messages\n",
    "        try:\n",
    "            reclist=[]\n",
    "            f= h5.File(fname,'r')\n",
    "            for event in f.items():\n",
    "                if event[0].startswith('event'):\n",
    "                    datarec={}\n",
    "                    \n",
    "                    #event id:\n",
    "                    datarec['eventid']=int(event[0].split('_')[1])\n",
    "                    \n",
    "                    #calorimeter:\n",
    "                    #azimuth\n",
    "                    datarec['calPhi']=event[1]['clusPhi'].value\n",
    "                    #rapidity\n",
    "                    datarec['calEta']=event[1]['clusEta'].value\n",
    "                    #energy deposit\n",
    "                    datarec['calE']=event[1]['clusE'].value\n",
    "                    #EM fraction\n",
    "                    datarec['calEM']=event[1]['clusEM'].value\n",
    "                    \n",
    "                    #tracks:\n",
    "                    #azimuth\n",
    "                    datarec['trackPhi']=event[1]['trackPhi'].value\n",
    "                    #rapidity\n",
    "                    datarec['trackEta']=event[1]['trackEta'].value\n",
    "                    \n",
    "                    #weight\n",
    "                    datarec['weight']=event[1]['weight'].value\n",
    "                    \n",
    "                    #passes standard regression?\n",
    "                    datarec['passSR']=event[1]['passSR'].value\n",
    "                    \n",
    "                    #SUSY theory masses\n",
    "                    #if masterrec['label']==1:\n",
    "                    #    datarec['mGlu']=event[1]['mGlu'].value\n",
    "                    #    datarec['mNeu']=event[1]['mNeu'].value\n",
    "                    #else:\n",
    "                    #    datarec['mGlu']=0.\n",
    "                    #    datarec['mNeu']=0.\n",
    "                    \n",
    "                    #append to master list\n",
    "                    reclist.append(merge_dicts(masterrec,datarec))\n",
    "            \n",
    "            #close file\n",
    "            f.close()\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        #append to records\n",
    "        records.append(pd.DataFrame(reclist))\n",
    "            \n",
    "    #return dataframe\n",
    "    return pd.concat(records)\n",
    "\n",
    "\n",
    "#data augmentation\n",
    "def augment_data(xarr,roll_angle):\n",
    "    #flip in x:\n",
    "    if np.random.random_sample()>=0.5:\n",
    "        xarr=np.fliplr(xarr)\n",
    "    #flip in y:\n",
    "    if np.random.random_sample()>=0.5:\n",
    "        xarr=np.flipud(xarr)\n",
    "    #roll in x with period 2pi/8\n",
    "    randroll=np.random.randint(0,8,size=1)[0]\n",
    "    #determine granularity:\n",
    "    rollunit=randroll*roll_angle\n",
    "    xarr=np.roll(xarr, shift=rollunit, axis=1)\n",
    "    \n",
    "    return xarr\n",
    "    \n",
    "    \n",
    "#preprocessor\n",
    "def preprocess_data(df,eta_range,phi_range,eta_bins,phi_bins):\n",
    "    #empty array\n",
    "    xvals  = np.zeros((df.shape[0], 3, phi_bins, eta_bins ),dtype='float32')\n",
    "    yvals  = np.zeros((df.shape[0],),dtype='int32')\n",
    "    eidvals = np.zeros((df.shape[0],),dtype='int32')\n",
    "    wvals  = np.zeros((df.shape[0],),dtype='float32')\n",
    "    pvals  = np.zeros((df.shape[0],),dtype='int32')\n",
    "    mgvals = np.zeros((df.shape[0],),dtype='float32')\n",
    "    mnvals = np.zeros((df.shape[0],),dtype='float32')\n",
    "    jzvals = np.zeros((df.shape[0],),dtype='int32')\n",
    "    \n",
    "    for i in range(df.shape[0]):\n",
    "        calPhi   = df.iloc[i]['calPhi']\n",
    "        calEta   = df.iloc[i]['calEta']\n",
    "        calE     = df.iloc[i]['calE']\n",
    "        calEM    = df.iloc[i]['calEM']\n",
    "        trackPhi = df.iloc[i]['trackPhi']\n",
    "        trackEta = df.iloc[i]['trackEta']\n",
    "        w        = df.iloc[i]['weight']\n",
    "        psr      = df.iloc[i]['passSR']\n",
    "        mg       = df.iloc[i]['mGlu']\n",
    "        mn       = df.iloc[i]['mNeu']\n",
    "        jz       = df.iloc[i]['jz']\n",
    "        \n",
    "        #data\n",
    "        xvals[i,0,:,:]=np.histogram2d(calPhi,calEta,\n",
    "                                        bins=(phi_bins, eta_bins),\n",
    "                                        weights=calE,\n",
    "                                        range=[phi_range,eta_range])[0]\n",
    "        xvals[i,1,:,:]=np.histogram2d(calPhi,calEta,\n",
    "                                        bins=(phi_bins, eta_bins),\n",
    "                                        weights=calEM,\n",
    "                                        range=[phi_range,eta_range])[0]\n",
    "        xvals[i,2,:,:]=np.histogram2d(trackPhi,trackEta,\n",
    "                                        bins=(phi_bins, eta_bins),\n",
    "                                        range=[phi_range,eta_range])[0]\n",
    "        \n",
    "        #obtain the rest\n",
    "        wvals[i]=w\n",
    "        pvals[i]=psr\n",
    "        mgvals[i]=mg\n",
    "        mnvals[i]=mn\n",
    "        jzvals[i]=jz\n",
    "        yvals[i]=df.iloc[i]['label']\n",
    "        eidvals[i]=df.iloc[i]['eventid']\n",
    "        \n",
    "    return xvals, yvals, wvals, pvals, mgvals, mnvals, jzvals, eidvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class hep_data_iterator:\n",
    "    \n",
    "    #class constructor\n",
    "    def __init__(self,\n",
    "                 datadf,\n",
    "                 max_frequency=None,\n",
    "                 even_frequencies=True,\n",
    "                 nbins=(100,100),\n",
    "                 eta_range = [-5,5],\n",
    "                 phi_range = [-3.1416, 3.1416],\n",
    "                 augment=False,\n",
    "                 compute_max=True\n",
    "                ):\n",
    "\n",
    "        #set parameters\n",
    "        self.nbins = nbins\n",
    "        self.eta_range = eta_range\n",
    "        self.phi_range = phi_range\n",
    "\n",
    "        #even frequencies?\n",
    "        self.even_frequencies=even_frequencies\n",
    "        self.augment=augment\n",
    "        self.compute_max=compute_max\n",
    "        \n",
    "        #compute bins depending on total range\n",
    "        #eta\n",
    "        #eta_step=(self.eta_range[1]-self.eta_range[0])/float(self.nbins[0]-1)\n",
    "        #self.eta_bins = np.arange(self.eta_range[0],self.eta_range[1]+eta_step,eta_step)\n",
    "        self.eta_bins=self.nbins[0]\n",
    "        #phi\n",
    "        #phi_step=(self.phi_range[1]-self.phi_range[0])/float(self.nbins[1]-1)\n",
    "        #self.phi_bins = np.arange(self.phi_range[0],self.phi_range[1]+phi_step,phi_step)\n",
    "        self.phi_bins=self.nbins[1]\n",
    "        \n",
    "        #dataframe\n",
    "        self.df = datadf\n",
    "        self.df.sort_values(by='label',inplace=True)\n",
    "        \n",
    "        #make class frequencies even:\n",
    "        tmpdf=self.df.groupby('label').count().reset_index()\n",
    "        self.num_classes=tmpdf.shape[0]\n",
    "        \n",
    "        #determine minimum frequency\n",
    "        min_frequency=tmpdf['calE'].min()\n",
    "        if max_frequency:\n",
    "            min_frequency=np.min([min_frequency,max_frequency])\n",
    "        elif not self.even_frequencies:\n",
    "            min_frequency=-1\n",
    "        \n",
    "        tmpdf=self.df.groupby(['label']).apply(lambda x: x[['calPhi',\n",
    "                                                            'calEta',\n",
    "                                                            'calE',\n",
    "                                                            'calEM',\n",
    "                                                            'trackPhi',\n",
    "                                                            'trackEta',\n",
    "                                                            'weight',\n",
    "                                                            'passSR',\n",
    "                                                            'mGlu',\n",
    "                                                            'mNeu',\n",
    "                                                            'jz',\n",
    "                                                            'eventid'\n",
    "                                                           ]].iloc[:min_frequency,:]).copy()\n",
    "        \n",
    "        tmpdf.reset_index(inplace=True)\n",
    "        del tmpdf['level_1']\n",
    "        \n",
    "        #copy tmpdf into self.df:\n",
    "        self.df=tmpdf.copy()\n",
    "        \n",
    "        #compute maxima:\n",
    "        if self.compute_max:\n",
    "            self.compute_data_max()\n",
    "            self.compute_weight_max()\n",
    "        \n",
    "        #number of examples\n",
    "        self.num_examples=self.df.shape[0]\n",
    "        \n",
    "        #shapes:\n",
    "        self.xshape=(3, self.phi_bins, self.eta_bins)\n",
    "    \n",
    "    \n",
    "    #shuffle data\n",
    "    def shuffle(self, seed=None):\n",
    "        if seed:\n",
    "            np.random.seed(seed)\n",
    "        self.df=self.df.reindex(np.random.permutation(self.df.index))\n",
    "    \n",
    "    \n",
    "    #compute max over all data\n",
    "    def compute_data_max(self):\n",
    "        '''compute the maximum over all event entries for rescaling data between -1 and 1'''\n",
    "        #initialize\n",
    "        self.max_abs=np.zeros(3)\n",
    "        #fill\n",
    "        self.max_abs[0]=self.df[['calPhi','calEta','calE']].apply(lambda x: np.max(np.histogram2d(x['calPhi'],x['calEta'],\n",
    "                                                                            bins=(self.phi_bins, self.eta_bins),\n",
    "                                                                            weights=x['calE'],\n",
    "                                                                            range=[self.phi_range,self.eta_range])[0]),\n",
    "                                                                  axis=1).max()\n",
    "        self.max_abs[1]=self.df[['calPhi','calEta','calEM']].apply(lambda x: np.max(np.histogram2d(x['calPhi'],x['calEta'],\n",
    "                                                                            bins=(self.phi_bins, self.eta_bins),\n",
    "                                                                            weights=x['calEM'],\n",
    "                                                                            range=[self.phi_range,self.eta_range])[0]),\n",
    "                                                                  axis=1).max()\n",
    "        self.max_abs[2]=self.df[['trackPhi','trackEta']].apply(lambda x: np.max(np.histogram2d(x['trackPhi'],x['trackEta'],\n",
    "                                                                            bins=(self.phi_bins, self.eta_bins),\n",
    "                                                                            range=[self.phi_range,self.eta_range])[0]),\n",
    "                                                                  axis=1).max()\n",
    "    \n",
    "    #compute maximum of weights\n",
    "    def compute_weight_max(self):\n",
    "        '''compute the maximum over all event weight entries for rescaling data between 0 and 1. Take abs to be safe'''\n",
    "        self.wmax=(self.df['weight'].abs()).apply(lambda x: np.max(x)).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curate file list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directory='/project/projectdirs/dasrepo/atlas_rpv_susy/hdf5/delphes_002_2017_01_11'\n",
    "filelists=[parse_filename(x,directory) for x in os.listdir(directory) if x.endswith('h5')]\n",
    "filenamedf=pd.DataFrame(filelists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select signal configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainselect=[{'mGlu':1400, 'mNeu': 850}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#select signal configuration\n",
    "sig_cfg_files=[]\n",
    "for item in trainselect:\n",
    "    sig_cfg_files+=list(filenamedf[ (filenamedf['mGlu']==item['mGlu']) & (filenamedf['mNeu']==item['mNeu']) ]['filename'])\n",
    "\n",
    "#select background configuration\n",
    "jzmin=3\n",
    "jzmax=11\n",
    "bg_cfg_files=list(filenamedf[ (filenamedf['jz']>=jzmin) & (filenamedf['jz']<=jzmax) ]['filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load additional signal files:\n",
    "other_sig_cfg_files=list( filenamedf[ (filenamedf['mGlu']>0.) | (filenamedf['mNeu']>0.) ]['filename'])\n",
    "other_sig_cfg_files=[x for x in other_sig_cfg_files if x not in sig_cfg_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load background files\n",
    "print(\"Loading background data.\")\n",
    "bgdf=load_data(bg_cfg_files)\n",
    "#sort\n",
    "bgdf.sort_values(by=['filename','eventid'],inplace=True)\n",
    "bgdf.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load signal data\n",
    "print(\"Loading signal data.\")\n",
    "sigdf=load_data(sig_cfg_files)\n",
    "#sort\n",
    "sigdf.sort_values(by=['filename','eventid'],inplace=True)\n",
    "sigdf.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load additional signal data\n",
    "print(\"Loading remaining signal data.\")\n",
    "othersigdf=load_data(other_sig_cfg_files)\n",
    "#sort\n",
    "othersigdf.sort_values(by=['filename','eventid'],inplace=True)\n",
    "othersigdf.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameters\n",
    "train_fraction=0.75\n",
    "validation_fraction=0.05\n",
    "nbins=(224,224)\n",
    "#nbins=(64,64)\n",
    "nsig_augment=1\n",
    "total_files_per_jz=640000\n",
    "total_files_per_theory=640000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#do the shuffle\n",
    "#background\n",
    "np.random.seed(13)\n",
    "bgdf=bgdf.reindex(np.random.permutation(bgdf.index))\n",
    "#signal\n",
    "np.random.seed(13)\n",
    "sigdf=sigdf.reindex(np.random.permutation(sigdf.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Reweighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Initial rescaling.\")\n",
    "bgdf['weight']/=np.float(total_files_per_jz)\n",
    "sigdf['weight']/=np.float(total_files_per_theory)\n",
    "#othersigdf['weight']/=np.float(total_files_per_theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine counts and merge with dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Determine frequencies.\")\n",
    "\n",
    "#background:\n",
    "bggroup=bgdf.groupby(['jz'])\n",
    "tmpdf=pd.DataFrame(bggroup['calE'].count())\n",
    "tmpdf.reset_index(inplace=True)\n",
    "tmpdf.rename(columns={'calE':'frequency'},inplace=True)\n",
    "bgdf=bgdf.merge(tmpdf,on='jz',how='left')\n",
    "\n",
    "#signal:\n",
    "siggroup=sigdf.groupby(['mGlu','mNeu'])\n",
    "tmpdf=pd.DataFrame(siggroup['calE'].count())\n",
    "tmpdf.reset_index(inplace=True)\n",
    "tmpdf.rename(columns={'calE':'frequency'},inplace=True)\n",
    "sigdf=sigdf.merge(tmpdf,on=['mGlu','mNeu'],how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Split ensemble.\")\n",
    "\n",
    "#compute sizes:\n",
    "#total\n",
    "num_sig_total=sigdf.shape[0]\n",
    "num_bg_total=bgdf.shape[0]\n",
    "\n",
    "#group sigdf according to mGlu and mNeu:\n",
    "siggroup=sigdf.groupby(['mGlu','mNeu'])\n",
    "bggroup=bgdf.groupby(['jz'])\n",
    "\n",
    "\n",
    "#training\n",
    "#for signal, group according to masses and take the fraction for every theory:\n",
    "trainsigdf=siggroup.apply(lambda x: x.iloc[:int(np.floor(x.shape[0]*train_fraction))])\n",
    "trainsigdf.reset_index(drop=True,inplace=True)\n",
    "#for background, group according to jz and take the fraction for every jz\n",
    "trainbgdf=bggroup.apply(lambda x: x.iloc[:int(np.floor(x.shape[0]*train_fraction))])\n",
    "trainbgdf.reset_index(drop=True,inplace=True)\n",
    "\n",
    "\n",
    "#validation\n",
    "valsigdf=siggroup.apply(lambda x: x.iloc[int(np.floor(x.shape[0]*train_fraction))\n",
    "                                        :int(np.floor(x.shape[0]*train_fraction))+int(np.floor(x.shape[0]*validation_fraction))])\n",
    "valsigdf.reset_index(drop=True,inplace=True)\n",
    "#for background, group according to jz and take the fraction for every jz\n",
    "valbgdf=bggroup.apply(lambda x: x.iloc[int(np.floor(x.shape[0]*train_fraction))\n",
    "                                       :int(np.floor(x.shape[0]*train_fraction))+int(np.floor(x.shape[0]*validation_fraction))])\n",
    "valbgdf.reset_index(drop=True,inplace=True)\n",
    "\n",
    "\n",
    "#test\n",
    "testsigdf=siggroup.apply(lambda x: x.iloc[int(np.floor(x.shape[0]*train_fraction))+int(np.floor(x.shape[0]*validation_fraction)):])\n",
    "testsigdf.reset_index(drop=True,inplace=True)\n",
    "#for background, group according to jz and take the fraction for every jz\n",
    "testbgdf=bggroup.apply(lambda x: x.iloc[int(np.floor(x.shape[0]*train_fraction))+int(np.floor(x.shape[0]*validation_fraction)):])\n",
    "testbgdf.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescale the weights according to splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rescale the weights according to frequencies in mn/mg and jz combinations:\n",
    "#training\n",
    "traindf=pd.concat([trainbgdf,trainsigdf])\n",
    "#traingroup=traindf.groupby(['jz','mGlu','mNeu'])\n",
    "#tmpdf=pd.DataFrame(traingroup['calE'].count())\n",
    "#tmpdf.reset_index(inplace=True)\n",
    "#tmpdf.rename(columns={'calE':'split_frequency'},inplace=True)\n",
    "#traindf=traindf.merge(tmpdf,on=['jz','mGlu','mNeu'],how='left')\n",
    "#traindf['split_fraction']=traindf['split_frequency']/traindf['frequency']\n",
    "#traindf['weight']/=traindf['split_fraction']\n",
    "\n",
    "\n",
    "#validation\n",
    "valdf=pd.concat([valbgdf,valsigdf])\n",
    "#valgroup=valdf.groupby(['jz','mGlu','mNeu'])\n",
    "#tmpdf=pd.DataFrame(valgroup['calE'].count())\n",
    "#tmpdf.reset_index(inplace=True)\n",
    "#tmpdf.rename(columns={'calE':'split_frequency'},inplace=True)\n",
    "#valdf=valdf.merge(tmpdf,on=['jz','mGlu','mNeu'],how='left')\n",
    "#valdf['split_fraction']=valdf['split_frequency']/valdf['frequency']\n",
    "#valdf['weight']/=valdf['split_fraction']\n",
    "\n",
    "\n",
    "#test\n",
    "testdf=pd.concat([testbgdf,testsigdf])\n",
    "#testgroup=testdf.groupby(['jz','mGlu','mNeu'])\n",
    "#tmpdf=pd.DataFrame(testgroup['calE'].count())\n",
    "#tmpdf.reset_index(inplace=True)\n",
    "#tmpdf.rename(columns={'calE':'split_frequency'},inplace=True)\n",
    "#testdf=testdf.merge(tmpdf,on=['jz','mGlu','mNeu'],how='left')\n",
    "#testdf['split_fraction']=testdf['split_frequency']/testdf['frequency']\n",
    "#testdf['weight']/=testdf['split_fraction']\n",
    "\n",
    "#finally, append the other test: no need for rescaling here:\n",
    "#testdf=pd.concat([testdf,othersigdf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Create iterators.\")\n",
    "\n",
    "#create iterators\n",
    "#training\n",
    "hditer_train=hep_data_iterator(traindf,nbins=nbins,even_frequencies=False,augment=True,compute_max=True)\n",
    "#shuffle this one\n",
    "hditer_train.shuffle(13)\n",
    "\n",
    "#validation\n",
    "hditer_validation=hep_data_iterator(valdf,nbins=nbins,even_frequencies=False,compute_max=False)\n",
    "\n",
    "#test\n",
    "hditer_test=hep_data_iterator(testdf,nbins=nbins,even_frequencies=False,compute_max=False)\n",
    "\n",
    "\n",
    "#the preprocessing for the validation iterator has to be taken from the training iterator\n",
    "#validation\n",
    "hditer_validation.max_abs=hditer_train.max_abs\n",
    "hditer_validation.wmax=hditer_train.wmax\n",
    "#test\n",
    "hditer_test.max_abs=hditer_train.max_abs\n",
    "hditer_test.wmax=hditer_train.wmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Max Weight: \",hditer_train.df['weight'].max()\n",
    "print \"Min Weight: \",hditer_train.df['weight'].min()\n",
    "print \"Median Weight: \",hditer_train.df['weight'].median()\n",
    "print \"Mean Weight: \",hditer_train.df['weight'].mean()\n",
    "print \"Sum Weight: \",hditer_train.df['weight'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datadir=\"/global/cscratch1/sd/tkurth/atlas_dl/data_delphes\"\n",
    "numnodes=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print ensemble sizes and determine the chunk size\n",
    "chunksize_train=int(np.ceil(2.*hditer_train.df.ix[ hditer_train.df.label==0 ].shape[0]/numnodes))\n",
    "chunksize_train=int(np.floor(chunksize_train/(2*nsig_augment)))*2*nsig_augment\n",
    "print \"Training size: \",int(np.ceil(2.*hditer_train.df.ix[ hditer_train.df.label==0 ].shape[0])),' chunk size: ',chunksize_train\n",
    "chunksize_validation=int(np.ceil(hditer_validation.num_examples/numnodes))\n",
    "print \"Validation size: \",hditer_validation.num_examples,' chunk size: ',chunksize_validation\n",
    "chunksize_test=np.min([int(np.ceil(hditer_test.num_examples/numnodes)),60000])\n",
    "print \"Test size: \",hditer_test.num_examples,' chunk size: ',chunksize_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Save training files.\")\n",
    "\n",
    "#here we have to treat background and signal separately\n",
    "bgtrain=hditer_train.df.ix[ hditer_train.df.label==0 ]\n",
    "sigtrain=hditer_train.df.ix[ hditer_train.df.label==1 ]\n",
    "upper=int(np.floor(numnodes*chunksize_train/2))\n",
    "\n",
    "for i in tqdm(range(0,numnodes)):\n",
    "    \n",
    "    #get background\n",
    "    ilow=i*chunksize_train/2\n",
    "    iup=np.min([(i+1)*chunksize_train/2,upper])\n",
    "    #preprocess\n",
    "    xbg,ybg,wbg,pbg,mgbg,mnbg,jzbg,eidbg = preprocess_data(bgtrain.iloc[ilow:iup], \\\n",
    "                                        hditer_train.eta_range, \\\n",
    "                                        hditer_train.phi_range, \\\n",
    "                                        hditer_train.eta_bins, \\\n",
    "                                        hditer_train.phi_bins)\n",
    "    for c in range(3):\n",
    "        xbg[:,c,:,:]/=hditer_train.max_abs[c]\n",
    "    \n",
    "    #get signal\n",
    "    ilow=i*chunksize_train/(2*nsig_augment)\n",
    "    iup=np.min([(i+1)*chunksize_train/(2*nsig_augment),upper])\n",
    "    #preprocess\n",
    "    xsg,ysg,wsg,psg,mgsg,mnsg,jzsg,eidsg = preprocess_data(sigtrain.iloc[ilow:iup], \\\n",
    "                                        hditer_train.eta_range, \\\n",
    "                                        hditer_train.phi_range, \\\n",
    "                                        hditer_train.eta_bins, \\\n",
    "                                        hditer_train.phi_bins)\n",
    "    for c in range(3):\n",
    "        xsg[:,c,:,:]/=hditer_train.max_abs[c]\n",
    "    \n",
    "    #tile the arrays\n",
    "    xsg=np.tile(xsg,(nsig_augment,1,1,1))\n",
    "    ysg=np.tile(ysg,(nsig_augment))\n",
    "    wsg=np.tile(wsg,(nsig_augment))\n",
    "    psg=np.tile(psg,(nsig_augment))\n",
    "    mgsg=np.tile(mgsg,(nsig_augment))\n",
    "    mnsg=np.tile(mnsg,(nsig_augment))\n",
    "    jzsg=np.tile(jzsg,(nsig_augment))\n",
    "    eidsg=np.tile(eidsg,(nsig_augment))\n",
    "    #augment the x-values\n",
    "    for k in range(0,xsg.shape[0]):\n",
    "        xsg[k][0]=augment_data(xsg[k][0],int(np.round(hditer_train.phi_bins/8.)))\n",
    "    \n",
    "    #stack them together\n",
    "    x=np.concatenate([xbg,xsg])\n",
    "    y=np.concatenate([ybg,ysg])\n",
    "    w=np.concatenate([wbg,wsg])\n",
    "    p=np.concatenate([pbg,psg])\n",
    "    mg=np.concatenate([mgbg,mgsg])\n",
    "    mn=np.concatenate([mnbg,mnsg])\n",
    "    jz=np.concatenate([jzbg,jzsg])\n",
    "    eid=np.concatenate([eidbg,eidsg])\n",
    "    \n",
    "    #write file\n",
    "    f = h5.File(datadir+'/hep_training_chunk'+str(i)+'.hdf5','w')\n",
    "    f['data']=x\n",
    "    f['label']=y\n",
    "    f['weight']=w\n",
    "    #normalize those weights for training\n",
    "    f['normweight']=w/hditer_train.wmax\n",
    "    f['psr']=p\n",
    "    f['mg']=mg\n",
    "    f['mn']=mn\n",
    "    f['jz']=jz\n",
    "    f['eid']=eid\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Save test files.\")\n",
    "\n",
    "#chunk it to fit it into memory\n",
    "for idx,i in tqdm(enumerate(range(0,hditer_test.num_examples,chunksize_test))):\n",
    "    iup=np.min([i+chunksize_test,hditer_test.num_examples])\n",
    "    \n",
    "    #preprocess\n",
    "    x,y,w,p,mg,mn,jz,eid = preprocess_data(hditer_test.df.iloc[i:iup], \\\n",
    "                            hditer_test.eta_range, \\\n",
    "                            hditer_test.phi_range, \\\n",
    "                            hditer_test.eta_bins, \\\n",
    "                            hditer_test.phi_bins)\n",
    "    for c in range(3):\n",
    "        x[:,c,:,:]/=hditer_train.max_abs[c]\n",
    "    \n",
    "    #write file\n",
    "    f = h5.File(datadir+'/hep_test_chunk'+str(idx)+'.hdf5','w')\n",
    "    f['data']=x\n",
    "    f['label']=y\n",
    "    f['weight']=w\n",
    "    f['normweight']=w/hditer_train.wmax\n",
    "    f['psr']=p\n",
    "    f['mg']=mg\n",
    "    f['mn']=mn\n",
    "    f['jz']=jz\n",
    "    f['eid']=eid\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Save validation files.\")\n",
    "\n",
    "for idx,i in tqdm(enumerate(range(0,hditer_validation.num_examples,chunksize_validation))):\n",
    "    iup=np.min([i+chunksize_validation,hditer_validation.num_examples])\n",
    "    \n",
    "    #preprocess\n",
    "    x,y,w,p,mg,mn,jz,eid = preprocess_data(hditer_validation.df.iloc[i:iup], \\\n",
    "                            hditer_validation.eta_range, \\\n",
    "                            hditer_validation.phi_range, \\\n",
    "                            hditer_validation.eta_bins, \\\n",
    "                            hditer_validation.phi_bins)\n",
    "    for c in range(3):\n",
    "        x[:,c,:,:]/=hditer_train.max_abs[c]\n",
    "    \n",
    "    #write the file\n",
    "    f = h5.File(datadir+'/hep_validation_chunk'+str(idx)+'.hdf5','w')\n",
    "    f['data']=x\n",
    "    f['label']=y\n",
    "    f['weight']=w\n",
    "    #normalize those weights for validation to compare with training loss\n",
    "    f['normweight']=w/hditer_train.wmax\n",
    "    f['psr']=p\n",
    "    f['mg']=mg\n",
    "    f['mn']=mn\n",
    "    f['jz']=jz\n",
    "    f['eid']=eid\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [deeplearning]",
   "language": "python",
   "name": "Python [deeplearning]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
