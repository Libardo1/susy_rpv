{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__author__ = 'tkurth'\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as mlines\n",
    "import matplotlib.font_manager as font_manager\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "from matplotlib import gridspec\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from nbfinder import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "%matplotlib inline\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "#sys.path.append('/global/homes/w/wbhimji/cori-envs/nersc-rootpy/lib/python2.7/site-packages/')\n",
    "#sys.path.append('/global/common/cori/software/root/6.06.06/lib/root')\n",
    "#import ROOT\n",
    "#import rootpy\n",
    "#import root_numpy as rnp\n",
    "import h5py as h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the font dictionaries (for plot title and axis titles)\n",
    "title_font = {'size':'16', 'color':'black', 'weight':'normal',\n",
    "              'verticalalignment':'bottom'} # Bottom vertical alignment for more space\n",
    "axis_font = {'size':'14'}\n",
    "font_prop_title = font_manager.FontProperties(size=40)\n",
    "font_prop_axis = font_manager.FontProperties(size=30)\n",
    "font_prop_axis_labels = font_manager.FontProperties(size=40)\n",
    "font_prop_legend = font_manager.FontProperties(size=26)\n",
    "#plt.style.use('seaborn-talk')\n",
    "#plt.style.available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a context manager to suppress stdout and stderr.\n",
    "class suppress_stdout_stderr(object):\n",
    "    '''\n",
    "    A context manager for doing a \"deep suppression\" of stdout and stderr in \n",
    "    Python, i.e. will suppress all print, even if the print originates in a \n",
    "    compiled C/Fortran sub-function.\n",
    "       This will not suppress raised exceptions, since exceptions are printed\n",
    "    to stderr just before a script exits, and after the context manager has\n",
    "    exited (at least, I think that is why it lets exceptions through).      \n",
    "\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # Open a pair of null files\n",
    "        self.null_fds =  [os.open(os.devnull,os.O_RDWR) for x in range(2)]\n",
    "        # Save the actual stdout (1) and stderr (2) file descriptors.\n",
    "        self.save_fds = (os.dup(1), os.dup(2))\n",
    "\n",
    "    def __enter__(self):\n",
    "        # Assign the null pointers to stdout and stderr.\n",
    "        os.dup2(self.null_fds[0],1)\n",
    "        os.dup2(self.null_fds[1],2)\n",
    "\n",
    "    def __exit__(self, *_):\n",
    "        # Re-assign the real stdout/stderr back to (1) and (2)\n",
    "        os.dup2(self.save_fds[0],1)\n",
    "        os.dup2(self.save_fds[1],2)\n",
    "        # Close the null files\n",
    "        os.close(self.null_fds[0])\n",
    "        os.close(self.null_fds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_dicts(dict1,dict2):\n",
    "    tmp = dict1.copy()\n",
    "    tmp.update(dict2)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#file string parser\n",
    "def parse_filename(fname,directory='.'):\n",
    "    directory=re.sub(r'^(.*?)(/+)$',r'\\1',directory)\n",
    "    \n",
    "    #signal file?\n",
    "    smatch=re.compile(r'^GG_RPV(.*?)_(.*?)_(.*?)\\.h5')\n",
    "    tmpres=smatch.findall(fname)\n",
    "    if tmpres:\n",
    "        tmpres=tmpres[0]\n",
    "        return {'rpv':int(tmpres[0]), \n",
    "                'mGlu':int(tmpres[1]), \n",
    "                'mNeu':int(tmpres[2]), \n",
    "                'jz': 0, \n",
    "                'filename': directory+'/'+fname}\n",
    "\n",
    "    #background file?\n",
    "    smatch=re.compile(r'^jetjet_JZ(.*?)\\.h5')\n",
    "    tmpres=smatch.findall(fname)\n",
    "    if tmpres:\n",
    "        return {'rpv': 0., \n",
    "                'mGlu': 0.,\n",
    "                'mNeu': 0., \n",
    "                'jz': int(tmpres[0]), \n",
    "                'filename': directory+'/'+fname}\n",
    "\n",
    "    #nothing at all\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(filelists,\n",
    "                group_name='CollectionTree',\n",
    "                dataset_name='histo',\n",
    "                type_='hdf5'):\n",
    "    \n",
    "    #iterate over elements in the filelists\n",
    "    records=[]\n",
    "    \n",
    "    for fname in tqdm(filelists):\n",
    "        #read specifics of that list\n",
    "        infile=fname.split('/')[-1]\n",
    "        masterrec=parse_filename(infile)\n",
    "        #determine if it is label or background\n",
    "        if masterrec['mGlu']>0 or masterrec['mNeu']>0:\n",
    "            masterrec['label']=1\n",
    "        else:\n",
    "            masterrec['label']=0\n",
    "        \n",
    "        #open the hdf5 file\n",
    "        #we don't want annoying stderr messages\n",
    "        try:\n",
    "            reclist=[]\n",
    "            f= h5.File(fname,'r')\n",
    "            for event in f.items():\n",
    "                if event[0].startswith('event'):\n",
    "                    datarec={}\n",
    "                    \n",
    "                    #event id:\n",
    "                    datarec['eventid']=int(event[0].split('_')[1])\n",
    "                    \n",
    "                    #calorimeter:\n",
    "                    #azimuth\n",
    "                    datarec['calPhi']=event[1]['clusPhi'].value\n",
    "                    #rapidity\n",
    "                    datarec['calEta']=event[1]['clusEta'].value\n",
    "                    #energy deposit\n",
    "                    datarec['calE']=event[1]['clusE'].value\n",
    "                    #EM fraction\n",
    "                    datarec['calEM']=event[1]['clusEM'].value\n",
    "                    \n",
    "                    #tracks:\n",
    "                    #azimuth\n",
    "                    datarec['trackPhi']=event[1]['trackPhi'].value\n",
    "                    #rapidity\n",
    "                    datarec['trackEta']=event[1]['trackEta'].value\n",
    "                    \n",
    "                    #weight\n",
    "                    datarec['weight']=event[1]['weight'].value\n",
    "                    \n",
    "                    #passes standard regression?\n",
    "                    datarec['passSR']=event[1]['passSR'].value\n",
    "                    \n",
    "                    #SUSY theory masses\n",
    "                    #if masterrec['label']==1:\n",
    "                    #    datarec['mGlu']=event[1]['mGlu'].value\n",
    "                    #    datarec['mNeu']=event[1]['mNeu'].value\n",
    "                    #else:\n",
    "                    #    datarec['mGlu']=0.\n",
    "                    #    datarec['mNeu']=0.\n",
    "                    \n",
    "                    #append to master list\n",
    "                    reclist.append(merge_dicts(masterrec,datarec))\n",
    "            \n",
    "            #close file\n",
    "            f.close()\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        #append to records\n",
    "        records.append(pd.DataFrame(reclist))\n",
    "            \n",
    "    #return dataframe\n",
    "    return pd.concat(records)\n",
    "\n",
    "\n",
    "#data augmentation\n",
    "def augment_data(xarr,roll_angle):\n",
    "    #flip in x:\n",
    "    if np.random.random_sample()>=0.5:\n",
    "        for c in range(xarr.shape[0]):\n",
    "            xarr[c,:,:]=np.fliplr(xarr[c,:,:])\n",
    "    #flip in y:\n",
    "    if np.random.random_sample()>=0.5:\n",
    "        for c in range(xarr.shape[0]):\n",
    "            xarr[c,:,:]=np.flipud(xarr[c,:,:])\n",
    "    #roll in x with period 2pi/8\n",
    "    randroll=np.random.randint(0,8,size=1)[0]\n",
    "    #determine granularity:\n",
    "    rollunit=randroll*roll_angle\n",
    "    for c in range(xarr.shape[0]):\n",
    "        xarr[c,:,:]=np.roll(xarr[c,:,:], shift=rollunit, axis=1)\n",
    "    #return augmented array\n",
    "    return xarr\n",
    "    \n",
    "    \n",
    "#preprocessor\n",
    "def preprocess_data(df,eta_range,phi_range,eta_bins,phi_bins):\n",
    "    #empty array\n",
    "    xvals  = np.zeros((df.shape[0], 3, phi_bins, eta_bins ),dtype='float32')\n",
    "    yvals  = np.zeros((df.shape[0],),dtype='int32')\n",
    "    eidvals = np.zeros((df.shape[0],),dtype='int32')\n",
    "    wvals  = np.zeros((df.shape[0],),dtype='float32')\n",
    "    pvals  = np.zeros((df.shape[0],),dtype='int32')\n",
    "    mgvals = np.zeros((df.shape[0],),dtype='float32')\n",
    "    mnvals = np.zeros((df.shape[0],),dtype='float32')\n",
    "    jzvals = np.zeros((df.shape[0],),dtype='int32')\n",
    "    \n",
    "    for i in range(df.shape[0]):\n",
    "        calPhi   = df.iloc[i]['calPhi']\n",
    "        calEta   = df.iloc[i]['calEta']\n",
    "        calE     = df.iloc[i]['calE']\n",
    "        calEM    = df.iloc[i]['calEM']\n",
    "        trackPhi = df.iloc[i]['trackPhi']\n",
    "        trackEta = df.iloc[i]['trackEta']\n",
    "        w        = df.iloc[i]['weight']\n",
    "        psr      = df.iloc[i]['passSR']\n",
    "        mg       = df.iloc[i]['mGlu']\n",
    "        mn       = df.iloc[i]['mNeu']\n",
    "        jz       = df.iloc[i]['jz']\n",
    "        \n",
    "        #data\n",
    "        xvals[i,0,:,:]=np.histogram2d(calPhi,calEta,\n",
    "                                        bins=(phi_bins, eta_bins),\n",
    "                                        weights=calE,\n",
    "                                        range=[phi_range,eta_range])[0]\n",
    "        xvals[i,1,:,:]=np.histogram2d(calPhi,calEta,\n",
    "                                        bins=(phi_bins, eta_bins),\n",
    "                                        weights=calEM,\n",
    "                                        range=[phi_range,eta_range])[0]\n",
    "        xvals[i,2,:,:]=np.histogram2d(trackPhi,trackEta,\n",
    "                                        bins=(phi_bins, eta_bins),\n",
    "                                        range=[phi_range,eta_range])[0]\n",
    "        \n",
    "        #obtain the rest\n",
    "        wvals[i]=w\n",
    "        pvals[i]=psr\n",
    "        mgvals[i]=mg\n",
    "        mnvals[i]=mn\n",
    "        jzvals[i]=jz\n",
    "        yvals[i]=df.iloc[i]['label']\n",
    "        eidvals[i]=df.iloc[i]['eventid']\n",
    "        \n",
    "    return xvals, yvals, wvals, pvals, mgvals, mnvals, jzvals, eidvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class hep_data_iterator:\n",
    "    \n",
    "    #class constructor\n",
    "    def __init__(self,\n",
    "                 datadf,\n",
    "                 max_frequency=None,\n",
    "                 even_frequencies=True,\n",
    "                 nbins=(100,100),\n",
    "                 eta_range = [-5,5],\n",
    "                 phi_range = [-3.1416, 3.1416],\n",
    "                 augment=False,\n",
    "                 compute_max=True\n",
    "                ):\n",
    "\n",
    "        #set parameters\n",
    "        self.nbins = nbins\n",
    "        self.eta_range = eta_range\n",
    "        self.phi_range = phi_range\n",
    "\n",
    "        #even frequencies?\n",
    "        self.even_frequencies=even_frequencies\n",
    "        self.augment=augment\n",
    "        self.compute_max=compute_max\n",
    "        \n",
    "        #compute bins depending on total range\n",
    "        #eta\n",
    "        #eta_step=(self.eta_range[1]-self.eta_range[0])/float(self.nbins[0]-1)\n",
    "        #self.eta_bins = np.arange(self.eta_range[0],self.eta_range[1]+eta_step,eta_step)\n",
    "        self.eta_bins=self.nbins[0]\n",
    "        #phi\n",
    "        #phi_step=(self.phi_range[1]-self.phi_range[0])/float(self.nbins[1]-1)\n",
    "        #self.phi_bins = np.arange(self.phi_range[0],self.phi_range[1]+phi_step,phi_step)\n",
    "        self.phi_bins=self.nbins[1]\n",
    "        \n",
    "        #dataframe\n",
    "        self.df = datadf\n",
    "        self.df.sort_values(by='label',inplace=True)\n",
    "        \n",
    "        #make class frequencies even:\n",
    "        tmpdf=self.df.groupby('label').count().reset_index()\n",
    "        self.num_classes=tmpdf.shape[0]\n",
    "        \n",
    "        #determine minimum frequency\n",
    "        min_frequency=tmpdf['calE'].min()\n",
    "        if max_frequency:\n",
    "            min_frequency=np.min([min_frequency,max_frequency])\n",
    "        elif not self.even_frequencies:\n",
    "            min_frequency=-1\n",
    "        \n",
    "        tmpdf=self.df.groupby(['label']).apply(lambda x: x[['calPhi',\n",
    "                                                            'calEta',\n",
    "                                                            'calE',\n",
    "                                                            'calEM',\n",
    "                                                            'trackPhi',\n",
    "                                                            'trackEta',\n",
    "                                                            'weight',\n",
    "                                                            'passSR',\n",
    "                                                            'mGlu',\n",
    "                                                            'mNeu',\n",
    "                                                            'jz',\n",
    "                                                            'eventid'\n",
    "                                                           ]].iloc[:min_frequency,:]).copy()\n",
    "        \n",
    "        tmpdf.reset_index(inplace=True)\n",
    "        del tmpdf['level_1']\n",
    "        \n",
    "        #copy tmpdf into self.df:\n",
    "        self.df=tmpdf.copy()\n",
    "        \n",
    "        #compute maxima:\n",
    "        if self.compute_max:\n",
    "            self.compute_data_max()\n",
    "            self.compute_weight_max()\n",
    "        \n",
    "        #number of examples\n",
    "        self.num_examples=self.df.shape[0]\n",
    "        \n",
    "        #shapes:\n",
    "        self.xshape=(3, self.phi_bins, self.eta_bins)\n",
    "    \n",
    "    \n",
    "    #shuffle data\n",
    "    def shuffle(self, seed=None):\n",
    "        if seed:\n",
    "            np.random.seed(seed)\n",
    "        self.df=self.df.reindex(np.random.permutation(self.df.index))\n",
    "    \n",
    "    \n",
    "    #compute max over all data\n",
    "    def compute_data_max(self):\n",
    "        '''compute the maximum over all event entries for rescaling data between -1 and 1'''\n",
    "        #initialize\n",
    "        self.max_abs=np.zeros(3)\n",
    "        #fill\n",
    "        self.max_abs[0]=self.df[['calPhi','calEta','calE']].apply(lambda x: np.max(np.histogram2d(x['calPhi'],x['calEta'],\n",
    "                                                                            bins=(self.phi_bins, self.eta_bins),\n",
    "                                                                            weights=x['calE'],\n",
    "                                                                            range=[self.phi_range,self.eta_range])[0]),\n",
    "                                                                  axis=1).max()\n",
    "        self.max_abs[1]=self.df[['calPhi','calEta','calEM']].apply(lambda x: np.max(np.histogram2d(x['calPhi'],x['calEta'],\n",
    "                                                                            bins=(self.phi_bins, self.eta_bins),\n",
    "                                                                            weights=x['calEM'],\n",
    "                                                                            range=[self.phi_range,self.eta_range])[0]),\n",
    "                                                                  axis=1).max()\n",
    "        self.max_abs[2]=self.df[['trackPhi','trackEta']].apply(lambda x: np.max(np.histogram2d(x['trackPhi'],x['trackEta'],\n",
    "                                                                            bins=(self.phi_bins, self.eta_bins),\n",
    "                                                                            range=[self.phi_range,self.eta_range])[0]),\n",
    "                                                                  axis=1).max()\n",
    "    \n",
    "    #compute maximum of weights\n",
    "    def compute_weight_max(self):\n",
    "        '''compute the maximum over all event weight entries for rescaling data between 0 and 1. Take abs to be safe'''\n",
    "        self.wmax=(self.df['weight'].abs()).apply(lambda x: np.max(x)).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curate file list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '850_03'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-fa8a2e50e599>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#directory='/project/projectdirs/dasrepo/atlas_rpv_susy/hdf5/delphes_002_2017_01_11'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/global/cscratch1/sd/wbhimji/delphes_005_2017_03_06_NoPU'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfilelists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparse_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mfilenamedf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilelists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-d4cebcb6c6e9>\u001b[0m in \u001b[0;36mparse_filename\u001b[0;34m(fname, directory)\u001b[0m\n\u001b[1;32m     10\u001b[0m         return {'rpv':int(tmpres[0]), \n\u001b[1;32m     11\u001b[0m                 \u001b[0;34m'mGlu'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0;34m'mNeu'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0;34m'jz'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 'filename': directory+'/'+fname}\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '850_03'"
     ]
    }
   ],
   "source": [
    "#directory='/project/projectdirs/dasrepo/atlas_rpv_susy/hdf5/delphes_002_2017_01_11'\n",
    "directory='/global/cscratch1/sd/wbhimji/delphes_005_2017_03_06_NoPU'\n",
    "filelists=[parse_filename(x,directory) for x in os.listdir(directory) if x.endswith('h5')]\n",
    "filenamedf=pd.DataFrame(filelists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select signal configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainselect=[{'mGlu':1400, 'mNeu': 850}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#select signal configuration\n",
    "sig_cfg_files=[]\n",
    "for item in trainselect:\n",
    "    sig_cfg_files+=list(filenamedf[ (filenamedf['mGlu']==item['mGlu']) & (filenamedf['mNeu']==item['mNeu']) ]['filename'])\n",
    "\n",
    "#select background configuration\n",
    "jzmin=3\n",
    "jzmax=11\n",
    "bg_cfg_files=list(filenamedf[ (filenamedf['jz']>=jzmin) & (filenamedf['jz']<=jzmax) ]['filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load additional signal files:\n",
    "other_sig_cfg_files=list( filenamedf[ (filenamedf['mGlu']>0.) | (filenamedf['mNeu']>0.) ]['filename'])\n",
    "other_sig_cfg_files=[x for x in other_sig_cfg_files if x not in sig_cfg_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load background files\n",
    "print(\"Loading background data.\")\n",
    "bgdf=load_data(bg_cfg_files)\n",
    "#sort\n",
    "bgdf.sort_values(by=['filename','eventid'],inplace=True)\n",
    "bgdf.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load signal data\n",
    "print(\"Loading signal data.\")\n",
    "sigdf=load_data(sig_cfg_files)\n",
    "#sort\n",
    "sigdf.sort_values(by=['filename','eventid'],inplace=True)\n",
    "sigdf.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load additional signal data\n",
    "print(\"Loading remaining signal data.\")\n",
    "othersigdf=load_data(other_sig_cfg_files)\n",
    "#sort\n",
    "othersigdf.sort_values(by=['filename','eventid'],inplace=True)\n",
    "othersigdf.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameters\n",
    "train_fraction=0.75\n",
    "validation_fraction=0.05\n",
    "nbins=(224,224)\n",
    "#nbins=(64,64)\n",
    "total_files_per_jz=640000\n",
    "total_files_per_theory=640000\n",
    "#this will yield even class frequencies, because there are 9 jz's:\n",
    "nsig_augment=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#do the shuffle\n",
    "#background\n",
    "np.random.seed(13)\n",
    "bgdf=bgdf.reindex(np.random.permutation(bgdf.index))\n",
    "#signal\n",
    "np.random.seed(13)\n",
    "sigdf=sigdf.reindex(np.random.permutation(sigdf.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Reweighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Initial rescaling.\")\n",
    "bgdf['weight']/=np.float(total_files_per_jz)\n",
    "sigdf['weight']/=np.float(total_files_per_theory)\n",
    "#othersigdf['weight']/=np.float(total_files_per_theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine counts and merge with dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Determine frequencies.\")\n",
    "\n",
    "#background:\n",
    "bggroup=bgdf.groupby(['jz'])\n",
    "tmpdf=pd.DataFrame(bggroup['calE'].count())\n",
    "tmpdf.reset_index(inplace=True)\n",
    "tmpdf.rename(columns={'calE':'frequency'},inplace=True)\n",
    "bgdf=bgdf.merge(tmpdf,on='jz',how='left')\n",
    "\n",
    "#signal:\n",
    "siggroup=sigdf.groupby(['mGlu','mNeu'])\n",
    "tmpdf=pd.DataFrame(siggroup['calE'].count())\n",
    "tmpdf.reset_index(inplace=True)\n",
    "tmpdf.rename(columns={'calE':'frequency'},inplace=True)\n",
    "sigdf=sigdf.merge(tmpdf,on=['mGlu','mNeu'],how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Split ensemble.\")\n",
    "\n",
    "#compute sizes:\n",
    "#total\n",
    "num_sig_total=sigdf.shape[0]\n",
    "num_bg_total=bgdf.shape[0]\n",
    "\n",
    "#group sigdf according to mGlu and mNeu:\n",
    "siggroup=sigdf.groupby(['mGlu','mNeu'])\n",
    "bggroup=bgdf.groupby(['jz'])\n",
    "\n",
    "\n",
    "#training\n",
    "#for signal, group according to masses and take the fraction for every theory:\n",
    "trainsigdf=siggroup.apply(lambda x: x.iloc[:int(np.floor(x.shape[0]*train_fraction))])\n",
    "trainsigdf.reset_index(drop=True,inplace=True)\n",
    "#for background, group according to jz and take the fraction for every jz\n",
    "trainbgdf=bggroup.apply(lambda x: x.iloc[:int(np.floor(x.shape[0]*train_fraction))])\n",
    "trainbgdf.reset_index(drop=True,inplace=True)\n",
    "\n",
    "\n",
    "#validation\n",
    "valsigdf=siggroup.apply(lambda x: x.iloc[int(np.floor(x.shape[0]*train_fraction))\n",
    "                                        :int(np.floor(x.shape[0]*train_fraction))+int(np.floor(x.shape[0]*validation_fraction))])\n",
    "valsigdf.reset_index(drop=True,inplace=True)\n",
    "#for background, group according to jz and take the fraction for every jz\n",
    "valbgdf=bggroup.apply(lambda x: x.iloc[int(np.floor(x.shape[0]*train_fraction))\n",
    "                                       :int(np.floor(x.shape[0]*train_fraction))+int(np.floor(x.shape[0]*validation_fraction))])\n",
    "valbgdf.reset_index(drop=True,inplace=True)\n",
    "\n",
    "\n",
    "#test\n",
    "testsigdf=siggroup.apply(lambda x: x.iloc[int(np.floor(x.shape[0]*train_fraction))+int(np.floor(x.shape[0]*validation_fraction)):])\n",
    "testsigdf.reset_index(drop=True,inplace=True)\n",
    "#for background, group according to jz and take the fraction for every jz\n",
    "testbgdf=bggroup.apply(lambda x: x.iloc[int(np.floor(x.shape[0]*train_fraction))+int(np.floor(x.shape[0]*validation_fraction)):])\n",
    "testbgdf.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescale the weights according to splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rescale the weights according to frequencies in mn/mg and jz combinations:\n",
    "#training\n",
    "traindf=pd.concat([trainbgdf,trainsigdf])\n",
    "#traingroup=traindf.groupby(['jz','mGlu','mNeu'])\n",
    "#tmpdf=pd.DataFrame(traingroup['calE'].count())\n",
    "#tmpdf.reset_index(inplace=True)\n",
    "#tmpdf.rename(columns={'calE':'split_frequency'},inplace=True)\n",
    "#traindf=traindf.merge(tmpdf,on=['jz','mGlu','mNeu'],how='left')\n",
    "#traindf['split_fraction']=traindf['split_frequency']/traindf['frequency']\n",
    "#traindf['weight']/=traindf['split_fraction']\n",
    "\n",
    "\n",
    "#validation\n",
    "valdf=pd.concat([valbgdf,valsigdf])\n",
    "#valgroup=valdf.groupby(['jz','mGlu','mNeu'])\n",
    "#tmpdf=pd.DataFrame(valgroup['calE'].count())\n",
    "#tmpdf.reset_index(inplace=True)\n",
    "#tmpdf.rename(columns={'calE':'split_frequency'},inplace=True)\n",
    "#valdf=valdf.merge(tmpdf,on=['jz','mGlu','mNeu'],how='left')\n",
    "#valdf['split_fraction']=valdf['split_frequency']/valdf['frequency']\n",
    "#valdf['weight']/=valdf['split_fraction']\n",
    "\n",
    "\n",
    "#test\n",
    "testdf=pd.concat([testbgdf,testsigdf])\n",
    "#testgroup=testdf.groupby(['jz','mGlu','mNeu'])\n",
    "#tmpdf=pd.DataFrame(testgroup['calE'].count())\n",
    "#tmpdf.reset_index(inplace=True)\n",
    "#tmpdf.rename(columns={'calE':'split_frequency'},inplace=True)\n",
    "#testdf=testdf.merge(tmpdf,on=['jz','mGlu','mNeu'],how='left')\n",
    "#testdf['split_fraction']=testdf['split_frequency']/testdf['frequency']\n",
    "#testdf['weight']/=testdf['split_fraction']\n",
    "\n",
    "#finally, append the other test: no need for rescaling here:\n",
    "#testdf=pd.concat([testdf,othersigdf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Create iterators.\")\n",
    "\n",
    "#create iterators\n",
    "#training\n",
    "hditer_train=hep_data_iterator(traindf,nbins=nbins,even_frequencies=False,augment=True,compute_max=True)\n",
    "#shuffle this one\n",
    "hditer_train.shuffle(13)\n",
    "\n",
    "#validation\n",
    "hditer_validation=hep_data_iterator(valdf,nbins=nbins,even_frequencies=False,compute_max=False)\n",
    "#shuffle this one, so that I do not need to parse all validation files to find positive and negative examples\n",
    "hditer_validation.shuffle(13)\n",
    "\n",
    "#test\n",
    "hditer_test=hep_data_iterator(testdf,nbins=nbins,even_frequencies=False,compute_max=False)\n",
    "#shuffle this one, so that I do not need to parse all test files to find positive and negative examples\n",
    "hditer_test.shuffle(13)\n",
    "\n",
    "\n",
    "#the preprocessing for the validation iterator has to be taken from the training iterator\n",
    "#validation\n",
    "hditer_validation.max_abs=hditer_train.max_abs\n",
    "hditer_validation.wmax=hditer_train.wmax\n",
    "#test\n",
    "hditer_test.max_abs=hditer_train.max_abs\n",
    "hditer_test.wmax=hditer_train.wmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Max Weight: \",hditer_train.df['weight'].max()\n",
    "print \"Min Weight: \",hditer_train.df['weight'].min()\n",
    "print \"Median Weight: \",hditer_train.df['weight'].median()\n",
    "print \"Mean Weight: \",hditer_train.df['weight'].mean()\n",
    "print \"Sum Weight: \",hditer_train.df['weight'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datadir=\"/global/cscratch1/sd/tkurth/atlas_dl/data_delphes_new\"\n",
    "numnodes=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print ensemble sizes and determine the chunk size\n",
    "chunksize_train=int(np.ceil(2.*hditer_train.df.ix[ hditer_train.df.label==0 ].shape[0]/numnodes))\n",
    "chunksize_train=int(np.floor(chunksize_train/(2*nsig_augment)))*2*nsig_augment\n",
    "print \"Training size: \",int(np.ceil(2.*hditer_train.df.ix[ hditer_train.df.label==0 ].shape[0])),' chunk size: ',chunksize_train\n",
    "chunksize_validation=int(np.ceil(hditer_validation.num_examples/numnodes))\n",
    "print \"Validation size: \",hditer_validation.num_examples,' chunk size: ',chunksize_validation\n",
    "chunksize_test=np.min([int(np.ceil(hditer_test.num_examples/numnodes)),60000])\n",
    "print \"Test size: \",hditer_test.num_examples,' chunk size: ',chunksize_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Save training files.\")\n",
    "\n",
    "#here we have to treat background and signal separately\n",
    "bgtrain=hditer_train.df.ix[ hditer_train.df.label==0 ]\n",
    "sigtrain=hditer_train.df.ix[ hditer_train.df.label==1 ]\n",
    "upper=int(np.floor(numnodes*chunksize_train/2))\n",
    "\n",
    "for i in tqdm(range(0,numnodes)):\n",
    "    \n",
    "    #get background\n",
    "    ilow=i*chunksize_train/2\n",
    "    iup=np.min([(i+1)*chunksize_train/2,upper])\n",
    "    #preprocess\n",
    "    xbg,ybg,wbg,pbg,mgbg,mnbg,jzbg,eidbg = preprocess_data(bgtrain.iloc[ilow:iup], \\\n",
    "                                        hditer_train.eta_range, \\\n",
    "                                        hditer_train.phi_range, \\\n",
    "                                        hditer_train.eta_bins, \\\n",
    "                                        hditer_train.phi_bins)\n",
    "    for c in range(3):\n",
    "        xbg[:,c,:,:]/=hditer_train.max_abs[c]\n",
    "    \n",
    "    #get signal\n",
    "    ilow=i*chunksize_train/(2*nsig_augment)\n",
    "    iup=np.min([(i+1)*chunksize_train/(2*nsig_augment),upper])\n",
    "    #preprocess\n",
    "    xsg,ysg,wsg,psg,mgsg,mnsg,jzsg,eidsg = preprocess_data(sigtrain.iloc[ilow:iup], \\\n",
    "                                        hditer_train.eta_range, \\\n",
    "                                        hditer_train.phi_range, \\\n",
    "                                        hditer_train.eta_bins, \\\n",
    "                                        hditer_train.phi_bins)\n",
    "    for c in range(3):\n",
    "        xsg[:,c,:,:]/=hditer_train.max_abs[c]\n",
    "\n",
    "    #tile the arrays\n",
    "    xsg=np.tile(xsg,(nsig_augment,1,1,1))\n",
    "    ysg=np.tile(ysg,(nsig_augment))\n",
    "    wsg=np.tile(wsg,(nsig_augment))\n",
    "    psg=np.tile(psg,(nsig_augment))\n",
    "    mgsg=np.tile(mgsg,(nsig_augment))\n",
    "    mnsg=np.tile(mnsg,(nsig_augment))\n",
    "    jzsg=np.tile(jzsg,(nsig_augment))\n",
    "    eidsg=np.tile(eidsg,(nsig_augment))\n",
    "    #augment the x-values\n",
    "    for k in range(0,xsg.shape[0]):\n",
    "        xsg[k,:,:,:]=augment_data(xsg[k,:,:,:],int(np.round(hditer_train.phi_bins/8.)))\n",
    "   \n",
    "    #stack them together\n",
    "    x=np.concatenate([xbg,xsg])\n",
    "    y=np.concatenate([ybg,ysg])\n",
    "    w=np.concatenate([wbg,wsg])\n",
    "    p=np.concatenate([pbg,psg])\n",
    "    mg=np.concatenate([mgbg,mgsg])\n",
    "    mn=np.concatenate([mnbg,mnsg])\n",
    "    jz=np.concatenate([jzbg,jzsg])\n",
    "    eid=np.concatenate([eidbg,eidsg])\n",
    "    \n",
    "    #write file\n",
    "    f = h5.File(datadir+'/hep_training_chunk'+str(i)+'.hdf5','w')\n",
    "    f['data']=x\n",
    "    f['label']=y\n",
    "    f['weight']=w\n",
    "    #normalize those weights for training\n",
    "    f['normweight']=w/hditer_train.wmax\n",
    "    f['psr']=p\n",
    "    f['mg']=mg\n",
    "    f['mn']=mn\n",
    "    f['jz']=jz\n",
    "    f['eid']=eid\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Save test files.\")\n",
    "\n",
    "#chunk it to fit it into memory\n",
    "for idx,i in tqdm(enumerate(range(0,hditer_test.num_examples,chunksize_test))):\n",
    "    iup=np.min([i+chunksize_test,hditer_test.num_examples])\n",
    "    \n",
    "    #preprocess\n",
    "    x,y,w,p,mg,mn,jz,eid = preprocess_data(hditer_test.df.iloc[i:iup], \\\n",
    "                            hditer_test.eta_range, \\\n",
    "                            hditer_test.phi_range, \\\n",
    "                            hditer_test.eta_bins, \\\n",
    "                            hditer_test.phi_bins)\n",
    "    for c in range(3):\n",
    "        x[:,c,:,:]/=hditer_train.max_abs[c]\n",
    "    \n",
    "    #write file\n",
    "    f = h5.File(datadir+'/hep_test_chunk'+str(idx)+'.hdf5','w')\n",
    "    f['data']=x\n",
    "    f['label']=y\n",
    "    f['weight']=w\n",
    "    f['normweight']=w/hditer_train.wmax\n",
    "    f['psr']=p\n",
    "    f['mg']=mg\n",
    "    f['mn']=mn\n",
    "    f['jz']=jz\n",
    "    f['eid']=eid\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Save validation files.\")\n",
    "\n",
    "for idx,i in tqdm(enumerate(range(0,hditer_validation.num_examples,chunksize_validation))):\n",
    "    iup=np.min([i+chunksize_validation,hditer_validation.num_examples])\n",
    "    \n",
    "    #preprocess\n",
    "    x,y,w,p,mg,mn,jz,eid = preprocess_data(hditer_validation.df.iloc[i:iup], \\\n",
    "                            hditer_validation.eta_range, \\\n",
    "                            hditer_validation.phi_range, \\\n",
    "                            hditer_validation.eta_bins, \\\n",
    "                            hditer_validation.phi_bins)\n",
    "    for c in range(3):\n",
    "        x[:,c,:,:]/=hditer_train.max_abs[c]\n",
    "    \n",
    "    #write the file\n",
    "    f = h5.File(datadir+'/hep_validation_chunk'+str(idx)+'.hdf5','w')\n",
    "    f['data']=x\n",
    "    f['label']=y\n",
    "    f['weight']=w\n",
    "    #normalize those weights for validation to compare with training loss\n",
    "    f['normweight']=w/hditer_train.wmax\n",
    "    f['psr']=p\n",
    "    f['mg']=mg\n",
    "    f['mn']=mn\n",
    "    f['jz']=jz\n",
    "    f['eid']=eid\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Test the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testfiles=[x for x in os.listdir(datadir) if x.endswith('.hdf5')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testresults=[]\n",
    "for fname in tqdm(testfiles):\n",
    "    f = h5.File(datadir+'/'+fname,'r')\n",
    "    data=f['data'].value\n",
    "    label=f['label'].value\n",
    "    normweight=f['normweight'].value\n",
    "    f.close()\n",
    "    \n",
    "    #scan filename for phase:\n",
    "    phase=fname.split(\"_\")[1]\n",
    "    \n",
    "    #compute stats:\n",
    "    maxvals=np.max(data,axis=(2,3))\n",
    "    minvals=np.min(data,axis=(2,3))\n",
    "    normvals=np.linalg.norm(data,axis=(2,3))\n",
    "    \n",
    "    #compute density\n",
    "    density=np.sum(data>0,axis=(2,3))/float(np.prod(data.shape[2:4]))\n",
    "    \n",
    "    #iterate over batches:\n",
    "    for ind in range(data.shape[0]):\n",
    "        tmpdict={'filename':fname, 'batchid': ind, 'phase':phase}\n",
    "        \n",
    "        tmpdict['max']=maxvals[ind,:]\n",
    "        tmpdict['min']=minvals[ind,:]\n",
    "        tmpdict['norm']=normvals[ind,:]\n",
    "        tmpdict['label']=label[ind]\n",
    "        tmpdict['normweight']=normweight[ind]\n",
    "        tmpdict['density']=density[ind]\n",
    "        testresults.append(tmpdict)\n",
    "\n",
    "#store in dataframe\n",
    "testdf=pd.DataFrame(testresults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot the results\n",
    "nbins=100\n",
    "nrows=3\n",
    "numcols=3\n",
    "fig, axvec= plt.subplots(figsize=(20*numcols, 10*nrows), nrows=nrows, ncols=numcols)\n",
    "\n",
    "#use those colors\n",
    "colors=['crimson','dodgerblue','aquamarine']\n",
    "\n",
    "for row,typ in enumerate(['max','norm','density']):\n",
    "    for channel in range(3):\n",
    "        #get axis\n",
    "        ax=axvec[row][channel]\n",
    "        \n",
    "        #set properties\n",
    "        ax.set_title(\"Type: \"+typ+\" channel \"+str(channel),fontproperties=font_prop_title)\n",
    "        for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "            label.set_fontproperties(font_prop_axis)\n",
    "            label.set_fontsize(32)\n",
    "        #ax.set_xlabel('memory-mode',fontproperties=font_prop_axis)\n",
    "        ax.set_ylabel('counts',fontproperties=font_prop_axis)\n",
    "        \n",
    "        #create histogram\n",
    "        for idx,phase in enumerate(testdf.phase.unique()):\n",
    "            #project data\n",
    "            data=testdf[typ].ix[ testdf[\"phase\"]==phase ].apply(lambda x: x[channel])\n",
    "            Y, X = np.histogram(data, nbins, density=True)\n",
    "            X=[ (X[i]+X[i+1])*0.5 for i in range(len(X)-1)]\n",
    "            width=(X[1]-X[0])/3.\n",
    "            ax.bar(X,Y,width=width, color=colors[idx])\n",
    "        \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
