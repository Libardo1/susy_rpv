{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__author__ = 'tkurth'\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from nbfinder import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "%matplotlib inline\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "#sys.path.append('/global/homes/w/wbhimji/cori-envs/nersc-rootpy/lib/python2.7/site-packages/')\n",
    "#sys.path.append('/global/common/cori/software/root/6.06.06/lib/root')\n",
    "#import ROOT\n",
    "#import rootpy\n",
    "#import root_numpy as rnp\n",
    "import h5py as h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a context manager to suppress stdout and stderr.\n",
    "class suppress_stdout_stderr(object):\n",
    "    '''\n",
    "    A context manager for doing a \"deep suppression\" of stdout and stderr in \n",
    "    Python, i.e. will suppress all print, even if the print originates in a \n",
    "    compiled C/Fortran sub-function.\n",
    "       This will not suppress raised exceptions, since exceptions are printed\n",
    "    to stderr just before a script exits, and after the context manager has\n",
    "    exited (at least, I think that is why it lets exceptions through).      \n",
    "\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # Open a pair of null files\n",
    "        self.null_fds =  [os.open(os.devnull,os.O_RDWR) for x in range(2)]\n",
    "        # Save the actual stdout (1) and stderr (2) file descriptors.\n",
    "        self.save_fds = (os.dup(1), os.dup(2))\n",
    "\n",
    "    def __enter__(self):\n",
    "        # Assign the null pointers to stdout and stderr.\n",
    "        os.dup2(self.null_fds[0],1)\n",
    "        os.dup2(self.null_fds[1],2)\n",
    "\n",
    "    def __exit__(self, *_):\n",
    "        # Re-assign the real stdout/stderr back to (1) and (2)\n",
    "        os.dup2(self.save_fds[0],1)\n",
    "        os.dup2(self.save_fds[1],2)\n",
    "        # Close the null files\n",
    "        os.close(self.null_fds[0])\n",
    "        os.close(self.null_fds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_dicts(dict1,dict2):\n",
    "    tmp = dict1.copy()\n",
    "    tmp.update(dict2)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#file string parser\n",
    "def parse_filename(fname,directory='.'):\n",
    "    directory=re.sub(r'^(.*?)(/+)$',r'\\1',directory)\n",
    "    \n",
    "    #signal file?\n",
    "    smatch=re.compile(r'^GG_RPV(.*?)_(.*?)_(.*?)\\.h5')\n",
    "    tmpres=smatch.findall(fname)\n",
    "    if tmpres:\n",
    "        tmpres=tmpres[0]\n",
    "        return {'rpv':int(tmpres[0]), 'mglu':int(tmpres[1]), 'mneu':int(tmpres[2]), 'name':directory+'/'+fname}\n",
    "\n",
    "    #background file?\n",
    "    smatch=re.compile(r'^jetjet_JZ(.*?)\\.h5')\n",
    "    tmpres=smatch.findall(fname)\n",
    "    if tmpres:\n",
    "        return {'jz':int(tmpres[0]), 'name':directory+'/'+fname}\n",
    "\n",
    "    #nothing at all\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(filelists,\n",
    "                group_name='CollectionTree',\n",
    "                dataset_name='histo',\n",
    "                type_='hdf5'):\n",
    "    \n",
    "    #iterate over elements in the filelists\n",
    "    records=[]\n",
    "    \n",
    "    for fname in tqdm(filelists):\n",
    "        #read specifics of that list\n",
    "        masterrec=parse_filename(fname.split('/')[-1])\n",
    "        #determine if it is label or background\n",
    "        if 'jz' in masterrec.keys():\n",
    "            masterrec['label']=0\n",
    "        else:\n",
    "            masterrec['label']=1\n",
    "        \n",
    "        #open the hdf5 file\n",
    "        #we don't want annoying stderr messages\n",
    "        try:\n",
    "            reclist=[]\n",
    "            f= h5.File(fname,'r')\n",
    "            for event in f.items():\n",
    "                if event[0].startswith('event'):\n",
    "                    datarec={}\n",
    "                    \n",
    "                    #calorimeter:\n",
    "                    #azimuth\n",
    "                    datarec['calPhi']=event[1]['clusPhi'].value\n",
    "                    #rapidity\n",
    "                    datarec['calEta']=event[1]['clusEta'].value\n",
    "                    #energy deposit\n",
    "                    datarec['calE']=event[1]['clusE'].value\n",
    "                    #EM fraction\n",
    "                    datarec['calEM']=event[1]['clusEM'].value\n",
    "                    \n",
    "                    #tracks:\n",
    "                    #azimuth\n",
    "                    datarec['trackPhi']=event[1]['trackPhi'].value\n",
    "                    #rapidity\n",
    "                    datarec['trackEta']=event[1]['trackEta'].value\n",
    "                    \n",
    "                    #weight\n",
    "                    datarec['weight']=event[1]['weight'].value\n",
    "                    \n",
    "                    #passes standard regression?\n",
    "                    datarec['passSR']=event[1]['passSR'].value\n",
    "                    \n",
    "                    #SUSY theory masses\n",
    "                    if masterrec['label']==1:\n",
    "                        datarec['mGlu']=event[1]['mGlu'].value\n",
    "                        datarec['mNeu']=event[1]['mNeu'].value\n",
    "                    else:\n",
    "                        datarec['mGlu']=0.\n",
    "                        datarec['mNeu']=0.\n",
    "                    \n",
    "                    reclist.append(merge_dicts(masterrec,datarec))\n",
    "            \n",
    "            #close file\n",
    "            f.close()\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        #append to records\n",
    "        records+=reclist\n",
    "            \n",
    "    #return dataframe\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "#data augmentation\n",
    "def augment_data(xarr,roll_angle):\n",
    "    #flip in x:\n",
    "    if np.random.random_sample()>=0.5:\n",
    "        xarr=np.fliplr(xarr)\n",
    "    #flip in y:\n",
    "    if np.random.random_sample()>=0.5:\n",
    "        xarr=np.flipud(xarr)\n",
    "    #roll in x with period 2pi/8\n",
    "    randroll=np.random.randint(0,8,size=1)[0]\n",
    "    #determine granularity:\n",
    "    rollunit=randroll*roll_angle\n",
    "    xarr=np.roll(xarr, shift=rollunit, axis=1)\n",
    "    \n",
    "    return xarr\n",
    "    \n",
    "    \n",
    "#preprocessor\n",
    "def preprocess_data(df,eta_range,phi_range,eta_bins,phi_bins):\n",
    "    #empty array\n",
    "    xvals = np.zeros((df.shape[0], 3, phi_bins, eta_bins ),dtype='float32')\n",
    "    yvals = np.zeros((df.shape[0],),dtype='int32')\n",
    "    wvals = np.zeros((df.shape[0],),dtype='float32')\n",
    "    pvals = np.zeros((df.shape[0],),dtype='int32')\n",
    "    mgvals = np.zeros((df.shape[0],),dtype='float32')\n",
    "    mnvals = np.zeros((df.shape[0],),dtype='float32')\n",
    "    \n",
    "    for i in range(df.shape[0]):\n",
    "        calPhi   = df.iloc[i]['calPhi']\n",
    "        calEta   = df.iloc[i]['calEta']\n",
    "        calE     = df.iloc[i]['calE']\n",
    "        calEM    = df.iloc[i]['calEM']\n",
    "        trackPhi = df.iloc[i]['trackPhi']\n",
    "        trackEta = df.iloc[i]['trackEta']\n",
    "        w        = df.iloc[i]['weight']\n",
    "        psr      = df.iloc[i]['passSR']\n",
    "        mg       = df.iloc[i]['mGlu']\n",
    "        mn       = df.iloc[i]['mNeu']                                       \n",
    "        \n",
    "        #data\n",
    "        xvals[i,0,:,:]=np.histogram2d(calPhi,calEta,\n",
    "                                        bins=(phi_bins, eta_bins),\n",
    "                                        weights=calE,\n",
    "                                        range=[phi_range,eta_range])[0]\n",
    "        xvals[i,1,:,:]=np.histogram2d(calPhi,calEta,\n",
    "                                        bins=(phi_bins, eta_bins),\n",
    "                                        weights=calEM,\n",
    "                                        range=[phi_range,eta_range])[0]\n",
    "        xvals[i,2,:,:]=np.histogram2d(trackPhi,trackEta,\n",
    "                                        bins=(phi_bins, eta_bins),\n",
    "                                        range=[phi_range,eta_range])[0]\n",
    "        \n",
    "        #obtain the rest\n",
    "        wvals[i]=w\n",
    "        pvals[i]=psr\n",
    "        mgvals[i]=mg\n",
    "        mnvals[i]=mn\n",
    "        yvals[i]=df.iloc[i]['label']\n",
    "        \n",
    "    return xvals, yvals, wvals, pvals, mgvals, mnvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class hep_data_iterator:\n",
    "    \n",
    "    #class constructor\n",
    "    def __init__(self,\n",
    "                 datadf,\n",
    "                 max_frequency=None,\n",
    "                 even_frequencies=True,\n",
    "                 shuffle=True,\n",
    "                 nbins=(100,100),\n",
    "                 eta_range = [-5,5],\n",
    "                 phi_range = [-3.1416, 3.1416],\n",
    "                 augment=False,\n",
    "                 compute_max=True\n",
    "                ):\n",
    "\n",
    "        #set parameters\n",
    "        self.shuffle = shuffle\n",
    "        self.nbins = nbins\n",
    "        self.eta_range = eta_range\n",
    "        self.phi_range = phi_range\n",
    "\n",
    "        #even frequencies?\n",
    "        self.even_frequencies=even_frequencies\n",
    "        self.augment=augment\n",
    "        self.compute_max=compute_max\n",
    "        \n",
    "        #compute bins depending on total range\n",
    "        #eta\n",
    "        #eta_step=(self.eta_range[1]-self.eta_range[0])/float(self.nbins[0]-1)\n",
    "        #self.eta_bins = np.arange(self.eta_range[0],self.eta_range[1]+eta_step,eta_step)\n",
    "        self.eta_bins=self.nbins[0]\n",
    "        #phi\n",
    "        #phi_step=(self.phi_range[1]-self.phi_range[0])/float(self.nbins[1]-1)\n",
    "        #self.phi_bins = np.arange(self.phi_range[0],self.phi_range[1]+phi_step,phi_step)\n",
    "        self.phi_bins=self.nbins[1]\n",
    "        \n",
    "        #dataframe\n",
    "        self.df = datadf\n",
    "        self.df.sort_values(by='label',inplace=True)\n",
    "        \n",
    "        #make class frequencies even:\n",
    "        tmpdf=self.df.groupby('label').count().reset_index()\n",
    "        self.num_classes=tmpdf.shape[0]\n",
    "        \n",
    "        #determine minimum frequency\n",
    "        min_frequency=tmpdf['calE'].min()\n",
    "        if max_frequency:\n",
    "            min_frequency=np.min([min_frequency,max_frequency])\n",
    "        elif not self.even_frequencies:\n",
    "            min_frequency=-1\n",
    "        \n",
    "        tmpdf=self.df.groupby(['label']).apply(lambda x: x[['calPhi',\n",
    "                                                            'calEta',\n",
    "                                                            'calE',\n",
    "                                                            'calEM',\n",
    "                                                            'trackPhi',\n",
    "                                                            'trackEta',\n",
    "                                                            'weight',\n",
    "                                                            'passSR',\n",
    "                                                            'mGlu',\n",
    "                                                            'mNeu'\n",
    "                                                           ]].iloc[:min_frequency,:]).copy()\n",
    "        \n",
    "        tmpdf.reset_index(inplace=True)\n",
    "        del tmpdf['level_1']\n",
    "        \n",
    "        #copy tmpdf into self.df:\n",
    "        self.df=tmpdf.copy()\n",
    "        \n",
    "        #compute maxima:\n",
    "        if self.compute_max:\n",
    "            self.compute_data_max()\n",
    "            self.compute_weight_max()\n",
    "        \n",
    "        #shuffle if wanted (highly recommended)\n",
    "        if self.shuffle:\n",
    "            self.df=self.df.reindex(np.random.permutation(self.df.index))\n",
    "        \n",
    "        #number of examples\n",
    "        self.num_examples=self.df.shape[0]\n",
    "        \n",
    "        #shapes:\n",
    "        self.xshape=(3, self.phi_bins, self.eta_bins)\n",
    "        \n",
    "    \n",
    "    #compute max over all data\n",
    "    def compute_data_max(self):\n",
    "        '''compute the maximum over all event entries for rescaling data between -1 and 1'''\n",
    "        #initialize\n",
    "        self.max_abs=np.zeros(3)\n",
    "        #fill\n",
    "        self.max_abs[0]=self.df[['calPhi','calEta','calE']].apply(lambda x: np.max(np.histogram2d(x['calPhi'],x['calEta'],\n",
    "                                                                            bins=(self.phi_bins, self.eta_bins),\n",
    "                                                                            weights=x['calE'],\n",
    "                                                                            range=[self.phi_range,self.eta_range])[0]),\n",
    "                                                                  axis=1).max()\n",
    "        self.max_abs[1]=self.df[['calPhi','calEta','calEM']].apply(lambda x: np.max(np.histogram2d(x['calPhi'],x['calEta'],\n",
    "                                                                            bins=(self.phi_bins, self.eta_bins),\n",
    "                                                                            weights=x['calEM'],\n",
    "                                                                            range=[self.phi_range,self.eta_range])[0]),\n",
    "                                                                  axis=1).max()\n",
    "        self.max_abs[2]=self.df[['trackPhi','trackEta']].apply(lambda x: np.max(np.histogram2d(x['trackPhi'],x['trackEta'],\n",
    "                                                                            bins=(self.phi_bins, self.eta_bins),\n",
    "                                                                            range=[self.phi_range,self.eta_range])[0]),\n",
    "                                                                  axis=1).max()\n",
    "    \n",
    "    #compute maximum of weights\n",
    "    def compute_weight_max(self):\n",
    "        '''compute the maximum over all event weight entries for rescaling data between 0 and 1. Take abs to be safe'''\n",
    "        self.wmax=(self.df['weight'].abs()).apply(lambda x: np.max(x)).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curate file list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directory='/project/projectdirs/dasrepo/atlas_rpv_susy/hdf5/prod005_2017_01_11'\n",
    "filelists=[parse_filename(x,directory) for x in os.listdir(directory) if x.endswith('h5')]\n",
    "filenamedf=pd.DataFrame(filelists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select signal configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainselect=[{'mglu':1400, 'mneu': 850}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select signal configuration\n",
    "sig_cfg_files=[]\n",
    "for item in trainselect:\n",
    "    sig_cfg_files+=list(filenamedf[ (filenamedf['mglu']==item['mglu']) & (filenamedf['mneu']==item['mneu']) ]['name'])\n",
    "\n",
    "#select background configuration\n",
    "jzmin=3\n",
    "jzmax=11\n",
    "bg_cfg_files=list(filenamedf[ (filenamedf['jz']>=jzmin) & (filenamedf['jz']<=jzmax) ]['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load additional signal files:\n",
    "other_sig_cfg_files=list( filenamedf[ (filenamedf['mglu']>0.) | (filenamedf['mneu']>0.) ]['name'])\n",
    "other_sig_cfg_files=[x for x in other_sig_cfg_files if x not in sig_cfg_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load background files\n",
    "bgdf=load_data(bg_cfg_files)\n",
    "#shuffle\n",
    "np.random.seed(13)\n",
    "bgdf=bgdf.reindex(np.random.permutation(bgdf.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load signal data\n",
    "sigdf=load_data(sig_cfg_files)\n",
    "#shuffle\n",
    "np.random.seed(13)\n",
    "sigdf=sigdf.reindex(np.random.permutation(sigdf.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load additional signal data\n",
    "othersigdf=load_data(other_sig_cfg_files)\n",
    "#shuffle\n",
    "#np.random.seed(13)\n",
    "#sigdf=sigdf.reindex(np.random.permutation(sigdf.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameters\n",
    "train_fraction=0.75\n",
    "validation_fraction=0.05\n",
    "nbins=(224,224)\n",
    "#nbins=(64,64)\n",
    "nsig_augment=24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compute sizes:\n",
    "#total\n",
    "num_sig_total=sigdf.shape[0]\n",
    "num_bg_total=bgdf.shape[0]\n",
    "\n",
    "#group sigdf according to mGlu and mNeu:\n",
    "siggroup=sigdf.groupby(['mGlu','mNeu'])\n",
    "\n",
    "#training\n",
    "#for signal, group according to masses and take the fraction for every theory:\n",
    "trainsigdf=siggroup.apply(lambda x: x.iloc[:int(np.floor(x.shape[0]*train_fraction))])\n",
    "trainsigdf.reset_index(drop=True,inplace=True)\n",
    "num_sig_train=trainsigdf.shape[0]\n",
    "num_bg_train=nsig_augment*num_sig_train\n",
    "\n",
    "#validation\n",
    "valsigdf=siggroup.apply(lambda x: x.iloc[int(np.floor(x.shape[0]*train_fraction))\n",
    "                                        :int(np.floor(x.shape[0]*train_fraction))+int(np.floor(x.shape[0]*validation_fraction))])\n",
    "valsigdf.reset_index(drop=True,inplace=True)\n",
    "num_sig_validation=valsigdf.shape[0]\n",
    "num_bg_validation=int(np.floor(bgdf.shape[0]*validation_fraction))\n",
    "\n",
    "#test\n",
    "testsigdf=siggroup.apply(lambda x: x.iloc[int(np.floor(x.shape[0]*train_fraction))+int(np.floor(x.shape[0]*validation_fraction)):])\n",
    "testsigdf.reset_index(drop=True,inplace=True)\n",
    "num_sig_test=testsigdf.shape[0]\n",
    "num_bg_test=bgdf.iloc[num_bg_train+num_bg_validation:].shape[0]\n",
    "\n",
    "\n",
    "\n",
    "#split the sets and rescale the weights\n",
    "#training\n",
    "traindf=pd.concat([bgdf.iloc[:num_bg_train],trainsigdf])\n",
    "#rescale signal\n",
    "traindf['weight'].ix[ traindf.label==1 ]/=( np.float(num_sig_train*nsig_augment)/np.float(num_sig_total) )\n",
    "#rescale background\n",
    "traindf['weight'].ix[ traindf.label==0 ]/= ( np.float(num_bg_train)/np.float(num_bg_total) )\n",
    "\n",
    "#validation\n",
    "validdf=pd.concat([bgdf.iloc[num_bg_train:num_bg_train+num_bg_validation],valsigdf])\n",
    "#rescale signal\n",
    "validdf['weight'].ix[ validdf.label==1 ]/=( np.float(num_sig_validation)/np.float(num_sig_total) )\n",
    "#rescale background\n",
    "validdf['weight'].ix[ validdf.label==0 ]/=( np.float(num_bg_validation)/np.float(num_bg_total) )\n",
    "\n",
    "#test: first work on splitted sets and rescale those accordingly\n",
    "testdf=pd.concat([bgdf.iloc[num_bg_train+num_bg_validation:],testsigdf])\n",
    "#rescale signal\n",
    "testdf['weight'].ix[ testdf.label==1 ]/=( np.float(num_sig_test)/np.float(num_sig_total) )\n",
    "#rescale background\n",
    "testdf['weight'].ix[ testdf.label==0 ]/=( np.float(num_bg_test)/np.float(num_bg_total) )\n",
    "#now, append the rest of the test samples and sort the whole thing, because order does not matter here:\n",
    "#testdf=pd.concat([testdf,othersigdf])\n",
    "testdf.sort_values(by=[\"mGlu\",\"mNeu\"],inplace=True)\n",
    "testdf.reset_index(drop=True,inplace=True)\n",
    "\n",
    "#create iterators\n",
    "hditer_train=hep_data_iterator(traindf,nbins=nbins,even_frequencies=False,augment=True,compute_max=True)\n",
    "hditer_validation=hep_data_iterator(validdf,nbins=nbins,even_frequencies=False,compute_max=False)\n",
    "hditer_test=hep_data_iterator(testdf,nbins=nbins,even_frequencies=False,compute_max=False)\n",
    "\n",
    "#the preprocessing for the validation iterator has to be taken from the training iterator\n",
    "#validation\n",
    "hditer_validation.max_abs=hditer_train.max_abs\n",
    "hditer_validation.wmax=hditer_train.wmax\n",
    "#test\n",
    "hditer_test.max_abs=hditer_train.max_abs\n",
    "hditer_test.wmax=hditer_train.wmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Weight:  46786.3890602\n",
      "Min Weight:  2.86535372051e-08\n",
      "Median Weight:  0.0254081439478\n",
      "Mean Weight:  17.3370874518\n",
      "Sum Weight:  6046274.57462\n"
     ]
    }
   ],
   "source": [
    "print \"Max Weight: \",hditer_train.df['weight'].max()\n",
    "print \"Min Weight: \",hditer_train.df['weight'].min()\n",
    "print \"Median Weight: \",hditer_train.df['weight'].median()\n",
    "print \"Mean Weight: \",hditer_train.df['weight'].mean()\n",
    "print \"Sum Weight: \",hditer_train.df['weight'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datadir=\"/global/cscratch1/sd/tkurth/atlas_dl/data_preselect_augmented\"\n",
    "numnodes=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size:  669598  chunk size:  5232\n",
      "Validation size:  128630  chunk size:  1004\n",
      "Test size:  2095260  chunk size:  16369\n"
     ]
    }
   ],
   "source": [
    "#print ensemble sizes and determine the chunk size\n",
    "chunksize_train=int(np.ceil(2.*hditer_train.df.ix[ hditer_train.df.label==0 ].shape[0]/numnodes))\n",
    "chunksize_train=int(np.floor(chunksize_train/(2*nsig_augment)))*2*nsig_augment\n",
    "print \"Training size: \",int(np.ceil(2.*hditer_train.df.ix[ hditer_train.df.label==0 ].shape[0])),' chunk size: ',chunksize_train\n",
    "chunksize_validation=int(np.ceil(hditer_validation.num_examples/numnodes))\n",
    "print \"Validation size: \",hditer_validation.num_examples,' chunk size: ',chunksize_validation\n",
    "chunksize_test=np.min([int(np.ceil(hditer_test.num_examples/numnodes)),60000])\n",
    "print \"Test size: \",hditer_test.num_examples,' chunk size: ',chunksize_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#here we have to treat background and signal separately\n",
    "bgtrain=hditer_train.df.ix[ hditer_train.df.label==0 ]\n",
    "sigtrain=hditer_train.df.ix[ hditer_train.df.label==1 ]\n",
    "upper=int(np.floor(numnodes*chunksize_train/2))\n",
    "\n",
    "for i in range(0,numnodes):\n",
    "    \n",
    "    #get background\n",
    "    ilow=i*chunksize_train/2\n",
    "    iup=np.min([(i+1)*chunksize_train/2,upper])\n",
    "    #preprocess\n",
    "    xbg,ybg,wbg,pbg,mgbg,mnbg=preprocess_data(bgtrain.iloc[ilow:iup], \\\n",
    "                        hditer_train.eta_range, \\\n",
    "                        hditer_train.phi_range, \\\n",
    "                        hditer_train.eta_bins, \\\n",
    "                        hditer_train.phi_bins)\n",
    "    for c in range(3):\n",
    "        xbg[:,c,:,:]/=hditer_train.max_abs[c]\n",
    "    \n",
    "    #get signal\n",
    "    ilow=i*chunksize_train/(2*nsig_augment)\n",
    "    iup=np.min([(i+1)*chunksize_train/(2*nsig_augment),upper])\n",
    "    #preprocess\n",
    "    xsg,ysg,wsg,psg,mgsg,mnsg=preprocess_data(sigtrain.iloc[ilow:iup], \\\n",
    "                        hditer_train.eta_range, \\\n",
    "                        hditer_train.phi_range, \\\n",
    "                        hditer_train.eta_bins, \\\n",
    "                        hditer_train.phi_bins)\n",
    "    for c in range(3):\n",
    "        xsg[:,c,:,:]/=hditer_train.max_abs[c]\n",
    "    \n",
    "    #tile the arrays\n",
    "    xsg=np.tile(xsg,(nsig_augment,1,1,1))\n",
    "    ysg=np.tile(ysg,(nsig_augment))\n",
    "    wsg=np.tile(wsg,(nsig_augment))\n",
    "    psg=np.tile(psg,(nsig_augment))\n",
    "    mgsg=np.tile(mgsg,(nsig_augment))\n",
    "    mnsg=np.tile(mnsg,(nsig_augment))\n",
    "    #augment the x-values\n",
    "    for k in range(0,xsg.shape[0]):\n",
    "        xsg[k][0]=augment_data(xsg[k][0],int(np.round(hditer_train.phi_bins/8.)))\n",
    "    \n",
    "    #stack them together\n",
    "    x=np.concatenate([xbg,xsg])\n",
    "    y=np.concatenate([ybg,ysg])\n",
    "    w=np.concatenate([wbg,wsg])\n",
    "    p=np.concatenate([pbg,psg])\n",
    "    mg=np.concatenate([mgbg,mgsg])\n",
    "    mn=np.concatenate([mnbg,mnsg])\n",
    "    \n",
    "    #write file\n",
    "    f = h5.File(datadir+'/hep_training_chunk'+str(i)+'.hdf5','w')\n",
    "    f['data']=x\n",
    "    f['label']=y\n",
    "    f['weight']=w\n",
    "    #normalize those weights for training\n",
    "    f['normweight']=w/hditer_train.wmax\n",
    "    f['psr']=p\n",
    "    f['mg']=mg\n",
    "    f['mn']=mn\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-dfaabd80410c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#write file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatadir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/hep_test_chunk'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.hdf5'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/home/ilan/minonda/conda-bld/work/h5py/_objects.c:2696)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/home/ilan/minonda/conda-bld/work/h5py/_objects.c:2654)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/global/common/cori/software/python/2.7-anaconda/envs/deeplearning/lib/python2.7/site-packages/h5py/_hl/group.pyc\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, name, obj)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguess_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m             \u001b[0mh5o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/cori/software/python/2.7-anaconda/envs/deeplearning/lib/python2.7/site-packages/h5py/_hl/group.pyc\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \"\"\"\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mdsid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_new_dset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m             \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/cori/software/python/2.7-anaconda/envs/deeplearning/lib/python2.7/site-packages/h5py/_hl/dataset.pyc\u001b[0m in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mdset_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdset_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#chunk it to fit it into memory\n",
    "for idx,i in enumerate(range(0,hditer_test.num_examples,chunksize_test)):\n",
    "    iup=np.min([i+chunksize_test,hditer_test.num_examples])\n",
    "    \n",
    "    #preprocess\n",
    "    x,y,w,p,mg,mn=preprocess_data(hditer_test.df.iloc[i:iup], \\\n",
    "                        hditer_test.eta_range, \\\n",
    "                        hditer_test.phi_range, \\\n",
    "                        hditer_test.eta_bins, \\\n",
    "                        hditer_test.phi_bins)\n",
    "    for c in range(3):\n",
    "        x[:,c,:,:]/=hditer_train.max_abs[c]\n",
    "    \n",
    "    #write file\n",
    "    f = h5.File(datadir+'/hep_test_chunk'+str(idx)+'.hdf5','w')\n",
    "    f['data']=x\n",
    "    f['label']=y\n",
    "    f['weight']=w\n",
    "    f['normweight']=w/hditer_train.wmax\n",
    "    f['psr']=p\n",
    "    f['mg']=mg\n",
    "    f['mn']=mn\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx,i in enumerate(range(0,hditer_validation.num_examples,chunksize_validation)):\n",
    "    iup=np.min([i+chunksize_validation,hditer_validation.num_examples])\n",
    "    \n",
    "    #preprocess\n",
    "    x,y,w,p,mg,mn=preprocess_data(hditer_validation.df.iloc[i:iup], \\\n",
    "                    hditer_validation.eta_range, \\\n",
    "                    hditer_validation.phi_range, \\\n",
    "                    hditer_validation.eta_bins, \\\n",
    "                    hditer_validation.phi_bins)\n",
    "    for c in range(3):\n",
    "        x[:,c,:,:]/=hditer_train.max_abs[c]\n",
    "    \n",
    "    #write the file\n",
    "    f = h5.File(datadir+'/hep_validation_chunk'+str(idx)+'.hdf5','w')\n",
    "    f['data']=x\n",
    "    f['label']=y\n",
    "    f['weight']=w\n",
    "    #normalize those weights for validation to compare with training loss\n",
    "    f['normweight']=w/hditer_train.wmax\n",
    "    f['psr']=p\n",
    "    f['mg']=mg\n",
    "    f['mn']=mn\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [deeplearning]",
   "language": "python",
   "name": "Python [deeplearning]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
