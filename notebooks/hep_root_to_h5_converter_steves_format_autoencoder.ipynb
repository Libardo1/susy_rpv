{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__author__ = 'tkurth'\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from nbfinder import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "%matplotlib inline\n",
    "import time\n",
    "import re\n",
    "#sys.path.append('/global/homes/w/wbhimji/cori-envs/nersc-rootpy/lib/python2.7/site-packages/')\n",
    "#sys.path.append('/global/common/cori/software/root/6.06.06/lib/root')\n",
    "#import ROOT\n",
    "#import rootpy\n",
    "#import root_numpy as rnp\n",
    "import h5py as h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a context manager to suppress stdout and stderr.\n",
    "class suppress_stdout_stderr(object):\n",
    "    '''\n",
    "    A context manager for doing a \"deep suppression\" of stdout and stderr in \n",
    "    Python, i.e. will suppress all print, even if the print originates in a \n",
    "    compiled C/Fortran sub-function.\n",
    "       This will not suppress raised exceptions, since exceptions are printed\n",
    "    to stderr just before a script exits, and after the context manager has\n",
    "    exited (at least, I think that is why it lets exceptions through).      \n",
    "\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # Open a pair of null files\n",
    "        self.null_fds =  [os.open(os.devnull,os.O_RDWR) for x in range(2)]\n",
    "        # Save the actual stdout (1) and stderr (2) file descriptors.\n",
    "        self.save_fds = (os.dup(1), os.dup(2))\n",
    "\n",
    "    def __enter__(self):\n",
    "        # Assign the null pointers to stdout and stderr.\n",
    "        os.dup2(self.null_fds[0],1)\n",
    "        os.dup2(self.null_fds[1],2)\n",
    "\n",
    "    def __exit__(self, *_):\n",
    "        # Re-assign the real stdout/stderr back to (1) and (2)\n",
    "        os.dup2(self.save_fds[0],1)\n",
    "        os.dup2(self.save_fds[1],2)\n",
    "        # Close the null files\n",
    "        os.close(self.null_fds[0])\n",
    "        os.close(self.null_fds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_dicts(dict1,dict2):\n",
    "    tmp = dict1.copy()\n",
    "    tmp.update(dict2)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#file string parser\n",
    "def parse_filename(fname,directory='.'):\n",
    "    directory=re.sub(r'^(.*?)(/+)$',r'\\1',directory)\n",
    "    \n",
    "    #signal file?\n",
    "    smatch=re.compile(r'^GG_RPV(.*?)_(.*?)_(.*?)\\.h5')\n",
    "    tmpres=smatch.findall(fname)\n",
    "    if tmpres:\n",
    "        tmpres=tmpres[0]\n",
    "        return {'rpv':int(tmpres[0]), 'mass1':int(tmpres[1]), 'mass2':int(tmpres[2]), 'name':directory+'/'+fname}\n",
    "\n",
    "    #background file?\n",
    "    smatch=re.compile(r'^jetjet_JZ(.*?)\\.h5')\n",
    "    tmpres=smatch.findall(fname)\n",
    "    if tmpres:\n",
    "        return {'jz':int(tmpres[0]), 'name':directory+'/'+fname}\n",
    "\n",
    "    #nothing at all\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(filelists,\n",
    "                group_name='CollectionTree',\n",
    "                branches=['clusPhi',\n",
    "                          'clusEta',\n",
    "                          'clusE',\n",
    "                         'weight',\n",
    "                         'passSR'],\n",
    "                dataset_name='histo',\n",
    "                type_='hdf5'):\n",
    "    \n",
    "    #iterate over elements in the filelists\n",
    "    records=[]\n",
    "    \n",
    "    for fname in filelists:\n",
    "        #read specifics of that list\n",
    "        masterrec=parse_filename(fname.split('/')[-1])\n",
    "        #determine if it is label or background\n",
    "        if 'jz' in masterrec.keys():\n",
    "            masterrec['label']=0\n",
    "        else:\n",
    "            masterrec['label']=1\n",
    "        \n",
    "        #open the hdf5 file\n",
    "        #we don't want annoying stderr messages\n",
    "        try:\n",
    "            reclist=[]\n",
    "            f= h5.File(fname,'r')\n",
    "            for event in f.items():\n",
    "                if event[0].startswith('event'):\n",
    "                    datarec={}\n",
    "                    \n",
    "                    datarec['CaloCalTopoClustersAuxDyn.calPhi']=event[1][branches[0]].value\n",
    "                    datarec['CaloCalTopoClustersAuxDyn.calEta']=event[1][branches[1]].value\n",
    "                    datarec['CaloCalTopoClustersAuxDyn.calE']=event[1][branches[2]].value\n",
    "                    datarec['CaloCalTopoClustersAuxDyn.weight']=event[1][branches[3]].value\n",
    "                    datarec['CaloCalTopoClustersAuxDyn.passSR']=event[1][branches[4]].value\n",
    "                    if masterrec['label']==1:\n",
    "                        datarec['CaloCalTopoClustersAuxDyn.mGlu']=event[1]['mGlu'].value\n",
    "                        datarec['CaloCalTopoClustersAuxDyn.mNeu']=event[1]['mNeu'].value\n",
    "                    else:\n",
    "                        datarec['CaloCalTopoClustersAuxDyn.mGlu']=0.\n",
    "                        datarec['CaloCalTopoClustersAuxDyn.mNeu']=0.\n",
    "                    \n",
    "                    reclist.append(merge_dicts(masterrec,datarec))\n",
    "            \n",
    "            #close file\n",
    "            f.close()\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        #append to records\n",
    "        records+=reclist\n",
    "            \n",
    "    #return dataframe\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "#data augmentation\n",
    "def augment_data(xarr,roll_angle):\n",
    "    #flip in x:\n",
    "    if np.random.random_sample()>=0.5:\n",
    "        xarr=np.fliplr(xarr)\n",
    "    #flip in y:\n",
    "    if np.random.random_sample()>=0.5:\n",
    "        xarr=np.flipud(xarr)\n",
    "    #roll in x with period 2pi/8\n",
    "    randroll=np.random.randint(0,8,size=1)[0]\n",
    "    #determine granularity:\n",
    "    rollunit=randroll*roll_angle\n",
    "    xarr=np.roll(xarr, shift=rollunit, axis=1)\n",
    "    \n",
    "    return xarr\n",
    "    \n",
    "    \n",
    "#preprocessor\n",
    "def preprocess_data(df,eta_range,phi_range,eta_bins,phi_bins):\n",
    "    #empty array\n",
    "    xvals = np.zeros((df.shape[0], 1, phi_bins, eta_bins ),dtype='float32')\n",
    "    yvals = np.zeros((df.shape[0],),dtype='int32')\n",
    "    wvals = np.zeros((df.shape[0],),dtype='float32')\n",
    "    pvals = np.zeros((df.shape[0],),dtype='int32')\n",
    "    mgvals = np.zeros((df.shape[0],),dtype='float32')\n",
    "    mnvals = np.zeros((df.shape[0],),dtype='float32')\n",
    "    \n",
    "    for i in range(df.shape[0]):        \n",
    "        phi, eta, E, w, psr, mg, mn =  df.iloc[i]['CaloCalTopoClustersAuxDyn.calPhi'], \\\n",
    "                                       df.iloc[i]['CaloCalTopoClustersAuxDyn.calEta'], \\\n",
    "                                       df.iloc[i]['CaloCalTopoClustersAuxDyn.calE'], \\\n",
    "                                       df.iloc[i]['CaloCalTopoClustersAuxDyn.weight'], \\\n",
    "                                       df.iloc[i]['CaloCalTopoClustersAuxDyn.passSR'], \\\n",
    "                                       df.iloc[i]['CaloCalTopoClustersAuxDyn.mGlu'], \\\n",
    "                                       df.iloc[i]['CaloCalTopoClustersAuxDyn.mNeu']\n",
    "        \n",
    "        xvals[i]=np.histogram2d(phi,eta,\n",
    "                                bins=(phi_bins, eta_bins),\n",
    "                                weights=E,\n",
    "                                range=[phi_range,eta_range])[0]\n",
    "        \n",
    "        #obtain the rest\n",
    "        wvals[i]=w\n",
    "        pvals[i]=psr\n",
    "        mgvals[i]=mg\n",
    "        mnvals[i]=mn\n",
    "        yvals[i]=df.iloc[i]['label']\n",
    "        \n",
    "    return xvals, yvals, wvals, pvals, mgvals, mnvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class hep_data_iterator:\n",
    "    \n",
    "    #class constructor\n",
    "    def __init__(self,\n",
    "                 datadf,\n",
    "                 max_frequency=None,\n",
    "                 even_frequencies=True,\n",
    "                 shuffle=True,\n",
    "                 nbins=(100,100),\n",
    "                 eta_range = [-5,5],\n",
    "                 phi_range = [-3.1416, 3.1416],\n",
    "                 augment=False\n",
    "                ):\n",
    "\n",
    "        #set parameters\n",
    "        self.shuffle = shuffle\n",
    "        self.nbins = nbins\n",
    "        self.eta_range = eta_range\n",
    "        self.phi_range = phi_range\n",
    "        \n",
    "        #even frequencies?\n",
    "        self.even_frequencies=even_frequencies\n",
    "        self.augment=augment\n",
    "        \n",
    "        #compute bins depending on total range\n",
    "        #eta\n",
    "        #eta_step=(self.eta_range[1]-self.eta_range[0])/float(self.nbins[0]-1)\n",
    "        #self.eta_bins = np.arange(self.eta_range[0],self.eta_range[1]+eta_step,eta_step)\n",
    "        self.eta_bins=self.nbins[0]\n",
    "        #phi\n",
    "        #phi_step=(self.phi_range[1]-self.phi_range[0])/float(self.nbins[1]-1)\n",
    "        #self.phi_bins = np.arange(self.phi_range[0],self.phi_range[1]+phi_step,phi_step)\n",
    "        self.phi_bins=self.nbins[1]\n",
    "        \n",
    "        #dataframe\n",
    "        self.df = datadf\n",
    "        self.df.sort_values(by='label',inplace=True)\n",
    "        \n",
    "        #make class frequencies even:\n",
    "        tmpdf=self.df.groupby('label').count().reset_index()\n",
    "        self.num_classes=tmpdf.shape[0]\n",
    "        \n",
    "        #determine minimum frequency\n",
    "        min_frequency=tmpdf['CaloCalTopoClustersAuxDyn.calE'].min()\n",
    "        if max_frequency:\n",
    "            min_frequency=np.min([min_frequency,max_frequency])\n",
    "        elif not self.even_frequencies:\n",
    "            min_frequency=-1\n",
    "        \n",
    "        tmpdf=self.df.groupby(['label']).apply(lambda x: x[['CaloCalTopoClustersAuxDyn.calPhi',\n",
    "                                                            'CaloCalTopoClustersAuxDyn.calEta',\n",
    "                                                            'CaloCalTopoClustersAuxDyn.calE',\n",
    "                                                            'CaloCalTopoClustersAuxDyn.weight',\n",
    "                                                            'CaloCalTopoClustersAuxDyn.passSR',\n",
    "                                                            'CaloCalTopoClustersAuxDyn.mGlu',\n",
    "                                                            'CaloCalTopoClustersAuxDyn.mNeu'\n",
    "                                                           ]].iloc[:min_frequency,:]).copy()\n",
    "        \n",
    "        tmpdf.reset_index(inplace=True)\n",
    "        del tmpdf['level_1']\n",
    "        \n",
    "        #copy tmpdf into self.df:\n",
    "        self.df=tmpdf.copy()\n",
    "        \n",
    "        #compute max:\n",
    "        self.compute_data_max()\n",
    "        self.compute_weight_max()\n",
    "        \n",
    "        #shuffle if wanted (highly recommended)\n",
    "        if self.shuffle:\n",
    "            self.df=self.df.reindex(np.random.permutation(self.df.index))\n",
    "        \n",
    "        #number of examples\n",
    "        self.num_examples=self.df.shape[0]\n",
    "        \n",
    "        #shapes:\n",
    "        self.xshape=(1, self.phi_bins, self.eta_bins)\n",
    "        \n",
    "    \n",
    "    #compute max over all data\n",
    "    def compute_data_max(self):\n",
    "        '''compute the maximum over all event entries for rescaling data between -1 and 1'''\n",
    "        self.max_abs=(self.df['CaloCalTopoClustersAuxDyn.calE'].abs()).apply(lambda x: np.max(x)).max()\n",
    "        \n",
    "    def compute_weight_max(self):\n",
    "        '''compute the maximum over all event weight entries for rescaling data between 0 and 1. Take abs to be safe'''\n",
    "        self.wmax=(self.df['CaloCalTopoClustersAuxDyn.weight'].abs()).apply(lambda x: np.max(x)).max()\n",
    "    \n",
    "    \n",
    "    #get a random chunk: here, even_freq means even frequencies in chunk.\n",
    "    def get_chunk(self,chunksize,even_freq=False):\n",
    "        shuffledf=self.df.reindex(np.random.permutation(self.df.index))\n",
    "        if not even_freq:\n",
    "            return shuffledf.copy()\n",
    "        else:\n",
    "            #group by classes:\n",
    "            chunksize_per_class=int(np.ceil(chunksize/self.num_classes))\n",
    "            tmpdf=shuffledf.groupby('label').apply(lambda x: x.iloc[:chunksize_per_class,:]).copy()\n",
    "            tmpdf.reset_index(drop=True,inplace=True)\n",
    "            return tmpdf.reindex(np.random.permutation(tmpdf.index))\n",
    "    \n",
    "    \n",
    "    #this is the batch iterator:\n",
    "    def next_batch(self,batchsize):\n",
    "        '''batch iterator'''\n",
    "        \n",
    "        #shuffle:\n",
    "        if self.shuffle:\n",
    "            self.df=self.df.reindex(np.random.permutation(self.df.index))\n",
    "        \n",
    "        #iterate\n",
    "        for idx in range(0,self.num_examples-batchsize,batchsize):\n",
    "            #yield next batch\n",
    "            x,y,w,p=preprocess_data(self.df.iloc[idx:idx+batchsize,:],\n",
    "                             self.eta_range,\n",
    "                             self.phi_range,\n",
    "                             self.eta_bins,self.phi_bins)\n",
    "            #rescale x:\n",
    "            x/=self.max_abs\n",
    "        \n",
    "            #return result\n",
    "            yield x,y,w,p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curate file list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directory='/project/projectdirs/dasrepo/atlas_rpv_susy/hdf5/prod004_2016_11_30'\n",
    "filelists=[parse_filename(x,directory) for x in os.listdir(directory) if x.endswith('h5')]\n",
    "filenamedf=pd.DataFrame(filelists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select signal configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select signal configuration\n",
    "#mass1=1400\n",
    "#mass2=850\n",
    "sig_cfg_files=list(filenamedf[ (filenamedf['mass1']>0.) & (filenamedf['mass2']>0.) ]['name'])\n",
    "#sig_cfg_files=list(filenamedf['name'])\n",
    "\n",
    "#select background configuration\n",
    "jzmin=1\n",
    "jzmax=11\n",
    "bg_cfg_files=list(filenamedf[ (filenamedf['jz']>=jzmin) & (filenamedf['jz']<=jzmax) ]['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load background files\n",
    "bgdf=load_data(bg_cfg_files)\n",
    "np.random.seed(13)\n",
    "bgdf=bgdf.reindex(np.random.permutation(bgdf.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load signal data\n",
    "sigdf=load_data(sig_cfg_files)\n",
    "np.random.seed(13)\n",
    "sigdf=sigdf.reindex(np.random.permutation(sigdf.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/common/cori/software/python/2.7-anaconda/envs/deeplearning/lib/python2.7/site-packages/ipykernel/__main__.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#parameters\n",
    "train_fraction=0.85\n",
    "train_fit_fraction=0.3\n",
    "validation_fraction=0.05\n",
    "nbins=(224,224)\n",
    "\n",
    "#create sizes:\n",
    "#total\n",
    "num_sig_total=sigdf.shape[0]\n",
    "num_bg_total=bgdf.shape[0]\n",
    "#training\n",
    "num_bg_train=int(np.floor(bgdf.shape[0]*train_fraction))\n",
    "#validation\n",
    "num_bg_validation=int(np.floor(bgdf.shape[0]*validation_fraction))\n",
    "\n",
    "#split the sets\n",
    "#we need two training sets here, because we fit the distribution also:\n",
    "traindf=bgdf.iloc[:int(np.floor(num_bg_train*(1.-train_fit_fraction)))]\n",
    "traindf_fit=bgdf.iloc[int(np.floor(num_bg_train*(1.-train_fit_fraction))):num_bg_train]\n",
    "validdf=bgdf.iloc[num_bg_train:num_bg_train+num_bg_validation]\n",
    "testdf=bgdf.iloc[num_bg_train+num_bg_validation:]\n",
    "\n",
    "#create iterators\n",
    "hditer_train=hep_data_iterator(traindf,nbins=nbins,even_frequencies=False)\n",
    "hditer_train_fit=hep_data_iterator(traindf_fit,nbins=nbins,even_frequencies=False)\n",
    "hditer_validation=hep_data_iterator(validdf,nbins=nbins,even_frequencies=False)\n",
    "hditer_test=hep_data_iterator(testdf,nbins=nbins,even_frequencies=False)\n",
    "hditer_test_signal=hep_data_iterator(sigdf,nbins=nbins,even_frequencies=False)\n",
    "\n",
    "#the preprocessing for the validation iterator has to be taken from the training iterator\n",
    "hditer_validation.max_abs=hditer_train.max_abs\n",
    "hditer_train_fit.max_abs=hditer_train.max_abs\n",
    "hditer_test.max_abs=hditer_train.max_abs\n",
    "hditer_test_signal.max_abs=hditer_train.max_abs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datadir=\"/global/cscratch1/sd/tkurth/atlas_dl/data_preselect_autoencoder\"\n",
    "numnodes=9300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: total =  1519654  chunksize =  163\n",
      "Fitting set: total =  651280  chunksize =  70\n",
      "Validation set: total =  127701  chunksize =  13\n",
      "Test-BG set: total =  255404  chunksize =  27\n",
      "Test-SG set: total =  1080838  chunksize =  116\n"
     ]
    }
   ],
   "source": [
    "#print ensemble sizes and determine the chunk size\n",
    "chunksize_train=int(np.ceil(hditer_train.num_examples/numnodes))\n",
    "print \"Training set: total = \", hditer_train.num_examples, \" chunksize = \", chunksize_train\n",
    "\n",
    "chunksize_train_fit=int(np.ceil(hditer_train_fit.num_examples/numnodes))\n",
    "print \"Fitting set: total = \", hditer_train_fit.num_examples, \" chunksize = \", chunksize_train_fit\n",
    "\n",
    "chunksize_validation=int(np.ceil(hditer_validation.num_examples/numnodes))\n",
    "print \"Validation set: total = \", hditer_validation.num_examples, \" chunksize = \", chunksize_validation\n",
    "\n",
    "chunksize_test=np.min([int(np.ceil(hditer_test.num_examples/numnodes)),60000])\n",
    "print \"Test-BG set: total = \", hditer_test.num_examples, \" chunksize = \", chunksize_test\n",
    "\n",
    "chunksize_test_signal=np.min([int(np.ceil(hditer_test_signal.num_examples/numnodes)),60000])\n",
    "print \"Test-SG set: total = \", hditer_test_signal.num_examples, \" chunksize = \", chunksize_test_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx,i in enumerate(range(0,hditer_train.num_examples,chunksize_train)):\n",
    "    iup=np.min([i+chunksize_train,hditer_train.num_examples])\n",
    "    \n",
    "    #preprocess\n",
    "    x,y,w,p,g,n=preprocess_data(hditer_train.df.iloc[i:iup], \\\n",
    "                                hditer_train.eta_range, \\\n",
    "                                hditer_train.phi_range, \\\n",
    "                                hditer_train.eta_bins, \\\n",
    "                                hditer_train.phi_bins)\n",
    "    x/=hditer_train.max_abs\n",
    "    \n",
    "    #write the file\n",
    "    f = h5.File(datadir+'/hep_train_chunk'+str(idx)+'.hdf5','w')\n",
    "    f['data']=x\n",
    "    f['label']=y\n",
    "    #reweight the weights for proper testing\n",
    "    f['weight']=w/( np.float(hditer_train.num_examples)/np.float(num_bg_total) )\n",
    "    #normalize those weights for training\n",
    "    f['normweight']=w/hditer_train.wmax\n",
    "    f['psr']=p\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for idx,i in enumerate(range(0,hditer_train_fit.num_examples,chunksize_train_fit)):\n",
    "    iup=np.min([i+chunksize_train_fit,hditer_train_fit.num_examples])\n",
    "    \n",
    "    #preprocess\n",
    "    x,y,w,p,g,n=preprocess_data(hditer_train_fit.df.iloc[i:iup], \\\n",
    "                                hditer_train_fit.eta_range, \\\n",
    "                                hditer_train_fit.phi_range, \\\n",
    "                                hditer_train_fit.eta_bins, \\\n",
    "                                hditer_train_fit.phi_bins)\n",
    "    x/=hditer_train.max_abs\n",
    "    \n",
    "    #write the file\n",
    "    f = h5.File(datadir+'/hep_trainfit_chunk'+str(idx)+'.hdf5','w')\n",
    "    f['data']=x\n",
    "    f['label']=y\n",
    "    #reweight the weights for proper testing\n",
    "    f['weight']=w/( np.float(hditer_train_fit.num_examples)/np.float(num_bg_total) )\n",
    "    #normalize those weights for training\n",
    "    f['normweight']=w/hditer_train.wmax\n",
    "    f['psr']=p\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test for background\n",
    "for idx,i in enumerate(range(0,hditer_test.num_examples,chunksize_test)):\n",
    "    iup=np.min([i+chunksize_test,hditer_test.num_examples])\n",
    "    \n",
    "    #preprocess\n",
    "    x,y,w,p,g,n=preprocess_data(hditer_test.df.iloc[i:iup], \\\n",
    "                                hditer_test.eta_range, \\\n",
    "                                hditer_test.phi_range, \\\n",
    "                                hditer_test.eta_bins, \\\n",
    "                                hditer_test.phi_bins)\n",
    "    x/=hditer_train.max_abs\n",
    "    \n",
    "    #write file\n",
    "    f = h5.File(datadir+'/hep_test_chunk'+str(idx)+'.hdf5','w')\n",
    "    f['data']=x\n",
    "    f['label']=y\n",
    "    #reweight the weights for proper testing. This time with respect to combined signal and bg test sets\n",
    "    f['weight']=w/( np.float(hditer_test.num_examples)/np.float(num_bg_total) )\n",
    "    f['psr']=p\n",
    "    f['mg']=g\n",
    "    f['mn']=n\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test for signal\n",
    "for idx,i in enumerate(range(0,hditer_test_signal.num_examples,chunksize_test_signal)):\n",
    "    iup=np.min([i+chunksize_test_signal,hditer_test_signal.num_examples])\n",
    "    \n",
    "    #preprocess\n",
    "    x,y,w,p,g,n=preprocess_data(hditer_test_signal.df.iloc[i:iup], \\\n",
    "                                hditer_test_signal.eta_range, \\\n",
    "                                hditer_test_signal.phi_range, \\\n",
    "                                hditer_test_signal.eta_bins, \\\n",
    "                                hditer_test_signal.phi_bins)\n",
    "    x/=hditer_train.max_abs\n",
    "    \n",
    "    #write file\n",
    "    f = h5.File(datadir+'/hep_test_signal_chunk'+str(idx)+'.hdf5','w')\n",
    "    f['data']=x\n",
    "    f['label']=y\n",
    "    #we take everything so no reweighting needed:\n",
    "    f['weight']=w\n",
    "    f['psr']=p\n",
    "    f['mg']=g\n",
    "    f['mn']=n\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx,i in enumerate(range(0,hditer_validation.num_examples,chunksize_validation)):\n",
    "    iup=np.min([i+chunksize_validation,hditer_validation.num_examples])\n",
    "    \n",
    "    #preprocess\n",
    "    x,y,w,p,g,n=preprocess_data(hditer_validation.df.iloc[i:iup], \\\n",
    "                                hditer_validation.eta_range, \\\n",
    "                                hditer_validation.phi_range, \\\n",
    "                                hditer_validation.eta_bins, \\\n",
    "                                hditer_validation.phi_bins)\n",
    "    x/=hditer_train.max_abs\n",
    "    \n",
    "    #write the file\n",
    "    f = h5.File(datadir+'/hep_validation_chunk'+str(idx)+'.hdf5','w')\n",
    "    f['data']=x\n",
    "    f['label']=y\n",
    "    f['weight']=w/( np.float(hditer_validation.num_examples)/np.float(num_bg_total) )\n",
    "    f['normweight']=w/hditer_train.wmax\n",
    "    f['psr']=p\n",
    "    f['mg']=g\n",
    "    f['mn']=n\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [deeplearning]",
   "language": "python",
   "name": "Python [deeplearning]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
