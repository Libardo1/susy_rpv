{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = 'tkurth'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from nbfinder import NotebookFinder\n",
    "import sys\n",
    "import os\n",
    "from os.path import join, exists\n",
    "from os import makedirs, mkdir\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "%matplotlib inline\n",
    "import time\n",
    "import h5py\n",
    "#from helper_fxns import suppress_stdout_stderr\n",
    "import copy\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataIterator(object):\n",
    "    def __init__(self, filelist, batch_size=128, shuffle=True, allow_fractional_batches=False, keys=[\"data\",\"label\",\"weight\",\"normweight\"]):\n",
    "        #keys\n",
    "        self.keys=keys\n",
    "        #batchsize and indices\n",
    "        self.batch_size=batch_size\n",
    "        #store the filelist\n",
    "        self.files=filelist\n",
    "        self.num_files=len(self.files)\n",
    "        #store the shuffle state\n",
    "        self.shuffle=shuffle\n",
    "        #allow fractional batches?\n",
    "        self.allow_fractional_batches=allow_fractional_batches\n",
    "        #file and event indices:\n",
    "        self.file_index=0\n",
    "        self.event_index=0\n",
    "        #hgroup:\n",
    "        self.hgroup={}\n",
    "        \n",
    "        #determine how many events we have:\n",
    "        self.num_events=0\n",
    "        for fname in self.files:\n",
    "            f=h5py.File(fname,'r')\n",
    "            count=f[self.keys[0]].shape[0]\n",
    "            f.close()\n",
    "            self.num_events+=count\n",
    "        \n",
    "        #shuffle files\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.files)\n",
    "            \n",
    "        #load the initial bunch of data\n",
    "        self.load_next_file()\n",
    "    \n",
    "    \n",
    "    #iterator\n",
    "    def __iter__(self):\n",
    "        #shuffle if requested\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.files)\n",
    "        #reset counters\n",
    "        self.file_index=0\n",
    "        self.event_index=0\n",
    "        #prefetch next\n",
    "        self.load_next_file()\n",
    "        #and go:\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    #load next file logic\n",
    "    def load_next_file(self):\n",
    "        #open file\n",
    "        f=h5py.File(self.files[self.file_index],'r')\n",
    "        #load data from file\n",
    "        for key in self.keys:\n",
    "            self.hgroup[key]=f[key].value\n",
    "        #close file\n",
    "        f.close()\n",
    "        \n",
    "        #datalength:\n",
    "        self.dlength=self.hgroup[self.keys[0]].shape[0]\n",
    "        \n",
    "        #shuffle data if requested\n",
    "        if self.shuffle:\n",
    "            reindex=np.random.permutation(self.dlength)\n",
    "            for key in self.keys:\n",
    "                self.hgroup[key]=self.hgroup[key][reindex]\n",
    "    \n",
    "    \n",
    "    #next function\n",
    "    def __next__(self):\n",
    "        #stop the iteration here\n",
    "        if self.file_index>=self.num_files:\n",
    "            raise StopIteration\n",
    "        \n",
    "        #grep data\n",
    "        #upper index\n",
    "        upper=np.min([self.dlength,self.event_index+self.batch_size])\n",
    "        #load data\n",
    "        tmphgroup={}\n",
    "        for key in self.keys:\n",
    "            tmphgroup[key]=self.hgroup[key][self.event_index:upper]\n",
    "\n",
    "        #load new file if needed:\n",
    "        if self.dlength<=(self.event_index+self.batch_size):\n",
    "            self.file_index+=1\n",
    "            \n",
    "            #check if the epoch is over\n",
    "            if self.file_index>=self.num_files:\n",
    "                #return the remainder\n",
    "                if self.allow_fractional_batches:\n",
    "                    return tmphgroup\n",
    "            else:\n",
    "                #prefetch the file\n",
    "                self.load_next_file()\n",
    "                #fetch the missing data:\n",
    "                rlength=self.batch_size-tmphgroup[self.keys[0]].shape[0]\n",
    "                for key in self.keys:\n",
    "                    tmphgroup[key]=np.concatenate([tmphgroup[key],self.hgroup[key][0:rlength]],axis=0)\n",
    "                self.event_index=rlength\n",
    "        else:\n",
    "            self.event_index+=self.batch_size\n",
    "        \n",
    "        #stop the iteration here\n",
    "        if self.file_index>=self.num_files:\n",
    "            raise StopIteration\n",
    "        \n",
    "        #return result\n",
    "        return tmphgroup\n",
    "    \n",
    "    \n",
    "    #backwards compatibility\n",
    "    def next(self):\n",
    "        return self.__next__()\n",
    "    \n",
    "    \n",
    "    #returns all the data in one big dictionary. HANDLE WITH CARE, it can easily overflow memory!\n",
    "    def get_all(self):\n",
    "        result={}\n",
    "        \n",
    "        #load first\n",
    "        f=h5py.File(self.files[0],'r')\n",
    "        for key in self.keys:\n",
    "            result[key]=f[key].value\n",
    "        f.close()\n",
    "        #load the rest\n",
    "        for fname in self.files[1:]:\n",
    "            f=h5py.File(fname,'r')\n",
    "            for key in self.keys:\n",
    "                result[key]=np.concatenate([result[key],f[key].value])\n",
    "            f.close()\n",
    "        #return result\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    #set the filelists\n",
    "    mainpath='/global/cscratch1/sd/tkurth/atlas_dl/data_delphes'\n",
    "    trainfiles=[mainpath+'/'+x for x in os.listdir(mainpath) if x.startswith('hep_training_')]\n",
    "    validationfiles=[mainpath+'/'+x for x in os.listdir(mainpath) if x.startswith('hep_validation_')]\n",
    "    testfiles=[mainpath+'/'+x for x in os.listdir(mainpath) if x.startswith('hep_test_')]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
