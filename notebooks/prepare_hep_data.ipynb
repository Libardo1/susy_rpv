{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__author__ = 'tkurth'\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as mlines\n",
    "import matplotlib.font_manager as font_manager\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "from matplotlib import gridspec\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from nbfinder import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "%matplotlib inline\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import h5py as h5\n",
    "\n",
    "#sqlite for storing the metadata\n",
    "import sqlite3 as sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dict merger\n",
    "def merge_dicts(dict1,dict2):\n",
    "    tmp = dict1.copy()\n",
    "    tmp.update(dict2)\n",
    "    return tmp\n",
    "\n",
    "#file string parser\n",
    "def parse_filename(fname,directory='.'):\n",
    "    directory=re.sub(r'^(.*?)(/+)$',r'\\1',directory)\n",
    "    \n",
    "    #signal file?\n",
    "    smatch=re.compile(r'^GG_RPV(.*?)_(.*?)_(.*?)_.*\\.h5')\n",
    "    tmpres=smatch.findall(fname)\n",
    "    if tmpres:\n",
    "        tmpres=tmpres[0]\n",
    "        return {'rpv':int(tmpres[0]), \n",
    "                'mGlu':int(tmpres[1]), \n",
    "                'mNeu':int(tmpres[2]), \n",
    "                'jz': 0,\n",
    "                'directory': directory,\n",
    "                'filename': fname}\n",
    "\n",
    "    #background file?\n",
    "    smatch=re.compile(r'^jetjet_JZ(.*?)_.*\\.h5')\n",
    "    tmpres=smatch.findall(fname)\n",
    "    if tmpres:\n",
    "        return {'rpv': 0., \n",
    "                'mGlu': 0.,\n",
    "                'mNeu': 0., \n",
    "                'jz': int(tmpres[0]),\n",
    "                'directory': directory,\n",
    "                'filename': fname}\n",
    "\n",
    "    #nothing at all\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read files from here\n",
    "#inputfile='/global/cscratch1/sd/tkurth/atlas_dl/metadata/inputfile.txt'\n",
    "#directories to read from\n",
    "#directories=['/global/cscratch1/sd/wbhimji/delphes_005_2017_03_06_NoPU-2',\n",
    "#             '/global/cscratch1/sd/wbhimji/delphes_005_2017_03_06_NoPU']\n",
    "#binning options\n",
    "eta_range = [-5,5]\n",
    "eta_bins = 224\n",
    "phi_range = [-3.1416, 3.1416]\n",
    "phi_bins = 224\n",
    "#parameters\n",
    "train_fraction=0.75\n",
    "validation_fraction=0.05\n",
    "nsig_augment=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#argument parsing\n",
    "parser = argparse.ArgumentParser(description='Preprocess Files for Training.')\n",
    "parser.add_argument('input', type=str, nargs=1, help='file which contains list of input files')\n",
    "parser.add_argument('output', type=str, nargs=1, help='sqlite db file for storing metadata')\n",
    "args = parser.parse_args()\n",
    "\n",
    "#fileparameters\n",
    "inputfile=args.input\n",
    "outputdbfilename=args.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curate File List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read input file:\n",
    "filelist=[]\n",
    "with open(inputfile) as f:\n",
    "    lines=f.readlines()\n",
    "    for line in lines:\n",
    "        filelist.append(line)\n",
    "    f.close()\n",
    "\n",
    "#get mapping of files to directories:\n",
    "filemap={}\n",
    "for item in filelist:\n",
    "    directory='/'.join(item.split('/')[:-1])\n",
    "    filename=item.split('/')[-1]\n",
    "    if directory not in filemap:\n",
    "        filemap[directory]=[filename]\n",
    "    else:\n",
    "        filemap[directory].append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filelist=[]\n",
    "normlist=[]\n",
    "for directory in filemap.keys():\n",
    "    #load files\n",
    "    filelist+=[parse_filename(x,directory) for x in filemap[directory]]\n",
    "    #load normalizations:\n",
    "    tmpdf=pd.read_csv(directory+'/DelphesNevents',sep=' ',index_col=False, header=None)\n",
    "    tmpdf['directory']=directory\n",
    "    normlist.append(tmpdf)\n",
    "filedf=pd.DataFrame(filelist)\n",
    "normdf=pd.concat(normlist)\n",
    "\n",
    "#parse the normalizations:\n",
    "normdf.rename(columns={1:'count'},inplace=True)\n",
    "normdf[0]=normdf[0].str.replace('QCDBkg_','')\n",
    "normdf['jz']=normdf[0].apply(lambda x: int(x.split('_')[0].split('JZ')[1]) if x.startswith('JZ') else 0)\n",
    "normdf['rpv']=normdf[0].apply(lambda x: int(x.split('_')[1].split('RPV')[1]) if x.startswith('GG') else 0.)\n",
    "normdf['mGlu']=normdf[0].apply(lambda x: int(x.split('_')[2]) if x.startswith('GG') else 0.)\n",
    "normdf['mNeu']=normdf[0].apply(lambda x: int(x.split('_')[3]) if x.startswith('GG') else 0.)\n",
    "\n",
    "#merge with filedf\n",
    "filedf=filedf.merge(normdf[['count','directory','rpv','jz','mGlu','mNeu']],how='left',on=['directory','rpv','jz','mGlu','mNeu'])\n",
    "\n",
    "#sort\n",
    "filedf.sort_values(by=['directory','filename'],inplace=True)\n",
    "filedf.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create connection to sql-db:\n",
    "con = sql.connect(metadatadir+\"/\"+outputdbfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#iterate over files, compute the max value for given binning and for weights:\n",
    "datadflist=[]\n",
    "for row in filedf.iterrows():\n",
    "    #open file\n",
    "    f = h5.File(row[1]['directory']+'/'+row[1]['filename'],'r')\n",
    "    \n",
    "    #iterate over items\n",
    "    rowlist=[]\n",
    "    for item in f.iteritems():\n",
    "        if not item[0].startswith('event'):\n",
    "            continue\n",
    "        \n",
    "        #copy rowdict\n",
    "        tmpdict=row[1].copy()\n",
    "        \n",
    "        #name\n",
    "        tmpdict[\"id\"]=item[0]\n",
    "        \n",
    "        #channel-0\n",
    "        clusPhi=item[1]['clusPhi'].value\n",
    "        clusEta=item[1]['clusEta'].value\n",
    "        clusE=item[1]['clusE'].value\n",
    "        #bin:\n",
    "        tmpdict[\"clusE_max\"]=np.max(np.histogram2d(clusPhi,clusEta,\n",
    "                                    bins=(phi_bins, eta_bins),weights=clusE,\n",
    "                                    range=[phi_range,eta_range])[0])\n",
    "        \n",
    "        #channel-1\n",
    "        clusEM=item[1]['clusEM'].value\n",
    "        #bin:\n",
    "        tmpdict[\"clusEM_max\"]=np.max(np.histogram2d(clusPhi,clusEta,\n",
    "                                    bins=(phi_bins, eta_bins),weights=clusEM,\n",
    "                                    range=[phi_range,eta_range])[0])\n",
    "        \n",
    "        #channel-2\n",
    "        trackPhi=item[1]['trackPhi'].value\n",
    "        trackEta=item[1]['trackEta'].value\n",
    "        #bin\n",
    "        tmpdict[\"track_max\"]=np.max(np.histogram2d(trackPhi,trackEta,\n",
    "                                    bins=(phi_bins, eta_bins),\n",
    "                                    range=[phi_range,eta_range])[0])\n",
    "        \n",
    "        #weight\n",
    "        tmpdict[\"weight_max\"]=np.max(item[1]['weight'].value)\n",
    "        \n",
    "        #append to list of rows\n",
    "        rowlist.append(tmpdict)\n",
    "        \n",
    "    #close the file\n",
    "    f.close()\n",
    "    \n",
    "    #write to database:\n",
    "    pd.DataFrame(rowlist).to_sql(\"metadata\", con, if_exists='append',chunksize=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "thorstendl",
   "language": "python",
   "name": "thorstendl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
