{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from helper_fxns.ipynb\n",
      "importing Jupyter notebook from print_n_plot.ipynb\n",
      "importing Jupyter notebook from build_network.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lasagne\n",
    "import time\n",
    "from nbfinder import NotebookFinder\n",
    "import sys\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "from matplotlib import patches\n",
    "from helper_fxns import early_stop\n",
    "from print_n_plot import *\n",
    "from build_network import build_network\n",
    "import logging\n",
    "#from data_loader import load_data\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    if batchsize > inputs.shape[0]:\n",
    "        batchsize=inputs.shape[0]\n",
    "    for start_idx in range(0,len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx: start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_logger(run_dir):\n",
    "    logger = logging.getLogger('log_train')\n",
    "    if not getattr(logger, 'handler_set', None):\n",
    "        logger.setLevel(logging.INFO)\n",
    "        fh = logging.FileHandler('%s/training.log'%(run_dir))\n",
    "        fh.setLevel(logging.INFO)\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setLevel(logging.INFO)\n",
    "        logger.addHandler(ch)\n",
    "        logger.addHandler(fh)\n",
    "    return logger\n",
    "\n",
    "def train_val(net_cfg, kwargs, data):\n",
    "    \n",
    "    logger = kwargs[\"logger\"]\n",
    "    x_tr, y_tr, x_val, y_val = data\n",
    "    logger.info(\"training set size: %i, val set size %i \" %( x_tr.shape[0], x_val.shape[0]))\n",
    "\n",
    "    tr_losses = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    tr_accs = []\n",
    "    for epoch in range(kwargs['num_epochs']):\n",
    "\n",
    "        start = time.time() \n",
    "        tr_loss = 0\n",
    "        tr_acc = 0\n",
    "        for iteration, (x, y) in enumerate(iterate_minibatches(x_tr,y_tr, batchsize=kwargs['batch_size'])):\n",
    "            #x = np.squeeze(x)\n",
    "            loss = net_cfg['tr_fn'](x, y)\n",
    "            weights = sum([np.sum(a.eval()) for a in get_all_params(net_cfg['network']) if str(a) == 'W'])\n",
    "            #logger.info(\"weights : %6.3f\" %(weights))\n",
    "            #logger.info(\"x avg : %5.5f shape: %s : iter: %i  loss : %6.3f \" % (np.mean(x), str(x.shape), iteration, loss))\n",
    "            _, acc = net_cfg['val_fn'](x,y)\n",
    "            logger.info(\"iteration % i train loss is %f\"% (iteration, loss))\n",
    "            logger.info(\"iteration % i train acc is %f\"% (iteration, acc))\n",
    "            tr_acc += acc\n",
    "            tr_loss += loss\n",
    "\n",
    "        train_end = time.time()\n",
    "        tr_avgacc = tr_acc / (iteration + 1)\n",
    "        tr_avgloss = tr_loss / (iteration + 1)\n",
    "\n",
    "\n",
    "        logger.info(\"train time : %5.2f seconds\" % (train_end - start))\n",
    "        logger.info(\"  epoch %i of %i train loss is %f\" % (epoch, kwargs[\"num_epochs\"], tr_avgloss))\n",
    "        logger.info(\"  epoch %i of %i train acc is %f percent\" % (epoch, kwargs[\"num_epochs\"], tr_avgacc * 100))\n",
    "        tr_losses.append(tr_avgloss)\n",
    "        tr_accs.append(tr_avgacc)\n",
    "\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        for iteration, (xval, yval) in enumerate(iterate_minibatches(x_val,y_val, batchsize=kwargs['batch_size'])):\n",
    "            #xval = np.squeeze(xval)\n",
    "            loss, acc = net_cfg['val_fn'](xval, yval)\n",
    "            val_loss += loss\n",
    "            val_acc += acc\n",
    "\n",
    "        val_avgloss = val_loss / (iteration + 1)\n",
    "        val_avgacc = val_acc / (iteration + 1)\n",
    "\n",
    "        logger.info(\"val time : %5.2f seconds\" % (time.time() - train_end))\n",
    "        logger.info(\"  epoch %i of %i val loss is %f\" % (epoch, kwargs[\"num_epochs\"], val_avgloss))\n",
    "        logger.info(\"  epoch %i of %i val acc is %f percent\" % (epoch, kwargs[\"num_epochs\"], val_avgacc * 100))\n",
    "\n",
    "        val_losses.append(val_avgloss)\n",
    "        val_accs.append(val_avgacc)\n",
    "\n",
    "        plot_learn_curve(tr_losses, val_losses, save_dir=kwargs[\"save_dir\"])\n",
    "        plot_learn_curve(tr_accs, val_accs, save_dir=kwargs[\"save_dir\"], name=\"acc\")\n",
    "        pickle.dump(net_cfg['network'],open(kwargs[\"save_dir\"] + \"/model.pkl\", 'w'))\n",
    "\n",
    "    #     if epoch % 5 == 0:\n",
    "    #         plot_filters(net_cfg['network'], save_dir=run_dir)\n",
    "    #         for iteration, (xval, yval) in enumerate(iterate_minibatches(x_val,y_val, batchsize=batchsize)):\n",
    "    #             plot_feature_maps(iteration, xval,net_cfg['network'], save_dir=run_dir)\n",
    "    #             break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "deeplearning_plus_root",
   "language": "python",
   "name": "deeplearning_plus_root"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
