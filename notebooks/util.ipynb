{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "import sys\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import logging\n",
    "#enable importing of notebooks\n",
    "from nbfinder import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "from os import makedirs, mkdir\n",
    "from os.path import join, exists\n",
    "from lasagne.layers import *\n",
    "# from print_n_plot import plot_ims_with_boxes, add_bbox, plot_im_with_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  def create_run_dir(results_dir=None, name=None):\n",
    "    if results_dir == None:\n",
    "        results_dir = './results'\n",
    "    \n",
    "    \n",
    "    makedir_if_not_there(results_dir)\n",
    "    run_num_file = os.path.join(results_dir, \"run_num.txt\")\n",
    "\n",
    "\n",
    "    if not os.path.exists(run_num_file):\n",
    "        print(\"making run num file....\")\n",
    "        f = open(run_num_file,'w')\n",
    "        f.write('0')\n",
    "        f.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    f = open(run_num_file,'r+')\n",
    "\n",
    "    run_num = int(f.readline()) + 1\n",
    "\n",
    "    f.seek(0)\n",
    "\n",
    "    f.write(str(run_num))\n",
    "    if name is None:\n",
    "        fname = 'run'\n",
    "    else:\n",
    "        fname = name\n",
    "    run_dir = os.path.join(results_dir,fname + str(run_num))\n",
    "    os.mkdir(run_dir)\n",
    "    return run_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dump_hyperparams(dic, path):\n",
    "    new_dic = {k:str(dic[k]) for k in dic.keys()}\n",
    "    with open(path + '/hyperparams.txt', 'w') as f:\n",
    "        for k,v in new_dic.iteritems():\n",
    "            f.write(k + ' : ' + v + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_logger(run_dir):\n",
    "    logger = logging.getLogger('log_train')\n",
    "    if not getattr(logger, 'handler_set', None):\n",
    "        logger.setLevel(logging.INFO)\n",
    "        fh = logging.FileHandler('%s/training.log'%(run_dir))\n",
    "        fh.setLevel(logging.INFO)\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setLevel(logging.INFO)\n",
    "        logger.addHandler(ch)\n",
    "        logger.addHandler(fh)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makedir_if_not_there(dirname):\n",
    "        if not exists(dirname):\n",
    "            try:\n",
    "                mkdir(dirname)\n",
    "            except OSError:\n",
    "                makedirs(dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(args, batchsize=128, shuffle=False):\n",
    "    assert len(args[0]) == len(args[1])\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(args[0]))\n",
    "        np.random.shuffle(indices)\n",
    "    if batchsize > args[0].shape[0]:\n",
    "        batchsize=args[0].shape[0]\n",
    "    for start_idx in range(0,len(args[0]) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx: start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield [arg[excerpt] for arg in args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_weights(metrics, kwargs, networks):\n",
    "\n",
    "    def _save_weights(name,suffix=\"\"):\n",
    "        params = get_all_param_values(networks[name])\n",
    "        model_dir = join(kwargs['save_path'], \"models\")\n",
    "        makedir_if_not_there(model_dir)\n",
    "        pickle.dump(params,open(join(model_dir, name + \"_\" + suffix + \".pkl\"), \"w\"))\n",
    "    \n",
    "    \n",
    "    max_metrics = [\"validation_acc\", \"validation_ams\", \"validation_sig_eff_at_cuts_bg_rej\"]\n",
    "    min_metrics = [\"validation_loss\"]\n",
    "    for k in max_metrics:\n",
    "        if len(metrics[k]) > 1:\n",
    "            if metrics[k][-1] > max(metrics[k][:-1]):\n",
    "                _save_weights(\"net\", \"best_\" + k)\n",
    "\n",
    "\n",
    "        else:\n",
    "            _save_weights(\"net\", \"best_\" + k)\n",
    "    for k in min_metrics:\n",
    "        if len(metrics[k]) > 1:\n",
    "            if metrics[k][-1] < min(metrics[k][:-1]):\n",
    "                _save_weights(\"net\", \"best_\" + k)\n",
    "\n",
    "\n",
    "    _save_weights(\"net\", \"cur\")\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
